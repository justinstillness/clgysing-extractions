================================================================================
================================================================================

                          AI OPERATIONS MANUAL

           A Practical System for AI-Assisted Business Operations

================================================================================
================================================================================

Document Code:   GOV-AIPM
Version:         1.0.7-DRAFT
Date:            December 28, 2025
Status:          Phase 6b Review and Validation

================================================================================
                            TABLE OF CONTENTS
PART I: FOUNDATIONS
  Section 1: Purpose & Scope
    1.1 Purpose
    1.2 Intended Audience
    1.3 Scope
    1.4 How to Use This Manual
      1.4.1 For New Users
      1.4.2 For Practitioners
      1.4.3 For Organizations
      1.4.4 For Reference
      1.4.5 Project Readiness Assessment
      1.4.6 User Workflow Overview
    1.5 Document Conventions
    1.6 Relationship to Other Documents

  Section 2: Core Philosophy
    2.1 Foundational Principles
      2.1.1 Chats Are Ephemeral; Records Are Not
      2.1.2 External Memory Always Wins
      2.1.3 Substance Before Presentation
      2.1.4 Use the Full System
    2.2 Operational Stance
      2.2.1 AI as Junior Associate, Not Oracle
      2.2.2 Trust but Verify
      2.2.3 Explicit Over Implicit
    2.3 What This System Is Not
    2.4 The Compound Effect
    2.5 The Three Cycles of AI-Assisted Work
      2.5.1 Project Lifecycle (Macro Cycle)
      2.5.2 Session Lifecycle (Meso Cycle)
      2.5.3 Interaction Cycle (Micro Cycle)
      2.5.4 The Parallel Support Track
      2.5.5 Navigating the Framework
    2.6 The Orientation Principle
      2.6.1 Orientation Triggers
      2.6.2 The Orientation Templates
      2.6.3 The Cost Equation
    2.7 AI as System Coordinator
      2.7.1 The Coordinator Concept
      2.7.2 Scope Monitoring Duty
      2.7.3 Phase Transition Orientation Duty
      2.7.4 Document Awareness Duty
      2.7.5 Proactive Orientation Duty
      2.7.6 Continuity Assurance Duty
      2.7.7 Decision Capture Duty
      2.7.8 What Users Should Expect

  Section 3: The Explicit Alignment Principle
    3.1 The Alignment Problem in Practice
    3.2 The Fifth-Grader Rule
    3.3 Implementation Practices
    3.4 When Alignment Matters Most
    3.5 The Dialog Intake System
      3.5.1 Purpose and Philosophy
      3.5.2 Entry Point Triage
      3.5.3 Confirmation Calibration Framework
      3.5.4 Context Freshness Protocol
      3.5.5 Touchpoint Reference
      3.5.6 Failure Mode Catalog Reference
    3.6 The Cost-Benefit Reality
    3.7 Common Failure Modes

PART II: PROJECT MANAGEMENT
  Section 4: Project Architecture & Token Management
    4.1 Chat Architecture
      4.1.1 Chat Types
      4.1.2 Chat Relationships
      4.1.3 Naming Conventions
    4.2 Token Management
      4.2.1 Token Economics
      4.2.2 Headroom Discipline
      4.2.3 Token-Efficient Practices
      4.2.4 Warning Signs of Context Exhaustion
      4.2.5 Token Exhaustion Recovery Protocol
    4.3 Session Lifecycle
      4.3.1 Session Initiation
      4.3.2 Active Session Management
      4.3.3 Session Closeout
      4.3.4 Emergency Exit
      4.3.5 Phase Transition Sessions

  Section 5: Canonical Project Artifacts
    5.1 Project Guide
    5.2 Decision Logs
    5.3 Session Logs
    5.4 Bootstrap Files
    5.5 Supporting Artifacts
    5.6 Artifact Relationships
    5.7 Backlog Register
    5.8 Project Closure Report

  Section 6: External Organization System
    6.1 Naming Conventions
      6.1.1 Chat Naming
      6.1.2 File Naming
      6.1.3 Version Indicators
    6.2 Folder Structure
      6.2.1 Chat-Based Organization
      6.2.2 Functional Organization
      6.2.3 Archive Structure
    6.3 Duplication Discipline
    6.4 Cloud Storage Integration
    6.5 Retrieval Patterns
    6.6 Organization Hygiene

PART III: GOVERNANCE INTEGRATION
  Section 7: The Governance Stack
    7.1 Governing Documents
    7.2 Resolutions
    7.3 SOPs & Policies
    7.4 Governance Maintenance

  Section 8: AI-Assisted Governance Development
    8.1 Governance R&D Workflow
    8.2 Resolution Drafting Workflow
    8.3 SOP Drafting Workflow
    8.4 Adoption & Implementation

PART IV: TOOLS & FORMATS
  Section 9: AI Role Separation
    9.1 The Case for Role Separation
    9.2 Primary Execution Role
    9.3 Secondary Validation Role
    9.4 Specialized Function Role
    9.5 Multi-AI Workflow Patterns
    9.6 Platform Selection Considerations
    9.7 Role Separation and Governance

  Section 10: Adjunct Tools
    10.1 Capture Tools
    10.2 Calendar and Scheduling
    10.3 Retrieval and Query Tools
    10.4 Conversion and Utility Tools
    10.5 Storage and Sync Tools
    10.6 Chat Export and Archival Tools
    10.7 Golden Prompt Library
    10.8 User-Side Workflow Tips

  Section 11: File Types and Formatting
    11.1 The Role of File Types in AI Workflows
    11.2 Input Format Considerations
    11.3 Output Format Considerations
    11.4 Format Conversion Pathways
    11.5 Formatting Standards for Project Artifacts
    11.6 Format and the Development Workflow
    11.7 Platform-Specific Considerations

PART V: ADVANCED PATTERNS
  Section 12: Extended Context
    12.1 Multi-Session Continuity
    12.2 Complex Project Management

  Section 13: Specialized Use Cases
    13.1 Research Projects
    13.2 Creative Projects
    13.3 Technical Projects

  Section 14: Failure Modes
    14.1 Common Pitfalls
    14.2 Recovery Strategies
    14.3 Cognitive Divergence Detection

PART VI: MAIL & EXTERNAL COMMUNICATIONS
  Section 15: Email Integration
    15.1 Email-to-AI Workflow
    15.2 AI-to-Email Workflow

  Section 16: External Communications
    16.1 Client Communications
    16.2 Stakeholder Communications

  Section 17: Reference
    17.1 Quick Reference Guide
    17.2 Glossary
    17.3 Index

APPENDICES
  Appendix A: Template Reference
  Appendix B: Prompt Reference (Golden Prompt Library)
  Appendix C: Tool Configuration Guide
  Appendix D: Troubleshooting Guide
  Appendix E: Version History
  Appendix F: Dialog Intake Failure Mode Catalog
  Appendix G: Artifact Taxonomy Reference
  Appendix H: Quick Start Guide


================================================================================
================================================================================

                            PART I: FOUNDATIONS

================================================================================
================================================================================


================================================================================
SECTION 1: PURPOSE & SCOPE
1.1 PURPOSE
-----------

This manual documents a complete, field-tested system for AI-assisted business 
operations. It addresses a specific operational challenge: AI tools offer 
substantial productivity gains but operate under constraints—limited memory 
across sessions, token ceilings, silent assumption errors, and overconfidence 
during extended use—that create risk when unmanaged.

Simultaneously, most business failures stem not from lack of capability but 
from poor documentation, decision drift, governance gaps, and cognitive 
overload. This system solves both problems together.

The manual provides:

- Practical methods for using AI effectively despite technical constraints
- A framework for managing complex, multi-session projects with continuity
- Integration patterns connecting AI-assisted work to business governance
- Standards for creating documentation that scales and remains auditable
- Protocols for preventing cognitive, operational, and legal drift

The underlying philosophy is simple: AI chats are ephemeral compute 
environments, not durable memory. All persistent state must live outside the 
AI in files, logs, guides, and structured records. External memory always wins.


1.2 INTENDED AUDIENCE
---------------------

This manual is designed for:

- Solo founders managing operations without dedicated staff
- Managers overseeing teams that use AI tools in their work
- Small teams requiring reliability, auditability, and continuity
- Consultants and advisors integrating AI into client engagements
- Any operator whose work spans multiple sessions and whose decisions carry 
  legal or operational weight

The system assumes:

- Projects routinely span multiple AI sessions
- Decisions have real consequences (legal, financial, operational)
- Memory loss—whether human or AI—is a genuine risk requiring mitigation
- The user values substance and defensibility over speed alone


1.3 SCOPE
---------

1.3.1 In Scope
--------------

This manual addresses:

AI Session Management
- Token economics and context window constraints
- Memory limitations and continuity strategies
- Session lifecycle from initiation through closeout

Project Architecture
- Root, branch, and execution chat structures
- Bootstrap creation and deployment
- Multi-session project continuity

Business Governance Integration
- Connecting AI-assisted work to governing documents
- Resolution drafting and adoption workflows
- SOP development and maintenance cycles

Risk Management
- Legal, operational, reputational, and cognitive risk categories
- AI-specific risks including hallucination, bias, and over-reliance
- Verification and quality assurance protocols

Security and Confidentiality
- Data classification frameworks
- Handling rules for sensitive information
- Provider considerations and retention awareness

Operational Infrastructure
- File organization and naming conventions
- Format standards for different document types
- Tool selection and role separation

Sustainability
- Cognitive state management and fatigue prevention
- Re-entry protocols after extended absence
- Onboarding and knowledge transfer

1.3.2 Out of Scope
------------------

This manual does not address:

- AI model selection, technical evaluation, or benchmarking
- API integration, programmatic deployment, or automation scripting
- Infrastructure architecture or system administration
- Model fine-tuning, training, or customization
- Vendor contract negotiation or procurement processes
- Specific legal advice (consult qualified counsel for jurisdiction-specific 
  requirements)

These exclusions reflect focus, not importance. Organizations with needs in 
these areas should develop complementary documentation or engage appropriate 
specialists.


1.4 HOW TO USE THIS MANUAL
--------------------------

1.4.1 For New Users
-------------------

Begin with the Quick Start section for orientation. This provides the core 
concepts in compressed form. Then read Part I (Foundations) to understand the 
philosophical grounding before proceeding to practical sections.

1.4.2 For Practitioners
-----------------------

Use Part II (Project Management) as your operational reference. The templates 
in Appendix A provide ready-to-use artifacts. Refer to specific sections as 
needed during active projects.

1.4.3 For Organizations
-----------------------

Part III (Governance Integration) connects this system to formal business 
governance. Organizations adopting this manual should review Section 7 
(Governance Stack) in conjunction with their existing governing documents 
and identify integration points.

1.4.4 For Reference
-------------------

The Glossary (Section 17.2) provides canonical definitions for all terms as 
used in this manual. When terminology seems ambiguous, consult the glossary 
first—definitions here override casual or industry-variable usage.

For deeper exploration of artifact classification and the universal project 
functions that underpin this framework, see Appendix G: Artifact Taxonomy 
Reference.


1.4.5 Project Readiness Assessment
----------------------------------

Before initiating a formal project with AI assistance, assess whether your 
idea is ready for structured execution. Not every task requires the full 
project management apparatus—but tasks that do require it benefit from 
honest readiness evaluation.

Answer these questions before completing the Project Initiation Form 
(TPL-001):

Can I state what I'm trying to produce in one sentence?
If not, the project objective is too vague for effective AI collaboration. 
A clear deliverable description—even if preliminary—provides essential 
direction.

Do I know who will use or receive the output?
Audience shapes everything: format, depth, tone, and verification 
requirements. Unknown audience suggests insufficient project definition.

Can I identify when the project is done?
Open-ended work without completion criteria tends to drift indefinitely. 
Define "done" before starting, even if the definition evolves.

Do I have the inputs needed, or know how to get them?
AI cannot work from context you don't provide. Missing source materials, 
references, or background information will stall progress.

Is this one project or several?
Scope confusion produces scattered work. If your "project" contains 
multiple independent objectives, consider separating them.

If you cannot answer these questions, you may not be ready for formal 
project initiation. Consider an exploration session instead.

Exploration Sessions

Some ideas need development before they're ready for structured execution. 
An exploration session uses AI to clarify thinking rather than produce 
deliverables.

Exploration prompts:
- "Help me think through what I'm trying to accomplish with [X]."
- "What questions should I be able to answer before starting [X]?"
- "What are the components of [X]? Which should I tackle first?"
- "What would success look like for [X]?"

Exploration sessions are low-stakes and do not require full project 
infrastructure. Their output may include clarified objectives suitable 
for subsequent project initiation—or recognition that the idea needs 
more development outside AI assistance.

The path from exploration to formal project:

1. Conduct exploration session(s) to clarify thinking
2. Attempt to answer the readiness questions above
3. If answers are clear, complete Project Initiation Form (TPL-001)
4. If answers remain unclear, continue exploration or pause the idea

Not every idea graduates to project status. The readiness assessment 
prevents premature commitment to projects that lack foundation for success.


1.4.6 User Workflow Overview
----------------------------

This section provides consolidated guidance for the most common question 
users face: "What do I need to provide to the AI, and when?"

The answer depends on where you are in your workflow. Four situations 
cover nearly all cases: starting a new project, continuing session to 
session, resuming after extended absence, and transitioning between 
project phases. Each has specific upload requirements and expected 
behaviors.


1.4.6.1 Starting Your First Project
-----------------------------------

When you initiate a new project with AI assistance, you are establishing 
context from scratch. No prior state exists to carry forward; everything 
must be explicitly provided.

What to Upload:
- This manual (or relevant sections for your project type)
- Completed Project Initiation Form (TPL-001), or be prepared to develop 
  one through dialog with the AI

What the AI Does:
- Recognizes the framework and adopts its operational behaviors
- Reviews your TPL-001 or asks structured questions to develop one
- Clarifies ambiguities through targeted questions
- Generates an initial Project Guide draft based on your inputs
- Confirms alignment before proceeding to substantive work

What You Do:
- Review the AI's understanding and correct any misinterpretations
- Confirm the Project Guide captures your intent
- Approve the AI to begin work, or iterate on alignment first

Expected Outcome:
An established project with a Project Guide that defines scope, objectives, 
constraints, and initial phase structure. Both you and the AI share a 
verified understanding of what you're building and how you'll proceed.

Common Pitfall: Jumping straight into work without completing TPL-001 
orientation. This creates implicit assumptions that surface as misalignment 
later.


1.4.6.2 Continuing a Project (Session to Session)
-------------------------------------------------

Most AI-assisted work happens in this mode: continuing a project across 
multiple sessions, with each session picking up where the last one ended.

What to Upload:
- Bootstrap from the prior session (TPL-002)
- Any files the bootstrap indicates are needed for the planned work

What the AI Does:
- Orients to the project state as captured in the bootstrap
- Confirms understanding of where you left off
- Identifies the planned work for this session
- Proceeds with execution per the bootstrap's direction

What You Do:
- Verify the AI's stated understanding matches your expectations
- Clarify any questions before substantive work begins
- Conduct session work
- At session end, ensure a new bootstrap is generated for next time

Key Principle: The bootstrap carries tactical session state; the Project 
Guide carries strategic project definition. For routine session-to-session 
continuation, the bootstrap alone is usually sufficient. The AI should 
request the Project Guide if it needs strategic context beyond what the 
bootstrap provides.

Common Pitfall: Not generating a bootstrap at session end, then struggling 
to reconstruct context at next session start.


1.4.6.3 Resuming After Extended Absence
---------------------------------------

Extended absence—more than a few days, or any gap where you've lost your 
own mental context—requires more thorough re-orientation than routine 
session continuation.

What to Upload:
- Re-Entry Checklist (TPL-007) as a structural guide
- Project Guide (current version)
- Project Roadmap (TPL-010) if one exists
- Most recent bootstrap

What the AI Does:
- Re-orients to full project context, not just last session state
- Assesses current project position against the roadmap
- Identifies any drift or staleness in project materials
- Flags issues requiring attention before resuming substantive work
- Confirms alignment with you before proceeding

What You Do:
- Walk through the Re-Entry Checklist with the AI
- Verify the AI's assessment of project state
- Address any flagged issues
- Confirm readiness to resume before beginning new work

When to Use This Mode:
- After gaps of more than a few days
- When you've lost your own context and need to reorient yourself
- When project materials may have grown stale
- After significant life or work disruptions that affected your attention

The extra investment in re-orientation prevents the silent drift that 
accumulates when assumptions substitute for verified understanding.

Common Pitfall: Treating extended absence as routine continuation, then 
discovering mid-session that you and the AI have different understandings 
of project state.


1.4.6.4 Transitioning Between Phases
------------------------------------

Project phases represent distinct bodies of work with different objectives. 
Moving from one phase to the next is not the same as moving from one session 
to the next—it requires returning to strategic context and planning the 
upcoming phase explicitly.

What to Upload:
- Project Guide (strategic authority)
- Project Roadmap (TPL-010) showing phase structure
- Bootstrap from the final session of the prior phase

What the AI Does:
- Reviews the Project Guide for the upcoming phase definition
- Updates the Roadmap: marks prior phase complete, updates current position
- Generates Current Phase Detail (Phase Execution Brief) for the new phase:
  * Phase objectives
  * Estimated session allocation
  * Expected deliverables
  * Success criteria for phase completion
- Provides a "horizon view" summary: where you've been, where you're going
- Confirms the phase plan with you before proceeding

What You Do:
- Ensure your Project Guide is current (update if scope changed during 
  prior phase)
- Review the AI-generated phase plan
- Confirm or adjust before new phase work begins
- Note the phase transition in your project records

This is the "return to central brain" moment—stepping back from tactical 
execution to verify strategic alignment before the next body of work.

Cross-reference: See Section 4.3.5 for the full Phase Transition Protocol.

Common Pitfall: Treating phase transitions as ordinary session boundaries, 
missing the opportunity for strategic recalibration.


1.4.6.5 Summary Reference
-------------------------

| Situation                | Key Upload(s)                | AI Priority Action           |
|--------------------------|------------------------------|------------------------------|
| First Project            | Manual + TPL-001             | Framework adoption, PG draft |
| Session Continuation     | Bootstrap                    | State confirmation, execute  |
| Extended Absence         | TPL-007, PG, Roadmap, Boot   | Full re-orientation          |
| Phase Transition         | PG, Roadmap, Bootstrap       | Phase plan, horizon view     |

Abbreviations: PG = Project Guide, Boot = Bootstrap

When uncertain which mode applies, err toward more orientation rather than 
less. The cost of redundant context-setting is minutes; the cost of 
misalignment is hours.


1.5 DOCUMENT CONVENTIONS
------------------------

Format Indicators:
- TXT/Markdown: Used for working documents, logs, and AI-facing content
- HTML/Presentation: Used for final deliverables and human-facing documents

Notation:
- [EXAMPLE] indicates placeholder text requiring customization
- [TBD] indicates content pending resolution
- Cross-references use "See Section X.X" format

Version Control:
- Material changes trigger version increment (1.0 → 1.1)
- Minor corrections use point versions (1.0 → 1.01)
- All versions maintain revision history


1.6 RELATIONSHIP TO OTHER DOCUMENTS
------------------------------------

This manual operates as a standalone reference but integrates with 
organizational governance documentation:

- Governing Documents: This manual does not supersede articles, operating 
  agreements, or bylaws; it provides operational methodology within the 
  authority those documents establish

- Resolutions: AI-assisted work may require formal adoption via resolution; 
  this manual provides drafting support but does not itself constitute 
  authorization

- SOPs: This manual may be referenced by or incorporated into standard 
  operating procedures; Section 8 provides specific guidance on SOP 
  integration

- Policies: Organizations may adopt this manual by policy reference or 
  extract specific provisions into standalone policies


================================================================================
SECTION 2: CORE PHILOSOPHY
2.1 FOUNDATIONAL PRINCIPLES
---------------------------

This system rests on four principles that govern all operational decisions. 
These are not preferences but structural requirements—violating them 
introduces failure modes that compound over time.


2.1.1 Chats Are Ephemeral; Records Are Not
------------------------------------------

AI chat sessions are temporary compute environments. They provide processing 
power and reasoning capability within a bounded context, but they are not 
storage systems. Every chat session will end. Context will be lost. The AI 
will not remember.

This is not a flaw to work around but a constraint to design for. Systems 
that treat chat history as memory eventually fail—through token exhaustion, 
context window limits, provider changes, or simple session termination.

The operational implication: anything that matters must exist outside the 
chat. If it only lives in conversation history, it is already at risk.


2.1.2 External Memory Always Wins
---------------------------------

High-functioning operations—whether individual or organizational—externalize 
state. They maintain:

- Files that capture deliverables and working documents
- Logs that record decisions, rationale, and events
- Guides that preserve strategic direction and project architecture  
- SOPs that encode procedures independent of any individual's recall
- Calendars that enforce cadence without relying on memory

This externalization is not bureaucratic overhead. It is the mechanism by 
which work survives transitions—between sessions, between team members, 
between periods of dormancy, between tools.

Never rely on chat recall. Never assume the AI remembers. Never trust that 
you will remember either. Build systems that function when memory fails, 
because memory always eventually fails.


2.1.3 Substance Before Presentation
-----------------------------------

During creation, correctness matters more than appearance. Draft in plain 
text. Focus on accuracy, completeness, and logical structure. Defer 
formatting decisions until content is stable.

This principle serves multiple purposes:

Token Efficiency: Plain text consumes fewer tokens than formatted documents, 
preserving context window capacity for substantive work.

Revision Flexibility: Unformatted content is easier to restructure. Premature 
formatting creates friction against necessary changes.

Quality Focus: Formatting can mask weak content. Plain text forces attention 
to substance.

Tool Compatibility: TXT and Markdown work across all AI tools and text 
editors without conversion artifacts.

The time for presentation is after substance is sound. Formatting is 
downstream of thinking.


2.1.4 Use the Full System
-------------------------

This manual offers flexibility. Templates include optional fields. Processes 
allow abbreviated versions for simpler projects. Users may customize 
workflows to their context.

That flexibility is not an invitation to cherry-pick.

The prescription is clear: use the complete system, especially when starting. 
Every template, every log, every checklist exists because its absence 
created a failure in real-world use. The components interlock. Skipping 
the Decision Log makes the Bootstrap less useful. Skipping the Session 
Closeout makes the Weekly Review incomplete. Shortcuts compound.

Users who adopt the full system and later streamline based on experience 
make informed trade-offs. Users who start by selecting only the pieces 
that seem convenient build habits around gaps they haven't yet encountered.

The principle: err toward completeness. Redundancy in documentation is 
cheap. Reconstructing lost context is expensive. When uncertain whether 
a step is necessary, do it. The cost of unnecessary diligence is minutes; 
the cost of insufficient diligence is hours or days of rework, or worse, 
decisions made on faulty foundations.

This applies doubly when onboarding. New users should resist the temptation 
to "start simple and add complexity later." Start complete. Simplify only 
after the full system is understood and its value demonstrated.


2.2 OPERATIONAL STANCE
----------------------

These principles translate into a specific operational stance toward AI 
tools—neither dismissive nor uncritical.


2.2.1 AI as Junior Associate, Not Oracle
----------------------------------------

AI tools function best when treated as capable but unseasoned assistants. 
They can draft, analyze, organize, and reason within their training. They 
cannot independently verify facts, guarantee accuracy, or substitute for 
domain expertise.

This framing sets appropriate expectations:

- Outputs require review before reliance
- Confident tone does not indicate correctness
- The human remains accountable for final decisions
- AI contribution is significant but not authoritative

The junior associate model also implies development: with good prompting, 
clear context, and appropriate verification, AI tools become more useful 
over time within a given project—not because they learn, but because the 
human learns to direct them effectively.


2.2.2 Trust but Verify
----------------------

AI outputs should be treated as first drafts requiring verification, not 
finished products requiring only formatting. The appropriate level of 
verification scales with stakes:

Low Stakes: Quick reasonableness check; does this make sense?

Medium Stakes: Cross-reference against known sources; spot-check specifics

High Stakes: Independent verification; second-AI validation; human expert 
review

Section 14 (Failure Modes) provides the full framework. 
The principle here is that verification is not optional—it is integral to 
the workflow, built in rather than bolted on.


2.2.3 Explicit Over Implicit
----------------------------

AI tools make assumptions to fill gaps in context. These assumptions are 
often reasonable but sometimes wrong—and wrong assumptions compound silently. 
The solution is aggressive explicitness.

State what you are doing and why. Articulate constraints even when they seem 
obvious. Specify desired outputs in detail. Check for understanding before 
proceeding.

This practice—developed in Section 3 as the Explicit Alignment Principle—
prevents the silent errors that plague AI-assisted work. Over-explanation 
is quality control, not inefficiency.


2.3 WHAT THIS SYSTEM IS NOT
---------------------------

Clarity about boundaries prevents misapplication:

Not a Replacement for Expertise: This system helps organize and execute 
knowledge work; it does not generate domain expertise. Legal, financial, 
technical, and strategic decisions require appropriate qualifications.

Not a Guarantee: Following this system reduces risk and improves reliability 
but cannot eliminate all failure modes. Judgment remains essential.

Not Rigid Prescription: The frameworks here are starting points. Adapt them 
to your context, tools, and constraints. The principles matter more than 
the specific implementations.

Not Tool-Specific: While examples may reference particular AI tools, the 
system is designed for portability. Core methods transfer across providers 
and platforms.


2.4 THE COMPOUND EFFECT
-----------------------

These principles produce compound returns over time. Early investment in 
externalized memory, explicit communication, and verification protocols 
pays dividends:

- Projects resume cleanly after interruption
- Decisions remain defensible months later
- New team members onboard from documentation rather than oral history
- Audit trails exist without reconstruction
- Cognitive load decreases as systems carry memory burden

The alternative—relying on chat history, assuming shared context, deferring 
documentation—produces compound costs. Small shortcuts accumulate into 
significant drift, lost work, and indefensible decisions.

This manual documents the practices that produce compound gains. The 
investment is real but so is the return.


2.5 THE THREE CYCLES OF AI-ASSISTED WORK
----------------------------------------

AI-assisted operations unfold across three nested temporal cycles, each with 
distinct concerns, tools, and failure modes. Understanding these cycles 
provides a mental map for navigating the system—knowing which tools apply 
when, diagnosing problems at the right level, and maintaining orientation 
as work proceeds.

The three cycles are: Project Lifecycle (macro), Session Lifecycle (meso), 
and Interaction Cycle (micro). A fourth element—the Parallel Support Track—
encompasses activities outside the AI environment that enable success within 
it.


2.5.1 Project Lifecycle (Macro Cycle)
-------------------------------------

The Project Lifecycle spans the entire arc of a project from conception 
through completion. This is the strategic level—where objectives are defined, 
resources allocated, and success ultimately measured.

Temporal Scope: Days to months, depending on project complexity.

Key Question: "Where is this project in its overall journey?"

Phases:

Pre-AI Planning. Before engaging AI, clarify what you're building, why it 
matters, and what success looks like. This phase may involve stakeholder 
consultation, resource assessment, and scope definition—work that happens 
before any AI session begins.

Project Definition. Formal capture of project parameters: objectives, 
constraints, deliverables, timeline. The Project Initiation Form (TPL-001) 
structures this definition. A Project Guide may be created for complex 
projects requiring sustained documentation.

Execution Phases. The bulk of project work, typically spanning multiple 
sessions. Progress is measured against defined milestones. The Project 
Roadmap (TPL-010) tracks phase completion and upcoming work.

Completion. Final deliverables produced, reviewed, and accepted. Project 
artifacts archived for future reference. Lessons captured for process 
improvement.

Templates Operating at This Level:
- TPL-001: Project Initiation Form (defines the project)
- TPL-007: Re-Entry Checklist (resuming after extended absence)
- TPL-010: Project Roadmap (tracking overall progress)

Common Macro-Level Failures:
- Scope creep undetected until late stages
- Insufficient upfront definition leading to repeated pivots
- Loss of project thread across extended timelines
- Completion criteria never clarified, causing indefinite extension


2.5.2 Session Lifecycle (Meso Cycle)
------------------------------------

The Session Lifecycle encompasses what happens each time you sit down to 
work with AI—from preparation through closeout. This is the tactical level 
where daily work happens.

Temporal Scope: Minutes to hours per session.

Key Question: "What do I need to do before, during, and after this session?"

Phases:

Preparation. Before opening the AI chat: gather materials, review prior 
session state, prepare context documents, identify session objectives. 
Preparation quality directly affects session productivity.

Initiation. Establishing context at session start: uploading bootstrap, 
providing project materials, confirming AI alignment with project state. 
The Re-Entry Checklist (TPL-007) guides this phase for returning projects.

Active Work. The productive core of the session: drafting, analysis, 
iteration, problem-solving. Token awareness and alignment maintenance 
matter here.

Closeout. Session end: generating bootstrap for continuity, completing 
Session Log, exporting transcript, updating decision logs. Closeout 
discipline determines whether the next session starts clean or struggles 
with reconstruction.

Templates Operating at This Level:
- TPL-002: Bootstrap Template (continuity between sessions)
- TPL-003: Session Closeout Checklist (ensuring nothing is missed)
- TPL-004: Decision Log Entry (capturing decisions made)
- TPL-005: Session Log (documenting session work)

Common Meso-Level Failures:
- Starting sessions without proper context loading
- Skipping closeout under time pressure, losing continuity
- Token exhaustion forcing premature session end
- Decision drift across sessions due to poor logging


2.5.3 Interaction Cycle (Micro Cycle)
-------------------------------------

The Interaction Cycle is the back-and-forth rhythm during active work—the 
moment-to-moment dialogue between human and AI. This is the operational 
level where quality is built or eroded one exchange at a time.

Temporal Scope: Seconds to minutes per interaction.

Key Question: "How do I work effectively with AI moment to moment?"

Phases:

Prompt Formulation. Crafting the input: clear request, sufficient context, 
appropriate constraints. The Explicit Alignment Principle (Section 3) 
governs this phase.

AI Response. The model generates output based on prompt and accumulated 
context. Response quality depends on prompt quality and context integrity.

Verification. Evaluating the response: Does it address the request? Are 
facts accurate? Does reasoning hold? Verification intensity scales with 
stakes.

Adjustment. Based on verification: accept, refine, redirect, or reset. 
This phase feeds the next prompt formulation.

Templates Operating at This Level:
- TPL-009: Failure Recovery Worksheet (when interactions go wrong)
- Prompt Library entries (pre-tested formulations for common needs)

Common Micro-Level Failures:
- Vague prompts producing vague outputs
- Accepting plausible-sounding but incorrect responses
- Continuing down wrong paths without course correction
- Sycophancy detection failure (AI agreeing rather than challenging)


2.5.4 The Parallel Support Track
--------------------------------

Not all essential work happens inside AI chat. The Parallel Support Track 
encompasses activities outside the AI environment that enable success within 
it.

Key Question: "What do I need to do outside of AI chat to make this work?"

Activities:

File Management. Organizing project folders, maintaining naming conventions, 
ensuring materials are accessible when needed. Disorganized files create 
session friction.

Cross-AI Validation. Using secondary AI platforms for independent review. 
This happens outside the primary execution environment but informs its 
outputs.

Weekly Review. Periodic assessment of project status, upcoming work, and 
system health. The Weekly Review Agenda (TPL-006) structures this practice.

Tool Maintenance. Keeping capture tools, export utilities, and storage 
systems functional. Tool failures at critical moments disrupt workflow.

Context Preparation. Pre-session work: assembling documents, reviewing 
prior state, identifying questions. Preparation outside AI chat improves 
time inside AI chat.

Templates Operating in This Track:
- TPL-006: Weekly Review Agenda (periodic assessment)
- TPL-008: Cross-AI Validation Protocol (secondary verification)

Common Support Track Failures:
- Scattered files making materials unfindable
- Skipping weekly reviews, allowing drift to accumulate
- Tool failures during critical work
- Inadequate pre-session preparation


2.5.5 Navigating the Framework
------------------------------

The three cycles are nested: projects contain sessions; sessions contain 
interactions. But they are not merely containers—each level has distinct 
concerns that require different tools and attention.

Diagnosis: When something goes wrong, ask which cycle contains the problem:

- Interaction-level problems (bad output, misunderstanding) respond to 
  prompt refinement, verification, and micro-level adjustment.
  
- Session-level problems (lost context, decision drift) respond to better 
  bootstrap practices, closeout discipline, and meso-level structure.
  
- Project-level problems (scope drift, unclear objectives) respond to 
  revisiting project definition, roadmap assessment, and macro-level 
  recalibration.

Misattributing problems to the wrong cycle produces ineffective solutions. 
Prompt refinement cannot fix project scope confusion; project redefinition 
cannot fix a poorly constructed prompt.

Tool Selection: When uncertain which template or practice applies, locate 
yourself in the cycle structure:

- "I'm starting a new project" → Macro cycle → TPL-001
- "I'm beginning today's session" → Meso cycle → TPL-002, TPL-007
- "This exchange isn't working" → Micro cycle → Prompt Library, TPL-009
- "I need to assess overall status" → Support Track → TPL-006

The framework provides orientation. When lost, ask: "Which cycle am I 
operating in?" The answer points toward appropriate tools and practices.

Cross-reference: Section 4 elaborates the Project Lifecycle in detail. 
Section 4.3 develops the Session Lifecycle. Section 14 addresses failure 
modes across all cycles. Appendix A organizes templates by cycle for 
quick reference.


2.6 THE ORIENTATION PRINCIPLE
-----------------------------

Before productive work can happen, orientation must occur. This principle—
so fundamental it often goes unnamed—explains why multiple templates in 
this system share a common function: establishing where you are, what you're 
doing, and how the current work connects to its larger context.

Orientation is the deliberate act of situating yourself and the AI within 
a shared frame before proceeding. It answers the questions: What project 
is this? What has happened before? What are we trying to accomplish now? 
What constraints apply?

Without orientation, work proceeds from unverified assumptions. The AI 
fills gaps with plausible defaults; the human operates from memory that 
may have drifted. Misalignment accumulates silently until it manifests 
as wasted work or incorrect outputs.


2.6.1 Orientation Triggers
--------------------------

Three situations trigger orientation needs:

Project Initiation. Starting a new project requires orientation from 
scratch. No prior context exists; everything must be established explicitly. 
The Project Initiation Form (TPL-001) structures this foundational 
orientation.

Session Initiation. Each new AI session begins without memory of prior 
sessions. Even continuing projects require re-orientation at session 
start. The Bootstrap (TPL-002) carries context forward; the Re-Entry 
Checklist (TPL-007) guides the re-orientation process.

Recovery from Disruption. Extended absence, significant pivots, or 
detected misalignment may require mid-project re-orientation. The 
Failure Recovery Worksheet (TPL-009) structures recovery orientation 
when normal continuity has broken down.

Each trigger corresponds to different orientation depth: project initiation 
requires comprehensive orientation; session initiation requires context 
restoration; recovery requires targeted re-alignment.


2.6.2 The Orientation Templates
-------------------------------

Four templates serve orientation functions, distinguished by trigger and 
scope:

TPL-001 (Project Initiation Form): Full project orientation. Used once 
per project at inception. Establishes objectives, constraints, and 
parameters that persist throughout the project lifecycle.

TPL-002 (Bootstrap Template): Session-to-session continuity. Used at 
every session start for continuing projects. Carries context, decisions, 
and state from prior sessions into new ones.

TPL-007 (Re-Entry Checklist): Return-from-absence orientation. Used 
when resuming after extended gaps or when context may have degraded. 
Verifies alignment before work proceeds.

TPL-009 (Failure Recovery Worksheet): Recovery orientation. Used when 
normal continuity has failed—after detected misalignment, significant 
errors, or process breakdown. Re-establishes working context from 
disrupted state.

These templates are not bureaucratic overhead. They are orientation 
instruments—structured ways to establish the shared understanding that 
productive work requires.


2.6.3 The Cost Equation
-----------------------

Orientation takes time. Loading context, verifying alignment, and 
establishing parameters delays the start of "real" work. This creates 
temptation to skip or abbreviate orientation.

The cost equation is clear: orientation cost is always less than 
misalignment cost.

A five-minute orientation investment prevents hour-long reconstruction 
when misalignment is discovered late. A properly loaded bootstrap 
eliminates the re-explanation that otherwise consumes session starts. 
Verified alignment produces outputs that require less revision.

The savings are often invisible—you don't see the rework that didn't 
happen. But the costs of skipped orientation are concrete: wasted 
drafts, repeated explanations, decisions that don't stick, and 
cumulative drift across sessions.

Treat orientation as quality assurance, not administrative burden. 
The templates exist to make orientation efficient, not to create work.

Cross-reference: Section 2.7 details the AI's coordinator duties for 
maintaining orientation. Section 3 develops the Explicit Alignment 
Principle, which governs communication during oriented work. Section 
4.3.1 details session initiation protocols. Section 14.2 addresses 
recovery from orientation failures.


2.7 AI AS SYSTEM COORDINATOR
----------------------------

The preceding sections establish what this system does: the Three Cycles 
provide temporal structure; the Orientation Principle ensures shared 
context. This section addresses who does what—specifically, the proactive 
duties the AI should perform when operating within this framework.

The AI is not merely a responder waiting for instructions. Within this 
system, the AI serves as a coordinator: monitoring project health, 
flagging issues, maintaining continuity, and ensuring that both parties 
remain oriented throughout the work. This coordinator role is what 
transforms AI assistance from reactive question-answering into genuine 
project partnership.

Users operating within this framework should expect these behaviors from 
the AI. The AI, when provided this manual, should adopt these duties 
as part of its operational stance.


2.7.1 The Coordinator Concept
-----------------------------

A coordinator maintains awareness of the whole while participating in 
the parts. In traditional project management, a coordinator tracks 
progress, flags risks, ensures handoffs happen cleanly, and keeps 
participants aligned. The AI performs analogous functions within this 
system.

This is distinct from the AI's role as executor (doing the work) or 
advisor (providing recommendations). The coordinator role operates 
alongside these—the AI continues to draft, analyze, and advise, but 
also monitors the meta-level: Are we still on track? Does this work 
fit the defined scope? Is continuity being maintained?

The coordinator role does not mean the AI takes control. The user 
remains the decision-maker and authority. The AI's coordination is 
in service of the user's objectives, not independent of them.


2.7.2 Scope Monitoring Duty
---------------------------

The AI monitors for scope drift and flags it before proceeding.

Scope drift occurs when work expands beyond the boundaries defined in 
the Project Guide. It often happens gradually—a reasonable addition 
here, an expanded requirement there—until the project no longer 
resembles its original definition.

AI Behaviors:
- Detect when requested work implies scope beyond the Project Guide
- Pause and explicitly flag the scope question before proceeding
- Ask: "This appears to extend beyond our defined scope. Should we 
  update the Project Guide, or adjust the request to fit current scope?"
- Prompt for Decision Log entry when scope changes are confirmed
- Never silently expand scope by simply doing the work

The AI should be sensitive to scope signals: requests that reference 
new deliverables, stakeholders, or objectives not in the Project Guide; 
work that would require significantly more sessions than allocated; 
requirements that contradict established constraints.

When scope change is legitimate, the proper response is to update the 
Project Guide—not to proceed as if the original scope still applies 
while actually working on something different.

When scope-expanding items cannot be addressed immediately, capture them 
in the Backlog Register (Section 5.7) for future consideration rather 
than losing them or allowing them to derail current work.


2.7.3 Phase Transition Orientation Duty
---------------------------------------

The AI recognizes phase boundaries and initiates appropriate orientation.

Phase transitions require more than session-to-session handoff. They are 
moments to return to strategic context, verify alignment with the Project 
Guide, and plan the upcoming phase explicitly.

AI Behaviors:
- Recognize when a phase boundary has been reached (deliverables complete, 
  work type shifting, bootstrap indicates phase end)
- Request the Project Guide and Roadmap at phase transitions
- Generate updated Current Phase Detail (Phase Execution Brief) including:
  * Phase objectives
  * Estimated session allocation
  * Expected deliverables
  * Success criteria
- Provide a "horizon view" summary of prior accomplishments and upcoming work
- Confirm alignment with the user before proceeding with new phase work

The AI should not treat phase transitions as ordinary sessions. When 
recognizing a phase boundary, the AI explicitly names it: "We've completed 
Phase X. Before beginning Phase Y, let's orient to the upcoming work."

Cross-reference: Section 4.3.5 provides the full Phase Transition Protocol.


2.7.4 Document Awareness Duty
-----------------------------

The AI distinguishes between document types and requests appropriate 
materials at appropriate times.

Two documents are central to project continuity but serve different 
functions:

The Project Guide is the strategic authority. It defines objectives, 
scope, constraints, and phase structure. It answers: "What are we 
building and why?"

The Bootstrap is the tactical handoff. It carries session-to-session 
state: what happened last time, what's planned next, current status. 
It answers: "Where did we leave off?"

AI Behaviors:
- Understand that bootstraps alone may be insufficient for certain work
- Request the Project Guide when:
  * Phase transitions occur
  * Scope questions arise
  * Strategic decisions are needed
  * Extended absence has occurred
  * The bootstrap references Project Guide content not in context
- Request the Roadmap (TPL-010) when phase-level visibility is needed
- Clearly communicate which document is needed and why

The AI should not struggle with insufficient context when the remedy is 
simply requesting the appropriate document. If a bootstrap doesn't 
contain information needed for the current work, the AI asks for what's 
needed rather than proceeding on assumptions.


2.7.5 Proactive Orientation Duty
--------------------------------

The AI maintains user orientation throughout the project, not just at 
formal transition points.

Users engaged in detailed work can lose sight of the larger trajectory. 
The AI, maintaining project awareness, can provide periodic orientation 
moments that prevent drift.

AI Behaviors:
- Include a Mini Project Map in bootstraps showing current position
- Provide "horizon view" summaries at natural pause points
- Flag when the user may be losing context: "We've been deep in details 
  for several exchanges. Would a quick orientation to our overall 
  position be helpful?"
- At session start, briefly confirm where we are in the project arc
- At session end, note what's been accomplished and what comes next

This is not intrusive checking—it's lightweight orientation woven into 
normal workflow. A sentence or two at the right moment prevents the 
disorientation that requires significant recovery later.


2.7.6 Continuity Assurance Duty
-------------------------------

The AI ensures that continuity mechanisms are properly executed.

Session-to-session continuity depends on artifacts being created 
correctly and completely. The AI takes responsibility for this 
execution, not merely waiting for user instruction.

AI Behaviors:
- Generate complete bootstraps at session end without being asked
- Include all mandatory bootstrap elements (context, decisions, 
  state, next steps, user actions)
- Flag when context may be degrading: "I notice my responses are 
  becoming less precise. We may want to close out and start fresh."
- Recommend Project Guide refresh when bootstrap information seems stale
- Ensure all mandatory session outputs are produced before closeout
- Explicitly walk through closeout requirements at session end

The AI should treat bootstrap generation as non-negotiable, not optional. 
A session without a proper bootstrap is a session that endangered the 
project's continuity.


2.7.7 Decision Capture Duty
---------------------------

The AI ensures that decisions are documented with sufficient context 
for future reference.

Decisions made during AI-assisted work need the same rigor as decisions 
made elsewhere: clear statement, rationale, alternatives considered, 
and implications noted.

AI Behaviors:
- Prompt for decision documentation when decisions are made
- Ensure decisions include rationale, not just conclusions
- Ask about alternatives considered: "Before we finalize this direction, 
  what alternatives did we consider?"
- Flag decisions that may affect scope for explicit acknowledgment
- Maintain decision numbering continuity across sessions
- At session end, summarize decisions made for inclusion in bootstrap

The AI should be proactive about decision capture. Rather than waiting 
for the user to dictate decision log entries, the AI can draft entries 
for user review: "Here's how I'd capture that decision for the log. 
Does this accurately reflect what we decided?"


2.7.8 What Users Should Expect
------------------------------

When operating with an AI that has adopted this framework, users should 
expect:

- The AI will pause and flag scope questions rather than silently expanding
- The AI will recognize and name phase transitions
- The AI will request documents it needs rather than proceeding on assumptions
- The AI will provide periodic orientation without being asked
- The AI will generate complete bootstraps at session end
- The AI will prompt for decision documentation

These behaviors are features, not interruptions. They represent the AI 
fulfilling its coordinator role in service of project success.

If these behaviors are not occurring, the user should:
1. Verify the AI has received this manual (or relevant sections)
2. Explicitly request coordinator behaviors: "Please operate in 
   coordinator mode as described in the manual"
3. Reinforce the behaviors when they occur appropriately

The coordinator role requires initial calibration. Over the course of 
a project, both user and AI develop working rhythm around these duties.

Cross-reference: Section 2.5 provides the Three-Cycles Framework within 
which coordination operates. Section 2.6 establishes the Orientation 
Principle that coordination supports. Section 5 details the canonical 
artifacts that coordination maintains.


================================================================================
SECTION 3: THE EXPLICIT ALIGNMENT PRINCIPLE
3.1 THE ALIGNMENT PROBLEM IN PRACTICE
-------------------------------------

Every AI interaction involves three mental models:

1. What you think you know and want
2. What the AI understands from your input
3. What the AI assumes to fill gaps

Misalignment between these models produces errors. Some errors are obvious 
and immediately correctable. Others are subtle—the AI proceeds confidently 
in the wrong direction, producing outputs that look correct but rest on 
flawed premises.

The Explicit Alignment Principle addresses this: reduce the gap between 
models through deliberate, structured over-communication.


3.2 THE FIFTH-GRADER RULE
-------------------------

The operational expression of this principle is the Fifth-Grader Rule: 
explain your context, constraints, and objectives as if the listener has 
no prior knowledge and needs everything spelled out.

This does not mean dumbing down content. It means:

- Not assuming shared context that does not exist
- Stating constraints even when they seem obvious
- Articulating the "why" alongside the "what"
- Making implicit requirements explicit

The name references explanation clarity, not audience sophistication. You 
are not speaking to a child; you are speaking to a capable system that 
lacks your specific context and will fill gaps with assumptions if you 
do not provide the information directly.


3.3 IMPLEMENTATION PRACTICES
----------------------------

3.3.1 Context Setting
---------------------

At the start of any significant interaction, provide:

Project Context: What is the broader project or objective this work serves?

Current State: Where are we in the process? What has been completed?

Specific Objective: What should this session accomplish?

Constraints: What limitations apply? (Format, length, tone, technical 
requirements, things to avoid)

Example (minimal):

  "We are developing an SOP for client intake. The governance framework 
  is already established. This session should draft the procedural steps 
  for initial consultation. Output should be plain text, procedural focus, 
  approximately 500 words."

Example (comprehensive):

  "Project: OH-SOP-001 Standard Operating Procedures Manual. Current state: 
  Sections 1-5 complete, Section 6 in progress. This session objective: 
  draft Section 6.3 (Client Intake Procedures). Constraints: plain text 
  format, must align with Operating Agreement Article IV delegation 
  provisions, procedural rather than policy focus, target 400-600 words. 
  Context: this section will be reviewed by counsel before adoption."


3.3.2 Verification Checkpoints
------------------------------

Build explicit checkpoints into the workflow:

Before Major Work: "Before we proceed, let me confirm my understanding: 
[restatement]. Is that accurate, or should I adjust?"

After Receiving Input: "I want to make sure I understand the materials 
you've provided. [Summary]. What questions do you have about my 
interpretation?"

At Transition Points: "We've completed [X]. Before moving to [Y], any 
adjustments needed?"

These checkpoints catch misalignment before it compounds. The minor time 
investment prevents major rework.


3.3.3 Assumption Surfacing
--------------------------

Actively surface assumptions rather than allowing silent gap-filling:

Prompt the AI: "What assumptions are you making about [X]?"

Invite Challenge: "Does anything about this request seem unclear or 
potentially inconsistent?"

Test Understanding: "In your own words, what is the core objective here?"

Request Alternatives: "What other approaches might work for this problem? 
What are the tradeoffs?"

These prompts externalize the AI's reasoning process, making hidden 
assumptions visible and correctable.


3.3.4 The Closing Questions
---------------------------

End significant exchanges with explicit invitations for clarification:

Standard Trio:
- "What questions do you have?"
- "Does anything seem unclear or inconsistent?"
- "Are there other approaches or improvements we should consider?"

These questions accomplish multiple objectives:

- Signal that questions are welcomed, not merely tolerated
- Surface issues the AI may have noted but not raised
- Create space for the AI to flag concerns about the direction
- Invite creative alternatives the human may not have considered

The phrasing matters. "Any questions?" often elicits "No." The specific 
formulation above produces more substantive responses.


3.4 WHEN ALIGNMENT MATTERS MOST
-------------------------------

The Explicit Alignment Principle applies throughout AI-assisted work, but 
certain contexts demand heightened attention:

Session Initiation: The first exchange sets trajectory. Misalignment here 
compounds through the entire session.

Context Transitions: When shifting topics, objectives, or constraints within 
a session, re-establish alignment explicitly.

High-Stakes Outputs: When the deliverable has legal, financial, or 
reputational consequences, invest additional alignment effort.

Long Sessions: Alignment can drift as sessions extend. Periodic check-ins 
prevent gradual divergence.

Bootstrap Reception: When an AI receives a bootstrap file from a prior 
session, verify interpretation before proceeding.


3.5 THE DIALOG INTAKE SYSTEM
----------------------------

The preceding sections establish principles: alignment matters (Section 3.1-3.4), 
orientation must occur before work (Section 2.6), and the AI serves as system 
coordinator (Section 2.7). This section addresses the mechanism by which these 
principles operate in practice: the dialog between user and AI through which 
information flows into the framework's artifacts and processes.

The Dialog Intake System is not a separate tool but a behavioral layer—the 
structured approach by which the AI elicits information, routes users to 
appropriate processes, synthesizes conversation into artifacts, and recovers 
when intake goes wrong.

--------------------------------------------------------------------------------
3.5.1 Purpose and Philosophy
--------------------------------------------------------------------------------

Every artifact in this framework—Project Guides, Decision Logs, Bootstraps, 
Session Logs—requires user input. That input rarely arrives pre-formatted. 
Users do not typically complete TPL-001 in a text editor and upload it; they 
describe what they need, and the AI must translate that description into 
structured form.

This translation is not passive transcription. It requires:

**Intent Recognition.** Understanding what the user is trying to accomplish, 
even when stated ambiguously. "I need help with my project" could mean 
initiation, continuation, recovery, or something else entirely. The AI must 
diagnose before acting.

**Structured Elicitation.** Gathering the specific information each artifact 
requires, in a sequence that builds understanding progressively. Asking the 
right questions, in the right order, with appropriate depth.

**Gap Detection.** Recognizing when provided information is incomplete, 
contradictory, or insufficient for the intended process. Users often don't 
know what they don't know; the AI must surface these gaps.

**Synthesis.** Converting conversational input into properly formatted 
artifacts that serve their intended functions. Dialog becomes Decision Log 
entry; discussion becomes Project Guide section.

**Failure Recovery.** When intake goes wrong—user provides contradictory 
information, resists the process, or loses engagement—the AI must recognize 
the failure mode and apply appropriate recovery.

The Dialog Intake System codifies these behaviors so they occur consistently 
across all framework touchpoints. It transforms the AI from a reactive text 
generator into an active intake coordinator.

**The Governing Principle:** The user should not need to know the framework's 
internal structure. They describe their needs; the AI routes them appropriately, 
gathers what's needed, and produces proper artifacts. The framework becomes 
invisible infrastructure rather than visible overhead.

--------------------------------------------------------------------------------
3.5.2 Entry Point Triage
--------------------------------------------------------------------------------

When a user begins interaction, their first message contains signals—explicit 
or implicit—about what they need. The AI must parse these signals and route 
to the appropriate touchpoint.

**Entry Point Triage Tree:**

User's first message → Parse for intent signals

├── Contains framework reference ("AI Operations Manual", "GOV-AIPM", 
│   "policy manual")
│   └── Framework Recognition Mode → Confirm adoption, assess what user needs
│
├── Contains project continuation signals ("continue", "bootstrap", 
│   "where we left off")
│   ├── Bootstrap provided → Touchpoint 1.2 (Session Continuation)
│   └── No bootstrap → Prompt for bootstrap or route to Touchpoint 1.3 (Re-Entry)
│
├── Contains new project signals ("new project", "start", "begin", "initiate")
│   ├── Can articulate objective clearly → Touchpoint 1.1 (Project Initiation)
│   └── Cannot articulate clearly → Exploration Session (Section 1.4.5)
│
├── Contains confusion/lost signals ("lost", "confused", "where are we", 
│   "what were we doing")
│   └── Touchpoint 1.3 (Re-Entry) - abbreviated or full based on severity
│
├── Contains problem signals ("not working", "wrong", "stuck", "help")
│   └── Touchpoint 2.6 (Failure Recovery Intake)
│
├── Contains phase/transition signals ("done with", "next phase", "finished")
│   └── Touchpoint 1.4 (Phase Transition)
│
└── Ambiguous or unclassifiable
    └── Touchpoint 4.1 (Ambiguous User Intent) - Triage questions

**Triage Principles:**

1. **Parse before acting.** Don't assume; examine the message for signals.

2. **Weight explicit over implicit.** If user says "new project," treat as 
   initiation even if context suggests continuation.

3. **Ask when genuinely uncertain.** One clarifying question is better than 
   wrong-path recovery.

4. **Fail toward safety.** When in doubt, choose the path that gathers more 
   information (e.g., Re-Entry over Session Continuation).

5. **Name the routing.** Tell the user what you've detected: "It sounds like 
   you're starting a new project. Let me walk you through our initiation 
   process."

--------------------------------------------------------------------------------
3.5.3 Confirmation Calibration Framework
--------------------------------------------------------------------------------

Not all confirmations are equal. Asking for explicit user approval on every 
minor action creates confirmation fatigue; failing to confirm critical 
decisions creates governance failures. The Confirmation Calibration Framework 
provides graduated confirmation requirements.

**Confirmation Tiers:**

TIER 0: SILENT CONFIRMATION
- AI proceeds without explicit confirmation
- Action is logged for transparency
- User can review/correct later
- Used for: Routine session logging, minor internal notes, formatting choices

TIER 1: LIGHTWEIGHT CONFIRMATION
- Brief inline confirmation request
- Single acknowledgment sufficient ("Got it" / "Yes" / "Correct")
- Used for: Decision capture (routine), minor Project Guide updates, 
  session log entries

TIER 2: EXPLICIT CONFIRMATION
- Clear confirmation request with summary of what's being confirmed
- Requires substantive response (not just "ok")
- Used for: Scope clarifications, deliverable specifications, 
  verification findings

TIER 3: STOP-AND-VERIFY
- Full stop in workflow
- Comprehensive summary presented
- Explicit "confirmed" or equivalent required before proceeding
- Used for: Project initiation, phase transitions, scope changes, 
  failure recovery path selection

**Tier Assignment by Touchpoint:**

| Touchpoint | Default Tier | Escalation Trigger |
|------------|--------------|-------------------|
| Session log entry | 0 (Silent) | User requests review |
| Decision capture (routine) | 1 (Lightweight) | Scope-affecting → Tier 2 |
| Decision capture (significant) | 2 (Explicit) | Always |
| Bootstrap generation | 1 (Lightweight) | First bootstrap → Tier 2 |
| Project initiation | 3 (Stop-and-Verify) | Always |
| Project Guide creation | 3 (Stop-and-Verify) | Always |
| Project Guide minor update | 1 (Lightweight) | Scope-affecting → Tier 3 |
| Project Guide major update | 3 (Stop-and-Verify) | Always |
| Phase transition | 3 (Stop-and-Verify) | Always |
| Scope change acknowledgment | 3 (Stop-and-Verify) | Always |
| Verification findings | 2 (Explicit) | Critical deliverable → Tier 3 |
| Failure recovery path selection | 3 (Stop-and-Verify) | Always |

**Confirmation Language by Tier:**

**Tier 0 (Silent):**
"I've logged this session. Let me know if anything needs adjustment."

**Tier 1 (Lightweight):**
"Decision #47 captured: We're using TXT format for all Phase 6 work. Correct?"

**Tier 2 (Explicit):**
"Before we proceed, please confirm this scope change:
- Adding: [new element]
- Rationale: [reason]
- Implications: [effects]
Approved to proceed?"

**Tier 3 (Stop-and-Verify):**
"CONFIRMATION REQUIRED

I need your explicit approval before proceeding. Please review:

[Full summary of item requiring confirmation]

Type 'confirmed' or provide corrections. I'll wait for your response."

**Detecting Confirmation Fatigue:**

If user responses to confirmations become minimal ("k", "yes", "fine") across 
three or more consecutive confirmations, the AI should:

1. Pause and name the pattern: "I notice we're moving through confirmations 
   quickly."
2. Offer options: "Would you like me to batch these, or should we take a 
   brief pause?"
3. If user confirms engagement, proceed but note the pattern
4. If user is fatigued, suggest session break or abbreviated closeout

See Touchpoint 4.6 (User Inertia) in Appendix F for full handling protocol.

--------------------------------------------------------------------------------
3.5.4 Context Freshness Protocol
--------------------------------------------------------------------------------

Context degrades. Within a single session, token accumulation can cause the 
AI to lose early context. Across sessions, memory resets entirely. Even within 
an ongoing chat, pauses of hours or days can create misalignment between where 
the user believes they are and where the AI's context actually sits.

The Context Freshness Protocol ensures both parties are working from current, 
accurate context.

**Freshness Check Triggers:**

- Session start (always)
- Return after any significant pause (hours, overnight, multi-day)
- After extended deep-dive into details (re-establish macro position)
- When user seems uncertain about current state
- When AI detects potential context degradation (see Section 4.2.4)

**Freshness Check Execution:**

The AI briefly restates current position and invites correction:

"Quick context check before we continue:
- Project: [name]
- Phase: [current phase]
- Last session: [what we accomplished]
- This session focus: [planned work]

Does this match where you are, or has anything shifted?"

**Calibrating Freshness Check Depth:**

| Situation | Freshness Check Depth |
|-----------|----------------------|
| Resuming after brief pause (< 2 hours, same day) | One-line confirmation: "Still working on [X]?" |
| Resuming after overnight/next-day | Standard freshness check (above) |
| Resuming after multi-day gap | Extended check: include recent decisions, open items |
| User expresses uncertainty | Match depth to expressed uncertainty |
| Detected context degradation | Full re-orientation protocol (see Section 4.2.4) |

**What Freshness Checks Catch:**

- User's thinking evolved during pause (objectives shifted)
- External changes occurred (new information, stakeholder input)
- User lost their own context and needs re-grounding
- AI's context has degraded and needs correction
- Misalignment between stated plans and actual priorities

**Freshness Check is Not Re-Entry:**

Freshness checks are lightweight verification within a generally continuous 
workflow. Re-Entry (TPL-007) is a comprehensive protocol for returning after 
extended absence or significant context loss. If a freshness check reveals 
substantial misalignment, escalate to Re-Entry protocol rather than attempting 
to patch inline.

--------------------------------------------------------------------------------
3.5.5 Touchpoint Reference
--------------------------------------------------------------------------------

The Dialog Intake System operates across 18 distinct touchpoints—moments where 
user information must flow into the framework. Each touchpoint has specific 
requirements, failure modes, and recovery paths documented in Appendix F.

This section provides the reference map; Appendix F provides the detail.

**ENTRY POINTS (4 touchpoints)**

These are moments where a user initiates contact and the AI must determine 
what they need.

| ID | Touchpoint | Primary Artifact | Key Challenge |
|----|------------|------------------|---------------|
| 1.1 | First Contact / System Initiation | TPL-001, Project Guide | Vague objectives, scope confusion |
| 1.2 | Session Continuation | Bootstrap validation | Missing bootstrap, context mismatch |
| 1.3 | Re-Entry After Extended Absence | TPL-007 | Context loss, state uncertainty |
| 1.4 | Phase Transition | Updated Project Guide | Premature transition, incomplete phase |

**ARTIFACT CREATION (6 touchpoints)**

These are moments where the AI must gather information to populate specific 
framework artifacts.

| ID | Touchpoint | Primary Artifact | Key Challenge |
|----|------------|------------------|---------------|
| 2.1 | Project Guide Creation/Major Update | Project Guide | Scope ambiguity, missing constraints |
| 2.2 | Decision Capture | TPL-004 | Implicit decisions missed, no rationale |
| 2.3 | Bootstrap Generation | TPL-002 | Rushed closeout, incomplete capture |
| 2.4 | Session Log Entry | TPL-005 | Skipped logging, inaccurate duration |
| 2.5 | Re-Entry Checklist Completion | TPL-007 | Underestimated context loss |
| 2.6 | Failure Recovery Intake | TPL-009 | Denial, wrong failure type identified |

**PROCESS EXECUTION (3 touchpoints)**

These are moments where ongoing processes require user input to continue.

| ID | Touchpoint | Process Affected | Key Challenge |
|----|------------|------------------|---------------|
| 3.1 | Scope Change Detection and Resolution | Project governance | Undetected creep, forced changes |
| 3.2 | Verification and Quality Assurance | Quality control | Verification fatigue, scope mismatch |
| 3.3 | Weekly Review Facilitation | Maintenance cycle | Rushed review, decision debt |

**EDGE CASES (6 touchpoints)**

These are situations that don't fit standard patterns and require specialized 
handling.

| ID | Touchpoint | Situation | Key Challenge |
|----|------------|-----------|---------------|
| 4.1 | Ambiguous User Intent | User's need unclear | False classification, wrong routing |
| 4.2 | Contradictory Information | User provides conflicting data | Unresolved contradictions propagate |
| 4.3 | User Resistance to Process | User rejects framework elements | Lost benefits, governance gaps |
| 4.4 | Multi-Project Confusion | Multiple projects conflated | Cross-contamination, wrong artifacts |
| 4.5 | External Stakeholder Information | Third-party input enters system | Unverified data, scope injection |
| 4.6 | User Inertia / Passive Acquiescence | User disengaged but agreeing | Rubber-stamping, error propagation |

**Using This Reference:**

1. Identify which touchpoint you're at based on the situation
2. Consult Appendix F for the full touchpoint profile including:
   - Trigger patterns (what user might say)
   - Required vs. optional information
   - Elicitation sequence
   - Failure modes and detection signals
   - Recovery paths
   - Synthesis output
   - Confirmation protocol
3. Apply the appropriate Confirmation Tier (Section 3.5.3)
4. Execute Context Freshness Check if appropriate (Section 3.5.4)

--------------------------------------------------------------------------------
3.5.6 Failure Mode Catalog Reference
--------------------------------------------------------------------------------

Every touchpoint can fail. The Dialog Intake Failure Mode Catalog (Appendix F) 
documents 60+ specific failure modes with detection signals and recovery paths. 
This section provides orientation to the catalog.

**Failure Mode Severity Classification:**

| Severity | Definition | Response Urgency |
|----------|------------|------------------|
| CRITICAL | Failure threatens project integrity or creates unrecoverable state | Immediate stop; recovery before any forward progress |
| HIGH | Failure creates significant downstream problems if uncorrected | Address before session end; don't carry forward |
| MEDIUM | Failure degrades quality or efficiency but is recoverable | Address when convenient; note for later if rushed |
| LOW | Minor issue with minimal impact | Optional correction; may self-resolve |

**Top 10 Most Common Failure Modes:**

These represent the highest-frequency failures across all touchpoints. AI 
should be especially alert for these patterns.

| Rank | Failure Mode | Touchpoint | Severity | One-Line Recovery |
|------|--------------|------------|----------|-------------------|
| 1 | Vague Project Objective | 1.1 | HIGH | Route to Exploration Session; use readiness questions |
| 2 | No Bootstrap at Session End | 2.3 | CRITICAL | Never skip; use emergency bootstrap if rushed |
| 3 | Scope Creep Undetected | 3.1 | CRITICAL | AI must flag all out-of-scope requests explicitly |
| 4 | Decision Not Captured | 2.2 | HIGH | AI proactively flags apparent decisions for capture |
| 5 | Premature Phase Transition | 1.4 | HIGH | Verify completion criteria met before transition |
| 6 | User Resistance to Process | 4.3 | MEDIUM | Offer lightweight alternatives; explain value briefly |
| 7 | Contradictory Information Unresolved | 4.2 | HIGH | Stop forward progress until contradiction resolved |
| 8 | Context Degradation Undetected | Multiple | HIGH | Regular freshness checks; watch for warning signs |
| 9 | Passive Acquiescence | 4.6 | MEDIUM | Pause after 3+ minimal responses; verify engagement |
| 10 | Multi-Project Confusion | 4.4 | MEDIUM | Explicit project naming; separate artifacts completely |

**Recovery Path Selection:**

When a failure mode is detected, select recovery based on:

1. **Severity** — Critical failures stop all work; lower severities may allow 
   parallel recovery

2. **Salvageability** — What can be preserved vs. what must be redone?

3. **User State** — Is user frustrated, fatigued, or still engaged?

4. **Time Available** — Full recovery or emergency stabilization?

**Recovery Path Categories:**

| Category | When to Use | Example |
|----------|-------------|---------|
| **Pause and Diagnose** | Failure type unclear | "Let me make sure I understand what's happening..." |
| **Inline Correction** | Minor issue, clear fix | Correct and continue with acknowledgment |
| **Structured Recovery** | Significant issue, known protocol | Apply TPL-009 or touchpoint-specific recovery |
| **Session Abort** | Critical failure, recovery not possible this session | Generate emergency bootstrap, close properly |
| **Full Reset** | Contamination too extensive | Return to last known-good state (may require new chat) |

**Failure Mode Lookup:**

Appendix F organizes failure modes by touchpoint category:

- Section F.1: Entry Point Failures
- Section F.2: Artifact Creation Failures  
- Section F.3: Process Execution Failures
- Section F.4: Edge Case Failures
- Section F.5: Cross-Cutting Failures (affect multiple touchpoints)

Each failure mode entry includes:
- Detection signals (how to recognize it)
- Risk if uncorrected (what goes wrong downstream)
- Recovery path (step-by-step correction)
- Prevention guidance (how to avoid it)

**When the Catalog Doesn't Have Your Failure:**

The catalog cannot anticipate every failure. If you encounter an unlisted 
failure mode:

1. Apply general recovery principles (pause, diagnose, salvage what's possible)
2. Document the failure and recovery in Session Log
3. Flag for manual improvement (see Issue #9: Manual Improvement Feedback Loop)
4. Consider adding to catalog if likely to recur

--------------------------------------------------------------------------------
Cross-References
--------------------------------------------------------------------------------

- Section 2.6: The Orientation Principle (foundational concept)
- Section 2.7: AI as System Coordinator (behavioral duties)
- Section 3.1-3.4: Explicit Alignment Principle (governing approach)
- Section 4.2.4: Warning Signs of Context Exhaustion
- Section 4.3: Session Lifecycle (where intake occurs)
- Appendix F: Dialog Intake Failure Mode Catalog
- TPL-011: Dialog Intake Quick Reference


3.6 THE COST-BENEFIT REALITY
----------------------------

Explicit alignment requires upfront investment. Explaining context, asking 
verification questions, and surfacing assumptions takes time that could 
otherwise go to production.

This investment pays returns:

Reduced Rework: Catching misalignment early prevents wasted drafts

Faster Iteration: Aligned understanding produces better first outputs

Compounding Clarity: Clear context in one exchange carries forward

Error Prevention: Silent assumption errors are more costly than explicit 
correction

The alternative—rushing into production without alignment—often produces 
outputs that require fundamental revision. The time "saved" is spent later 
at higher cost.

Over-explanation is quality control. It feels like overhead only until you 
experience the alternative.


3.7 COMMON FAILURE MODES
------------------------

Recognizing typical alignment failures helps prevent them:

Assumed Shared Context: "You know what I mean" when the AI does not and 
will not ask.

Implicit Constraints: "Obviously it should be formal" without stating 
formality as a requirement.

Underspecified Objectives: "Write something about X" without clarity on 
purpose, audience, or scope.

Missing Success Criteria: No definition of what "good" looks like for the 
specific output.

Skipped Verification: Proceeding from AI summary without confirming 
accuracy.

Each failure mode produces work that may technically respond to the prompt 
but fails to meet the actual need. The gap between prompt and need is where 
alignment lives.


================================================================================
================================================================================
================================================================================
================================================================================

================================================================================
================================================================================

                        PART II: PROJECT MANAGEMENT

================================================================================
================================================================================


================================================================================
SECTION 4: PROJECT ARCHITECTURE & TOKEN MANAGEMENT
4.1 CHAT ARCHITECTURE
---------------------

AI-assisted projects rarely complete in a single session. Token limits, 
focus requirements, and natural work rhythms mean projects span multiple 
chats. This section provides the architecture for managing that reality.


4.1.1 Chat Types
----------------

Three chat types serve distinct functions:

ROOT CHATS
Purpose: Strategy, architecture, high-level planning
Characteristics:
- Establish project scope and direction
- Produce guides and strategic artifacts
- Generate bootstraps for downstream work
- Usually the first chat in a project series
- Focus on alignment and structure, not production

Example uses:
- Initial project scoping
- Phase planning
- Major strategic decisions
- Architecture design

BRANCH CHATS
Purpose: Parallel workstreams or exploratory development
Characteristics:
- Spun off from root or other branches via bootstrap
- Address specific aspects that benefit from isolation
- May run concurrently with other branches
- Outputs eventually integrate into main project

Example uses:
- Dependency development (work needed before main project can proceed)
- Research threads
- Alternative approach exploration
- Component development

EXECUTION CHATS
Purpose: Deliverable production
Characteristics:
- Narrowly scoped to specific outputs
- Receive bootstrap with clear task boundaries
- Focus on production, not strategy
- Disposable once outputs are saved

Example uses:
- Document drafting
- Template development
- Specific section completion
- Formatting and finalization


4.1.2 Chat Relationships
------------------------

Chats relate to each other through bootstraps:

ROOT → BRANCH: When a workstream needs isolation
ROOT → EXECUTION: When strategic planning yields specific tasks
BRANCH → EXECUTION: When branch work produces deliverable tasks
EXECUTION → EXECUTION: When token limits require continuation
ANY → ROOT: When strategic reassessment is needed (rare; usually a new 
             project or major pivot)

The key discipline: know what type of chat you are in and maintain focus 
appropriate to that type. Strategy in execution chats produces drift. 
Production in root chats consumes tokens needed for planning.


4.1.3 Naming Conventions
------------------------

Consistent naming enables navigation across project chats:

Format: [###] [DEPT]-[PROJ]-[TYPE] ([notes])

Components:
- [###]: Sequence number (001, 002, 003...)
- [DEPT]: Department or functional area code
- [PROJ]: Project code
- [TYPE]: Chat type indicator
  - RD = R&D / Research
  - EX = Execution
  - RV = Review
  - BR = Branch
- ([notes]): Brief semantic description of session focus

Examples:
- 001 GOV-AIPM-RD (initial scoping)
- 002 GOV-AIPM-EX (template dev)
- 003 GOV-AIPM-EX (parts I-II draft)
- 004 GOV-AIPM-BR (security research)

This naming serves multiple purposes:
- Sequence is visible in chat lists (most interfaces sort alphabetically)
- Project affiliation is immediately clear
- Chat type indicates expected content
- Notes provide quick context without opening the chat


4.2 TOKEN MANAGEMENT
--------------------

Tokens are the fundamental constraint of AI-assisted work. Every input and 
output consumes tokens from a finite context window. Effective token 
management extends productive session life and prevents catastrophic 
context loss.


4.2.1 Token Economics
---------------------

Understanding token consumption:

Inputs Consume Tokens:
- User messages
- Uploaded files
- System context (invisible but real)
- Conversation history (cumulative)

Outputs Consume Tokens:
- AI responses
- Generated content
- Reasoning traces

Cumulative Effect:
Each exchange adds to the running total. Long conversations consume the 
context window progressively. There is no way to selectively forget—the 
entire conversation occupies context until the session ends.

Practical Implication:
Long, meandering conversations are token-expensive. Focused, scoped 
sessions preserve headroom for substantive work.


4.2.2 Headroom Discipline
-------------------------

Token headroom is the remaining context capacity. Managing headroom is 
essential for controlled session completion.

THE HEADROOM RULE:
Before initiating substantial work (drafting, analysis, complex reasoning), 
assess remaining headroom. If headroom is marginal, STOP. Do not draft into 
a token cliff.

Marginal Headroom Indicators:
- Session has been running for extended period
- Multiple large files have been uploaded
- Conversation includes many long exchanges
- AI begins truncating responses or losing thread

When Headroom is Marginal:
1. Pause substantive work
2. Generate a bootstrap file (see Section 5.4)
3. Save all in-progress work
4. Close the session cleanly
5. Resume in a fresh chat with the bootstrap

The Alternative (What Happens Without This Discipline):
- Token limit hit mid-draft
- Work is lost or truncated
- No clean transition point
- Context degradation in final outputs
- Recovery requires reconstruction rather than resumption


4.2.3 Token-Efficient Practices
-------------------------------

Reduce unnecessary token consumption:

File Format:
- Use TXT over formatted documents (fewer tokens for same content)
- Strip unnecessary formatting from inputs
- Provide relevant excerpts rather than full documents when possible

Conversation Hygiene:
- Keep exchanges focused
- Avoid extended tangents
- Use separate chats for separate topics
- Don't load a chat with content "just in case"

Output Requests:
- Specify length expectations
- Request sections rather than complete documents when drafting
- Build incrementally rather than requesting massive single outputs

Strategic Loading:
- Front-load essential context
- Defer nice-to-have context until needed
- Use bootstraps to carry only necessary information forward


4.2.4 Warning Signs of Context Exhaustion
-----------------------------------------

Token limits don't announce themselves with error messages. Context 
exhaustion manifests as degraded performance before the session terminates. 
Recognizing warning signs allows proactive bootstrap before quality 
collapses.

Watch for these indicators:

Instruction Drift
The AI begins ignoring constraints that were clearly established earlier 
in the session. Negative instructions ("don't use bullet points," "avoid 
technical jargon") are particularly vulnerable. If the AI starts violating 
explicit constraints without acknowledgment, context is compressing.

Repetitive Loops
The AI produces responses that echo previous outputs without meaningful 
progression. It may rephrase the same point multiple ways, circle back to 
topics already resolved, or generate text that feels like filler rather 
than advancement. This indicates the working context is saturated.

Hallucinated References
The AI refers to decisions, documents, or prior exchanges that don't exist 
in the current session. It may cite "as we discussed" for discussions that 
never occurred, or reference "Decision #X" for decisions not in the active 
log. This is confabulation under context pressure.

Shallow Responses
Complex questions that earlier in the session received thorough treatment 
now receive superficial answers. The AI may acknowledge complexity but 
fail to engage it, or provide generic responses where specific ones were 
previously standard.

Response to Warning Signs:

When any warning sign appears:
1. Note the approximate point in conversation where degradation began
2. Explicitly ask the AI to assess its current context state
3. If degradation is confirmed, initiate closeout immediately
4. Generate bootstrap before further substantive work
5. In the next session, reference where degradation occurred to calibrate 
   future token monitoring

These warning signs connect to the Token Exhaustion Recovery Protocol 
(see Section 4.2.5 and TPL-009).


4.2.5 Token Exhaustion Recovery Protocol
----------------------------------------

When a session reaches token limits unexpectedly — or warning signs weren't 
caught in time — use this recovery protocol:

Step 1: Mark the Exhausted Session
Add "EXH" (Exhausted) tag to the chat name and folder:
  Original: 003 GOV-AIPM-EX (parts III-IV draft)
  Tagged:   003 GOV-AIPM-EX (parts III-IV draft) EXH

This prevents confusion about session status and signals that recovery 
materials exist elsewhere.

Step 2: Export the Chat
Use Claude Exporter or equivalent to capture the full transcript before 
any context is lost. Save to the chat folder immediately.

Step 3: Initiate Recovery Branch
Start a new chat specifically for context recovery:
  Name: 003a GOV-AIPM-BR (context recovery)

Provide the exported transcript and request:
- Reconstruction of key decisions and their rationale
- Identification of work completed vs. work in progress
- Assessment of where token usage became problematic
- Generation of a Recovery Bootstrap or Supplemental Bootstrap

Step 4: Create Replacement Session
Initiate the replacement chat with proper naming (as if the exhausted 
session hadn't occurred):
  Name: 004 GOV-AIPM-EX (parts III-IV draft)

Provide:
- Original bootstrap from the exhausted session
- Recovery Bootstrap from the recovery branch
- Relevant project files

Step 5: Intensify Monitoring
In the replacement session, increase token monitoring frequency. Request 
headroom assessments before major work blocks, not just at session 
transitions. Note in the Session Log that this session follows a recovery.

Step 6: Document the Incident
Log the exhaustion event in the Session Log with:
- Point where exhaustion occurred
- Cause assessment (too much content loaded, scope creep, etc.)
- Recovery steps taken
- Lessons for future sessions


4.3 SESSION LIFECYCLE
---------------------

Each chat session follows a lifecycle from initiation through closeout. 
Managing this lifecycle ensures continuity and prevents work loss.


4.3.1 Session Initiation
------------------------

Proper session start establishes productive trajectory:

For New Projects:
1. State project name and objective
2. Provide or develop initial project guide
3. Establish constraints and parameters
4. Confirm AI understanding before proceeding

For Continuation (Via Bootstrap):
1. Provide bootstrap file
2. Request AI confirmation of understanding
3. Answer any clarifying questions
4. Confirm readiness before substantive work

For Branch Sessions:
1. Provide branch-specific bootstrap
2. Clarify relationship to main project
3. Define scope boundaries
4. Establish integration expectations


4.3.2 Active Session Management
-------------------------------

During productive work:

Maintain Focus:
- Stay within session scope
- Defer out-of-scope items to appropriate chats
- Note but don't pursue tangents

Monitor Tokens:
- Be aware of session length
- Notice if responses become truncated or less coherent
- Plan exit before limits force it

Checkpoint Progress:
- Confirm understanding at transition points
- Request file outputs for completed work
- Don't let substantial work exist only in conversation

Document Decisions:
- Note significant decisions as they occur
- Capture rationale, not just conclusions
- These feed the decision log


4.3.3 Session Closeout
----------------------

Proper closeout preserves work and enables clean resumption:

Before Ending Any Session:
1. Ensure all deliverables are saved as files
2. Generate bootstrap if continuation is expected
3. Update session log with work accomplished
4. Note any decisions made
5. Identify next actions
6. Review Backlog Register for any items to carry forward
7. Export chat transcript (via Claude Exporter or equivalent)

The Session Closeout Checklist (Template TPL-003) provides the complete 
protocol. The discipline is non-negotiable: sessions end cleanly or work 
is at risk.


4.3.4 Emergency Exit
--------------------

When token limits approach unexpectedly:

Immediate Actions:
1. Stop substantive work
2. Request the AI generate a minimal bootstrap immediately
3. Save the bootstrap
4. End the session

Recovery:
- Use bootstrap to resume in fresh chat
- Note the emergency exit in session log
- Review what caused the headroom misjudgment

Prevention:
- Build larger margins into headroom assessment
- Use shorter, more frequent sessions
- Monitor tokens more actively


4.3.5 Phase Transition Sessions
-------------------------------

Phase transitions are not ordinary session boundaries. Moving from Phase N 
to Phase N+1 requires more than tactical handoff—it requires strategic 
recalibration. This section provides the protocol for phase boundary 
navigation.


4.3.5.1 Recognizing Phase Boundaries
------------------------------------

Phase transitions occur when:

- Planned phase deliverables are complete
- The project reaches a natural inflection point (e.g., research complete, 
  now moving to execution)
- Work type shifts significantly (e.g., from drafting to review)
- The bootstrap explicitly indicates phase completion

Not every significant milestone is a phase boundary. Phases are major 
divisions of work with distinct objectives. If you're unsure whether a 
transition has occurred, consult your Project Guide—the phase structure 
defined there is authoritative.


4.3.5.2 Phase Transition Upload Requirements
--------------------------------------------

At phase boundaries, provide:

Project Guide: The authoritative strategic plan. Phase transitions require 
returning to strategic context, not just tactical state.

Project Roadmap (TPL-010): Shows the full phase structure, current position, 
and what the upcoming phase entails. The roadmap is your horizon-level 
view.

Bootstrap from Prior Phase: The final session's bootstrap carries forward 
any tactical details needed for continuity.

Phase-Specific Materials: If the upcoming phase requires reference 
materials not previously loaded—research outputs, stakeholder inputs, 
revised specifications—include them now.


4.3.5.2.1 Token Considerations for Phase Transitions
----------------------------------------------------

Phase transitions require heavier document loading than typical session 
continuations. This increased context consumption warrants explicit attention.

Before uploading phase transition materials:

1. Start a fresh session. Phase transitions should not occur mid-session; 
   the prior session should close out cleanly before the transition begins.

2. Request a headroom assessment before loading documents. Establish 
   baseline capacity to inform loading decisions.

3. Load documents in priority order: Project Guide first, then Roadmap, 
   then Bootstrap. This ensures strategic context is established before 
   tactical details.

4. If materials are extensive, consider providing summaries rather than 
   full documents for reference items that won't require direct editing.

5. After loading, confirm remaining headroom before proceeding with 
   phase planning work. Do not assume sufficient capacity remains.

When Phase Work Involves Substantial Deliverables:

If the upcoming phase includes large-scale work—drafting lengthy documents, 
revising extensive materials, producing multiple deliverables—proactively 
request that the AI develop a segmentation strategy before execution begins.

A segmentation strategy breaks substantial work into session-sized units:

- Identify natural division points (sections, chapters, components)
- Estimate work blocks that fit comfortably within session headroom
- Plan closeout points where work can pause cleanly
- Structure bootstraps to carry forward partial progress

This prevents the failure mode of beginning a large deliverable, exhausting 
context mid-draft, and losing work or producing degraded output. The 
segmentation conversation happens once, at phase start; the resulting plan 
guides execution across multiple sessions.

Example prompt: "This phase requires drafting a 40-page policy document. 
Before we begin, develop a segmentation strategy—how should we break this 
into session-sized work blocks with clean handoff points?"

Cross-reference: Section 4.2.2 (Headroom Discipline), Section 4.2.5 (Token 
Exhaustion Recovery Protocol)


4.3.5.3 AI Responsibilities at Phase Boundaries
-----------------------------------------------

When recognizing a phase transition, the AI should:

Review the Project Guide to understand the upcoming phase definition as 
originally planned. Compare this with current project state to identify 
any drift.

Update the Roadmap by marking the prior phase complete and updating 
the current position indicator. This maintains accurate project 
positioning.

Generate Current Phase Detail, also called the Phase Execution Brief, 
which includes:
- Phase objectives (what this phase should accomplish)
- Session allocation (estimated number of sessions for this phase)
- Expected deliverables (tangible outputs by phase end)
- Success criteria (how to know when the phase is complete)

Provide the Horizon View: a summary that orients the user to where 
the project has been and where it's going. This prevents the 
"head down, lost in the work" disorientation that accumulates during 
extended execution.

Confirm Alignment before proceeding with new phase work. Phase 
transitions are natural checkpoints for verifying that the project 
remains on intended trajectory.

Review the Backlog Register to assess which deferred items, if any, 
should be addressed in the upcoming phase or carried forward.


4.3.5.4 User Responsibilities at Phase Boundaries
-------------------------------------------------

Before the transition session:

- Ensure the Project Guide is current. If scope changed during the 
  prior phase, update the Guide before the transition session.
- Gather all materials for the upcoming phase.
- Review the prior phase's accomplishments and any open issues.

During the transition session:

- Provide all required documents (Project Guide, Roadmap, Bootstrap).
- Review the AI-generated phase plan.
- Confirm or adjust the phase plan before proceeding.
- Explicitly approve transition to new phase work.

After the session:

- Note the phase transition in project records.
- Archive prior phase materials as appropriate.
- Ensure the updated Roadmap is saved for future sessions.


4.3.5.5 The Horizon View
------------------------

Phase transitions provide natural orientation opportunities. The "horizon 
view" is a deliberate pause to see both backward and forward:

What We've Accomplished: Summary of prior phase deliverables, key 
decisions made, and how the project has evolved.

Where We're Going: Overview of upcoming phase objectives, expected 
work, and how this phase connects to project completion.

This moment of strategic visibility prevents accumulating drift. When 
you're deep in execution, it's easy to lose sight of the larger 
trajectory. Phase transitions force you to lift your head and verify 
you're still on course.

The horizon view is not optional overhead—it's the mechanism that keeps 
multi-phase projects coherent across what may be weeks or months of work.


4.3.5.6 Phase Transition Checklist
----------------------------------

Quick reference for phase boundary navigation:

Before Transition Session:
[ ] Project Guide is current (updated for any scope changes)
[ ] Prior phase deliverables are complete or accounted for
[ ] Upcoming phase materials are gathered

During Transition Session:
[ ] Upload: Project Guide, Roadmap, Bootstrap
[ ] AI generates Current Phase Detail / Phase Execution Brief
[ ] Review horizon view summary
[ ] Confirm phase plan alignment
[ ] Approve proceeding with new phase work

After Transition Session:
[ ] Updated Roadmap saved
[ ] Phase transition noted in project records
[ ] Prior phase materials archived if appropriate

Cross-reference: Section 1.4.6.4 provides the user workflow overview for 
phase transitions. Section 2.5.1 explains the Project Lifecycle (Macro 
Cycle) in which phases operate. Section 2.7.3 details the AI's Phase 
Transition Orientation Duty. TPL-010 (Project Roadmap) is the primary 
document for phase tracking.


================================================================================
SECTION 5: CANONICAL PROJECT ARTIFACTS
Projects generate many outputs—drafts, notes, messages, iterations. Most 
are transient. A small set of artifacts, however, carry durable authority 
and enable project continuity. These canonical artifacts form the external 
memory system that makes multi-session, long-running projects viable.

For users seeking deeper understanding of why these artifacts exist and how 
they relate to universal project functions, see Appendix G: Universal Project 
Functions Reference.


5.1 PROJECT GUIDE
-----------------

5.1.1 Definition
----------------

The Project Guide is the authoritative, living document that defines a 
project's identity, scope, and current state. It serves as the primary 
reorientation artifact for both humans and AI.


5.1.2 Contents
--------------

A complete Project Guide includes:

Project Identification:
- Project name (full and abbreviated)
- Project code
- Status
- Version and date

Overview:
- Objective statement
- Provenance (how the project originated)
- Audience

Scope:
- In-scope items (explicit list)
- Out-of-scope items (explicit exclusions)

Deliverables:
- Primary deliverables with descriptions
- Supporting deliverables
- Format specifications

Document Structure:
- Outline or table of contents for primary deliverables
- Section descriptions if helpful

Execution Plan:
- Phases with descriptions
- Sequencing and dependencies
- Target chats per phase

Success Criteria:
- Project-level completion indicators
- Quality thresholds

Assets:
- Current project assets (files, logs, guides)
- Assets to be generated

Integration Points:
- Connections to other projects or documents
- Dependencies

Open Items:
- Unresolved questions
- Pending decisions
- Known issues

Decision Summary:
- Key decisions made (reference to full decision log)


5.1.3 Maintenance Rules
-----------------------

Update Triggers:
- Material change in scope
- Significant decision affecting project direction
- Phase completion
- Learning that invalidates prior assumptions

Update Process:
- Increment version number
- Add entry to revision history
- Ensure all sections reflect current state
- Distribute updated version to relevant contexts

What Does NOT Trigger Update:
- Routine progress within established plan
- Minor clarifications
- Session-level activities (these go in session logs)

Version Discipline:
- Material changes: increment major version (1.0 → 2.0)
- Additions without structural change: increment minor version (1.0 → 1.1)
- Corrections and clarifications: increment point version (1.0 → 1.01)


5.1.4 Template Reference
------------------------

Template TPL-001 (Project Initiation Form) provides the structure for 
creating new Project Guides. The form captures the essential elements; 
the Guide expands and maintains them throughout project life.


5.2 DECISION LOGS
-----------------

5.2.1 Definition
----------------

The Decision Log is a chronological record of decisions made during a 
project, including the rationale, alternatives considered, and context.


5.2.2 Purpose
-------------

Decision logs serve multiple functions:

Prevent Revisitation:
Documented decisions with rationale prevent re-arguing settled issues. 
When someone asks "why did we do it this way?" the log provides the answer.

Enable Auditability:
For decisions with legal, compliance, or strategic significance, the log 
provides defensibility. The reasoning is preserved, not just the outcome.

Support Learning:
Reviewing decisions retrospectively—especially ones that proved wrong—
builds judgment. The log enables this review.

Facilitate Handoffs:
New team members or returning participants can understand not just what 
was decided but why. Context transfers with the decision.


5.2.3 Entry Structure
---------------------

Each decision entry includes:

- Decision number (sequential within project)
- Date
- Brief description
- Full rationale
- Alternatives considered (if applicable)
- Implications or dependencies
- Session reference (which chat)

Template TPL-004 (Decision Log Entry) provides the standard format.


5.2.4 Maintenance Rules
-----------------------

Append-Only at Raw Level:
New decisions are added; existing entries are not modified. If a decision 
is reversed or superseded, a new entry documents that change, referencing 
the original.

Periodic Summarization:
The raw log grows over project life. Periodically (weekly, monthly, or at 
phase boundaries), summarize decisions into reports. These summaries enable 
quick orientation without reading every entry.

Review Cadence:
Decisions should be reviewed cyclically—not to second-guess but to learn. 
What patterns emerge? What decisions proved prescient or problematic? Use 
past decisions to inform future judgment.


5.3 SESSION LOGS
----------------

5.3.1 Definition
----------------

Session Logs record what occurred in each AI chat session: objectives, 
work accomplished, decisions made, issues encountered, and outputs produced.


5.3.2 Purpose
-------------

Session logs provide:

Progress Tracking:
Across a multi-session project, logs show what was accomplished when.

Issue Documentation:
Problems encountered and how they were resolved (or deferred) are captured 
for future reference.

Output Registry:
Files produced in each session are listed, preventing loss and enabling 
retrieval.

Continuity Support:
When resuming after a gap, session logs help reconstruct the project's 
trajectory.


5.3.3 Entry Structure
---------------------

Each session entry includes:

Header:
- Session identifier (matching chat name)
- Date
- Duration (approximate)
- Session type (R&D / Execution / Review / Branch)

Body:
- Session objective
- Work accomplished
- Decisions made (reference to decision log)
- Issues encountered
- Files produced

Footer:
- Bootstrap status (generated? type? target?)
- Chat export status
- Next session notes

Template TPL-005 (Session Log) provides the standard format.


5.3.4 Relationship to Chat Exports
----------------------------------

Session logs are not chat transcripts. The full transcript (exported via 
Claude Exporter or equivalent) provides complete record; the session log 
provides structured summary. Both are valuable:

- Transcript: Complete but lengthy; useful for detailed reconstruction
- Session Log: Condensed and structured; useful for quick orientation

Standard practice: maintain both. Export transcripts for archive; maintain 
session logs for active reference.


5.4 BOOTSTRAP FILES
-------------------

5.4.1 Definition
----------------

Bootstrap files are minimal, scoped context packets designed to initialize 
a new AI chat with the information needed to continue work from a prior 
session or branch into new work.


5.4.2 Purpose
-------------

Bootstraps solve the memory problem. Since AI chats do not persist memory 
across sessions, and since humans also forget, bootstraps carry essential 
context forward. They enable:

- Clean continuation after session ends
- Branching into parallel workstreams
- Resumption after extended absence
- Handoff to different team members
- Recovery from failed sessions


5.4.3 Bootstrap Principles
--------------------------

Minimal Sufficiency:
Include what is needed; exclude what is not. Bootstraps should be as short 
as possible while remaining complete. Every unnecessary element consumes 
tokens in the receiving session.

Scoped Context:
A bootstrap carries context for a specific purpose. A bootstrap for 
continuing Section 5 drafting differs from one for security research. 
Scope determines content.

Self-Contained:
The recipient (AI or human) should be able to understand the bootstrap 
without external reference. If understanding requires opening other files, 
the bootstrap is incomplete—though it may reference files that should be 
provided alongside it.

Actionable:
Bootstraps should make clear what the receiving session should do. Context 
without direction leaves the recipient guessing.


5.4.4 Bootstrap Types
---------------------

CONTINUATION Bootstrap:
- Purpose: Resume work in progress
- Contents: Project context, current state, immediate next steps
- Trigger: Token limits, natural session breaks

BRANCH Bootstrap:
- Purpose: Spin off parallel workstream
- Contents: Overall project context, branch-specific scope, integration 
  expectations
- Trigger: Need for isolated focus on subtopic

RECOVERY Bootstrap:
- Purpose: Resume after failure or extended gap
- Contents: Comprehensive project state, what was lost or uncertain, 
  recovery objectives
- Trigger: Session failure, extended dormancy, team change


5.4.5 Bootstrap Structure
-------------------------

Template TPL-002 (Bootstrap Template) provides the standard structure:

- Project identification
- Source and target chat information
- Project context
- Current state
- Completed work summary
- End-of-session files from source
- Open items
- Immediate next actions
- Constraints and parameters
- Files to reference
- Special instructions
- Questions to address at session start


5.4.6 Bootstrap Discipline
--------------------------

Create Bootstraps Proactively:
Don't wait until token limits force exit. Create bootstraps at natural 
transition points, even if you might continue in the same session. The 
bootstrap exists; you can choose not to use it.

Test Bootstraps:
The best test of a bootstrap is using it. When you open a new session with 
a bootstrap, note whether the AI understood correctly or needed 
clarification. Refine your bootstrap practice based on what works.

Link Bootstraps for Review:
When reviewing project history, bootstraps from sequential sessions show 
the project's evolution. Maintain a chain of bootstraps as project record.


5.5 SUPPORTING ARTIFACTS
------------------------

Beyond the four canonical artifacts, projects generate supporting materials:

Drafts:
Working versions of deliverables. Maintain version history. Date or version 
drafts to prevent confusion about currency.

Research Notes:
Information gathered during R&D phases. May be informal but should be 
preserved for reference.

Templates:
Reusable structures for common artifacts. See Appendix B for the standard 
template suite.

Reports:
Periodic summaries derived from logs. Weekly, monthly, or phase-based 
depending on project rhythm.


5.6 ARTIFACT RELATIONSHIPS
--------------------------

The canonical artifacts relate to each other:

Project Guide governs scope and direction; all other artifacts serve the 
project it defines.

Decision Logs record choices that shape the project; significant decisions 
may trigger Project Guide updates.

Session Logs document progress toward objectives defined in the Project 
Guide; they reference decisions made and files produced.

Bootstraps carry forward the essential elements of Project Guide, recent 
decisions, and session outcomes into new contexts.

The system is coherent when these artifacts align. Drift between them—a 
Project Guide that doesn't reflect recent decisions, session logs that 
don't match actual outputs—signals maintenance debt requiring attention.


5.7 BACKLOG REGISTER
--------------------

5.7.1 Definition
----------------

The Backlog Register is the systematic accumulation mechanism for items 
identified during a project but not addressed immediately. It captures 
deferred features, out-of-phase observations, process improvements, and 
parking lot ideas—all tagged with phase markers for traceability.

Unlike session logs that document what happened, the Backlog Register 
documents what was noticed but set aside. It prevents good ideas from 
being lost in chat transcripts while keeping active work focused on 
current priorities.


5.7.2 The Reporting Triad
-------------------------

The Backlog Register operates as part of an integrated reporting system:

BACKLOG REGISTER (accumulate): Captures items as they arise throughout 
the project lifecycle. Phase markers enable filtering by when items 
were identified.

PROJECT ROADMAP (mark progress): Tracks phase completion and current 
position. See TPL-010.

CLOSURE REPORT (synthesize): Consolidates accumulated items at project 
end. See Section 5.8 and TPL-022.

This triad eliminates the need for separate phase-end reports while 
ensuring complete capture from initiation through closure.


5.7.3 Register Sections
-----------------------

The Backlog Register contains four sections:

CURRENT PROJECT BACKLOG:
Items identified for this project version but deferred from immediate 
work. These remain candidates for completion before project closure. 
Tracked with priority levels and target phases.

VERSION BACKLOG:
Items explicitly designated for future project versions (v2.0, etc.). 
These are out of scope for the current version but documented for 
future consideration.

PROCESS IMPROVEMENTS:
Observations about the AI-assisted workflow, tools, templates, or 
methods. These don't affect project deliverables but may improve 
future projects or the framework itself.

PARKING LOT:
Ideas, tangents, questions, and observations that don't fit other 
categories. Low-commitment capture for items of uncertain future 
value. Reviewed at project close to determine final disposition.


5.7.4 Maintenance Pattern
-------------------------

Update Triggers:
- During sessions when scope-adjacent items are identified
- At session closeout as part of TPL-003 checklist review
- At phase transitions when assessing accumulated items
- At project closure when finalizing for TPL-022

AI Coordinator Role:
The AI maintains the Backlog Register as part of its coordinator duties:
- Proactively suggests items for capture when scope-adjacent content 
  emerges
- Updates the register at session closeout
- Reviews register contents at phase transitions
- Flags patterns (e.g., excessive deferrals from one phase may indicate 
  scope problems)


5.7.5 Phase Traceability
------------------------

Every item in the Backlog Register includes its PHASE IDENTIFIED field, 
enabling:

Mid-Project Filtering:
During active work, filter by phase to see what accumulated during 
specific project periods.

Pattern Detection:
Many items from one phase suggests scope creep or underestimated 
complexity. Few items may indicate thorough planning or limited 
exploration.

End-of-Project Synthesis:
Phase markers help the Closure Report organize lessons and deferrals 
chronologically.


5.7.6 Template Reference
------------------------

Template TPL-023 (Backlog Register) provides the standard structure. 
The register is a living document updated throughout the project 
lifecycle. At project closure, its contents transfer to the Closure 
Report (TPL-022) Section 4.


5.8 PROJECT CLOSURE REPORT
--------------------------

5.8.1 Definition
----------------

The Project Closure Report is the mandatory synthesis document completed 
at the end of every project managed under this framework. It validates 
completion against success criteria, documents outcomes, captures lessons, 
and prepares future projects with extracted insights.

No project is considered properly closed without a completed Closure Report.


5.8.2 Purpose
-------------

The Closure Report serves multiple functions:

VALIDATION: Confirms that success criteria from the Project Guide have 
been met (or documents why they were not).

DOCUMENTATION: Creates a permanent record of what the project achieved, 
decided, and produced.

LEARNING: Captures lessons learned—both successes and failures—for 
institutional memory.

HANDOFF: Prepares deferred items and insights for potential follow-on 
projects through the Future Project Seeding section.


5.8.3 Report Sections
---------------------

The Closure Report contains seven sections:

CRITERIA VALIDATION:
Reviews each success criterion from the Project Guide against actual 
outcomes. Status options: MET, PARTIALLY MET, NOT MET, DEFERRED, N/A.

OUTCOME SUMMARY:
Documents deliverables produced, key decisions made, and provides a 
narrative of what the project achieved.

LESSONS LEARNED:
Captures what worked well, what could be improved, surprises and 
discoveries, and process observations. Emphasis on candor—failures 
often teach more than successes.

DEFERRED ITEMS:
Consolidates content from the Backlog Register (TPL-023). Includes 
version backlog, out-of-scope items, unresolved questions, and 
parking lot items with potential future value.

ARCHIVE PLAN:
Documents how and where project materials will be preserved. Includes 
archive inventory, folder structure, and completeness verification.

SIGN-OFF:
Formal acknowledgment that the project has been assessed, documented, 
and closed according to this framework.

FUTURE PROJECT SEEDING:
Designed for direct upload with TPL-001 when initiating related future 
projects. Written for an AI that has no context about this project—
provides background, critical lessons, transferable assets, and 
suggested scope for follow-on work.


5.8.4 When to Complete
----------------------

Standard Closure:
Upon achieving all success criteria from the Project Guide. The 
Closure Report is the final project artifact.

Partial Closure:
When a project enters extended dormancy without formal closure. The 
user may later re-enter specifically to complete the Closure Report 
based on available artifacts.

Retrospective Closure:
When closing a project that has been dormant. Conduct re-entry protocol 
(TPL-007), review available artifacts, and complete the Closure Report 
with notation that closure was retrospective.


5.8.5 Integration Points
------------------------

The Closure Report integrates with other framework elements:

BACKLOG REGISTER (TPL-023):
Section 4 of the Closure Report draws directly from the Backlog 
Register. Review and finalize the register before completing closure.

PROJECT GUIDE:
Section 1 (Criteria Validation) references success criteria from the 
Project Guide. The Guide should be marked as final version.

FUTURE PROJECT INITIATION (TPL-001):
Section 7 (Future Project Seeding) is designed for upload with TPL-001 
when initiating follow-on projects. This creates continuity across 
the project lifecycle.


5.8.6 AI Coordinator Role
-------------------------

The AI assists in generating the Closure Report by:
- Reviewing the Backlog Register for deferred items
- Synthesizing lessons from session logs and decision records
- Drafting report sections for user review and confirmation
- Ensuring all sections are complete before sign-off

The user retains final authority on all assessments and lessons.


5.8.7 Template Reference
------------------------

Template TPL-022 (Project Closure Report) provides the standard structure. 
Completion is MANDATORY for all projects using this framework.

Cross-reference: See Appendix G for the theoretical foundation explaining 
why closure documentation matters (Function 12: Completion/Delivery).


================================================================================
SECTION 6: EXTERNAL ORGANIZATION SYSTEM
External memory—files, folders, and organized storage—is the durable 
foundation of AI-assisted work. This section establishes the organizational 
system that makes external memory functional.


6.1 NAMING CONVENTIONS
----------------------

Consistent naming enables retrieval, prevents confusion, and supports 
automation. Naming is not cosmetic; it is infrastructure.


6.1.1 Chat Naming
-----------------

Format: [###] [DEPT]-[PROJ]-[TYPE] ([notes])

Components:
- [###]: Three-digit sequence number
- [DEPT]: Department or functional area code
- [PROJ]: Project code
- [TYPE]: Chat type indicator (RD, EX, RV, BR)
- ([notes]): Brief description in parentheses

Examples:
- 001 GOV-AIPM-RD (initial scoping)
- 002 GOV-AIPM-EX (template dev)
- 003 FIN-BUDGET-EX (Q1 projection)
- 004 OPS-VENDOR-BR (contract research)

Why This Format:
- Numeric prefix ensures chronological sorting in chat lists
- Department-project codes enable quick identification
- Type indicator signals expected content
- Notes provide context without opening the chat


6.1.2 File Naming
-----------------

Format: [PROJ]_[Artifact]_[Identifier].[ext]

Components:
- [PROJ]: Project code
- [Artifact]: Artifact type (e.g., Bootstrap, Session_Log, Section)
- [Identifier]: Specific identifier (version, number, description)
- [ext]: File extension

Examples:
- GOV-AIPM_Bootstrap_Phase2.txt
- GOV-AIPM_Session_Log_002.txt
- GOV-AIPM_Section_01_Purpose_Scope.txt
- GOV-AIPM_Project_Guide_v1.01.txt

Template Files:
- [PROJ]-TPL-[###]_[Name].[ext]
- Example: GOV-AIPM-TPL-001_Project_Initiation_Form.txt

Why This Format:
- Project prefix groups related files in sorted views
- Underscores separate logical components
- Version or sequence numbers enable history tracking
- Descriptive names reduce need to open files to identify them


6.1.3 Version Indicators
------------------------

Files requiring version control include version in filename:

Formats:
- v1.0, v1.01, v1.1, v2.0 (for documents with formal versions)
- Draft_v1, Draft_v2 (for iterative drafts)
- _001, _002, _003 (for sequential artifacts like session logs)

Rules:
- Never overwrite without version increment
- Archive superseded versions (don't delete)
- Make current version identifiable (either by highest number or explicit 
  "current" indicator)


6.2 FOLDER STRUCTURE
--------------------

Two organizational approaches serve different retrieval needs: 
chronological/source-based and functional. Both are maintained.


6.2.1 Chat-Based Organization
-----------------------------

Each AI chat session gets a corresponding folder:

Structure:
/Projects
  /[PROJECT_CODE]
    /Chats
      /001 [DEPT]-[PROJ]-[TYPE] ([notes])
        - [All files produced in that session]
        - [Bootstrap created in that session]
        - [Session log entry]
        - [Chat transcript export]
      /002 [DEPT]-[PROJ]-[TYPE] ([notes])
        - [...]

Purpose:
- Complete record of each session
- Easy reconstruction of session context
- Source attribution for all artifacts
- Audit trail

Chat Transcript Archive:
The chat_transcript.md file in each chat folder is generated using Claude 
Exporter (https://www.claudexporter.com) or equivalent export tool. This 
file serves as the authoritative raw record of the session — the primary 
source for any verification audit (Tier 2 or Tier 3) and the foundation 
for context recovery if needed.

Export the transcript at session end, before closing the chat. The .md 
format is recommended for primary archive (preserves structure, searchable); 
.pdf is recommended for formal records requiring fixed presentation.

Filename should match session naming convention:
  [###]_[DEPT]-[PROJ]-[PHASE]_transcript.md
  Example: 003_GOV-AIPM-EX_parts-III-IV-draft_transcript.md

Rules:
- Folder names match chat names exactly
- Include everything produced in the session
- Include handwritten notes (scanned) if applicable
- Include chat export


6.2.2 Functional Organization
-----------------------------

Parallel structure organized by artifact type:

Structure:
/Projects
  /[PROJECT_CODE]
    /Guides
      - Project_Guide_v1.01.txt
      - [Other guides]
    /Logs
      /Decision_Logs
        - Decision_Log_001.txt
      /Session_Logs
        - Session_Log_001.txt
        - Session_Log_002.txt
    /Drafts
      - Section_01_Draft_v1.txt
      - Section_02_Draft_v1.txt
    /Deliverables
      - [Final outputs]
    /Templates
      - [Project-specific templates]

Purpose:
- Quick access to specific artifact types
- Non-chronological retrieval
- Working views during active development

Rules:
- Files are duplicated (copies in both structures)
- Chat folders are the source of truth
- Functional folders are convenience views
- Keep synchronized during active work


6.2.3 Archive Structure
-----------------------

When projects complete, transition to archive:

Structure:
/Archive
  /[YEAR]
    /[PROJECT_CODE]
      /Final_Deliverables
        - [Completed outputs only]
      /Project_Record
        - Project_Guide_Final.txt
        - Decision_Log_Complete.txt
        - Session_Summary.txt
      /Reference
        - [Selected supporting materials]

Rules:
- Archive selectively, not exhaustively
- Retain what would be needed to understand the project
- Retain what might be reused
- Dispose of transient working files (unless retention required)
- Document archive decisions if non-obvious


6.3 DUPLICATION DISCIPLINE
--------------------------

Files exist in multiple locations by design. This is not disorder; it is 
intentional redundancy serving different access patterns.


6.3.1 Why Duplicate
-------------------

Chat folders preserve source context. Functional folders enable quick 
access. Neither alone is sufficient.

The person reviewing "what happened in session 003" needs the chat folder.
The person seeking "the current project guide" needs the functional folder.

Duplication serves both needs without forcing a single organizational 
paradigm.


6.3.2 Managing Duplication
--------------------------

Source of Truth:
Chat folders are authoritative. When a file is produced, it originates in 
the corresponding chat folder.

Derived Copies:
Functional folders contain copies. When updating a file, update in both 
locations—or update in chat folder and recopy to functional folder.

Version Consistency:
Never have different versions in different locations under the same name. 
If versions diverge, synchronize immediately or rename to distinguish.

Active Projects:
During active work, maintain duplication actively. Sync is overhead but 
prevents confusion.

Completed Projects:
Upon completion, duplication may be reduced. Archive structure (Section 
6.2.3) consolidates to essential materials.


6.4 CLOUD STORAGE INTEGRATION
-----------------------------

Cloud storage (Google Drive, OneDrive, Dropbox, etc.) extends local 
organization with access, backup, and collaboration benefits.


6.4.1 Structure Alignment
-------------------------

Cloud folder structure should mirror local structure. Consistency enables:
- Seamless sync between local and cloud
- Clear mental model (same structure everywhere)
- Team access to organized materials

If organizational standards exist (such as OH folder conventions), cloud 
structure aligns with those standards.


6.4.2 Sync Considerations
-------------------------

Bidirectional Sync:
If using sync tools, changes in either location propagate. This requires 
discipline—don't edit the same file in multiple locations simultaneously.

Selective Sync:
Large projects may exceed practical sync limits. Sync active materials; 
archive older materials to cloud-only or local-only as appropriate.

Conflict Resolution:
Sync conflicts (two versions of same file) must be resolved promptly. 
Establish a rule: which location wins? Typically, the more recently 
modified intentional edit wins, but establish this in advance.


6.4.3 Backup Strategy
---------------------

Cloud storage is not automatically backup. True backup requires:

- Multiple copies (not just sync)
- Version history (ability to recover prior states)
- Geographic distribution (cloud providers offer this)
- Regular verification (confirm backups exist and are recoverable)

Establish backup cadence appropriate to project criticality. For business-
critical work, daily or continuous backup is reasonable.


6.5 RETRIEVAL PATTERNS
----------------------

Organization exists to serve retrieval. Common retrieval patterns should 
be fast.


6.5.1 Common Retrieval Needs
----------------------------

"What happened in session X?" → Chat folder for session X

"What is the current state of the project?" → Project Guide (functional 
folder or most recent chat folder)

"Why did we decide Y?" → Decision Log (functional folder)

"What files were produced for deliverable Z?" → Deliverables folder or 
chat folders for relevant sessions

"What did we do last week?" → Session logs for the week's sessions


6.5.2 Optimizing for Retrieval
------------------------------

Name for search: Use descriptive, searchable names. "meeting_notes.txt" 
is less retrievable than "GOV-AIPM_Session_Log_002.txt"

Consistent structure: Same organization across projects reduces cognitive 
load when switching contexts.

Index files: For complex projects, maintain an index file listing key 
materials and their locations.

Tags and metadata: If your storage system supports tagging, use it. 
Project codes, status indicators, and content types all make useful tags.


6.6 ORGANIZATION HYGIENE
------------------------

Organization degrades without maintenance. Build hygiene into workflow.


6.6.1 Session-Level Hygiene
---------------------------

At session end:
- Ensure all files are saved to chat folder
- Copy to functional folders as appropriate
- Update session log

This is built into the Session Closeout Checklist (TPL-003).


6.6.2 Weekly Hygiene
--------------------

Once per week:
- Verify functional folders are synchronized
- Clear any temporary or duplicate files
- Archive completed work if applicable

The Weekly Review Agenda (TPL-006) includes organization review.


6.6.3 Project Transition Hygiene
--------------------------------

When a project completes:
- Execute archive process
- Remove working files no longer needed
- Verify final deliverables are properly stored
- Update any cross-references in other projects




================================================================================
================================================================================
================================================================================
================================================================================

================================================================================
================================================================================

                      PART III: GOVERNANCE INTEGRATION

================================================================================
================================================================================


================================================================================
SECTION 7: THE GOVERNANCE STACK
7.1 GOVERNING DOCUMENTS
-----------------------

Every organization operates under foundational documents that establish 
authority, define structure, and constrain action. For corporations, these 
include articles of incorporation and bylaws. For LLCs, operating agreements 
and articles of organization. For partnerships, partnership agreements. For 
sole proprietors, the legal identity of the individual.

These governing documents sit at the top of the governance stack. They are 
not frequently changed. They establish who can act, what actions require 
approval, and how disputes are resolved. Everything below them derives 
authority from them.

AI-assisted work does not change this hierarchy. It operates within it.

The practical implication: before using AI to draft resolutions, policies, 
or operational procedures, understand what your governing documents permit. 
An AI-drafted resolution that exceeds the authority granted by the operating 
agreement is not merely ineffective — it may create liability or trigger 
disputes.

Key questions to answer before AI-assisted governance work:

- What actions require formal resolution vs. managerial discretion?
- Who has authority to adopt policies? Amend procedures?
- What approval thresholds apply (unanimous, majority, manager-only)?
- Are there topics reserved for member/shareholder approval?
- What notice or documentation requirements exist?

If you don't know the answers, review your governing documents first. AI 
can help analyze them, but the human must know what governs before 
delegating drafting work.


7.1.1 Document Hierarchy
------------------------

Governance documents form a hierarchy of authority:

Level 1: Constitutive Documents
- Articles of Incorporation / Organization
- Operating Agreement / Bylaws / Partnership Agreement
- Trust instruments (if applicable)

These establish the entity, define its structure, and set amendment 
procedures. Changes are infrequent and often require formal filing or 
high approval thresholds.

Level 2: Enabling Resolutions
- Board resolutions
- Member/Manager resolutions
- Consent actions

These authorize specific actions, adopt policies, or delegate authority 
within the bounds set by Level 1. They create the authorization trail for 
operational decisions.

Level 3: Policies and SOPs
- Standing policies (data handling, confidentiality, etc.)
- Standard Operating Procedures
- Operational guidelines

These implement the authority granted by Levels 1 and 2. They govern 
day-to-day operations and can typically be modified with less formality 
than higher levels.

Level 4: Forms, Templates, and Records
- Approved forms
- Transaction records
- Meeting minutes
- Logs and registers

These document activity and provide evidence of compliance with higher 
levels. They don't create authority but demonstrate its proper exercise.


7.1.2 AI's Role at Each Level
-----------------------------

AI assistance is appropriate at every level, but the nature of assistance 
differs:

Level 1 (Constitutive): AI can analyze existing documents, identify gaps, 
suggest amendments, and draft revision language. Human review is mandatory. 
Legal counsel review is strongly advised. AI does not file documents or 
make representations to state agencies.

Level 2 (Resolutions): AI excels at drafting resolutions — identifying 
required recitals, structuring whereas clauses, ensuring authorization 
language matches governing document requirements. The Resolution Drafting 
Workflow (Section 8.2) provides the detailed process.

Level 3 (Policies/SOPs): AI's strongest contribution. Policy and SOP 
development is iterative, detail-intensive, and benefits enormously from 
AI's ability to maintain consistency across lengthy documents. See SOP 
Drafting Workflow (Section 8.3).

Level 4 (Forms/Records): AI can design forms, populate templates, and 
assist with record organization. AI should not fabricate records or 
backdate documentation.


7.2 RESOLUTIONS
---------------

Resolutions are the connective tissue between governing documents and 
operational reality. They translate authority into action. A well-maintained 
resolution register provides an audit trail showing that actions were 
properly authorized.

AI-assisted resolution drafting is highly effective because resolutions 
follow predictable structures while requiring careful attention to specific 
details — exactly the combination where AI adds value.


7.2.1 Resolution Anatomy
------------------------

A properly structured resolution includes:

Title Block:
- Resolution number (sequential within register)
- Entity name
- Resolution type (Board, Member, Manager, Unanimous Consent, etc.)
- Date of adoption

Recitals (Whereas Clauses):
- Background establishing context
- Reference to authorizing provisions in governing documents
- Statement of purpose or need

Resolved Clauses:
- Specific actions authorized
- Delegation of authority (if any)
- Conditions or limitations
- Effective date

Attestation:
- Signature blocks appropriate to resolution type
- Secretary/Manager certification if required


7.2.2 Common Resolution Types
-----------------------------

Operational Resolutions:
- Opening bank accounts
- Authorizing signatories
- Approving contracts above threshold
- Establishing vendor relationships

Governance Resolutions:
- Adopting or amending policies
- Appointing officers or agents
- Establishing committees
- Delegating authority

Compliance Resolutions:
- Ratifying annual filings
- Adopting required policies (data privacy, records retention)
- Authorizing registered agent changes
- Confirming beneficial ownership information

Strategic Resolutions:
- Approving major transactions
- Authorizing new business lines
- Adopting strategic plans
- Entering partnerships or joint ventures


7.2.3 Resolution Register Maintenance
-------------------------------------

Maintain a resolution register that tracks:

- Resolution number and date
- Resolution type
- Subject matter (brief description)
- Status (Active, Superseded, Expired)
- Cross-references (what it supersedes, what supersedes it)
- Location of executed original

The register enables quick retrieval and prevents gaps or conflicts in 
the authorization chain.

AI can maintain register metadata and flag potential conflicts (e.g., 
a new resolution that contradicts an active one), but the human must 
verify and resolve conflicts.


7.3 SOPs & POLICIES
-------------------

Standard Operating Procedures and policies occupy Level 3 of the governance 
stack. They translate resolution-level authority into repeatable operational 
guidance.

The distinction:

Policies state what must or must not be done and why. They establish 
standards and boundaries. Example: "All client data must be classified 
before storage."

SOPs state how to do it. They provide step-by-step procedures implementing 
policy requirements. Example: "To classify client data: 1) Review data 
type against Classification Matrix (Appendix A)..."

Both benefit from AI-assisted development, but the workflows differ.


7.3.1 Policy Development
------------------------

Policies typically require:

- Clear statement of scope (what/who the policy covers)
- Rationale (why the policy exists)
- Requirements (what must be done)
- Exceptions process (how to handle edge cases)
- Enforcement (consequences of non-compliance)
- Review cycle (when policy is reassessed)

AI assists by:

- Drafting initial policy language from requirements discussions
- Ensuring consistent structure across policies
- Identifying gaps (missing exception handling, unclear scope)
- Cross-referencing related policies to prevent conflicts
- Generating plain-language summaries for training


7.3.2 SOP Development
---------------------

SOPs require:

- Numbered steps in logical sequence
- Decision points clearly marked
- Responsibility assignments (who does each step)
- References to tools, forms, or systems used
- Exception handling procedures
- Version control and revision history

AI assists by:

- Converting narrative process descriptions into structured steps
- Identifying missing steps or logical gaps
- Maintaining numbering consistency during revisions
- Generating process flowcharts or decision trees
- Ensuring terminology consistency across related SOPs

Section 8 provides detailed workflows for AI-assisted policy and SOP 
development.


7.3.3 Policy vs. SOP Maintenance
--------------------------------

Policies change infrequently. When business needs shift, regulatory 
requirements change, or gaps are identified, policies are revised through 
a controlled process that typically includes review, approval, and 
communication.

SOPs change more frequently. As tools evolve, processes improve, or 
errors are corrected, SOPs are updated. Revision should be easier than 
policy revision but still controlled — untracked SOP changes create 
confusion and compliance risk.

Establish clear ownership:

- Policy Owner: Accountable for policy content and review cycle
- SOP Owner: Accountable for procedure accuracy and maintenance
- These may be the same person or different people

AI can track revision history, flag documents due for review, and draft 
revision language, but humans must approve and communicate changes.


7.4 GOVERNANCE MAINTENANCE
--------------------------

Governance is not a project but an ongoing function. Documents must be 
reviewed, updated, and kept aligned as the organization evolves.


7.4.1 Review Cycles
-------------------

Establish review cycles appropriate to document type:

| Document Type | Typical Review Cycle | Trigger Events |
|---------------|---------------------|----------------|
| Governing Docs | Annual or as needed | Structural change, law change |
| Resolutions | As adopted | New need, supersession |
| Policies | Annual | Incident, audit finding, regulation change |
| SOPs | Semi-annual | Process change, error discovery, tool change |
| Forms | With parent SOP | Parent document revision |

AI can maintain a governance calendar alerting when reviews are due. 
The Weekly Review (TPL-006) includes governance items as a standing 
agenda topic.


7.4.2 Change Control
--------------------

When governance documents change:

1. Draft revision with clear redline/comparison to current version
2. Route for appropriate approval (per document hierarchy)
3. Update version number and revision history
4. Archive prior version (don't delete — maintain audit trail)
5. Communicate change to affected parties
6. Update any cross-references in related documents
7. Log change in appropriate register

AI assists with drafting, comparison generation, and cross-reference 
identification. Humans approve, communicate, and verify.


7.4.3 Gap Analysis
------------------

Periodically assess whether governance documentation matches operational 
reality:

- Are procedures being followed as documented?
- Do documents reflect current tools and systems?
- Are there operational practices without documented authority?
- Are there documented procedures no longer in use?

AI can assist gap analysis by comparing operational descriptions against 
documented procedures and flagging discrepancies. This is particularly 
effective when operational data (logs, records) can be compared against 
SOP requirements.

Discovery of gaps triggers either:
- Document update (if practice is correct, documentation is stale)
- Practice correction (if documentation is correct, practice has drifted)
- Escalation (if the appropriate response is unclear)


================================================================================
SECTION 8: AI-ASSISTED GOVERNANCE DEVELOPMENT
This section provides operational workflows for using AI to develop and 
maintain governance documentation. These workflows integrate with the 
project architecture (Section 4) and artifacts (Section 5) established 
earlier.


8.1 GOVERNANCE R&D WORKFLOW
---------------------------

Major governance work — new policy development, significant SOP revision, 
governance framework establishment — benefits from a dedicated R&D phase 
before execution.


8.1.1 When to Use R&D Phase
---------------------------

Initiate governance R&D when:

- Creating a new policy area (no existing documentation)
- Overhauling existing governance (major structural change)
- Addressing compliance gap (audit finding, regulatory change)
- Establishing governance for new business function
- Uncertainty exists about scope, structure, or approach

Skip R&D and proceed directly to execution when:

- Routine update to existing document
- Minor revision (clarification, correction)
- Well-defined scope with clear requirements
- Precedent exists from similar prior work


8.1.2 R&D Session Structure
---------------------------

A governance R&D session follows the standard Root Chat pattern (Section 
4.1.2) with governance-specific focus:

1. Context Setting
   - Identify the governance gap or need
   - Reference relevant governing documents
   - Identify stakeholders and approval requirements

2. Requirements Gathering
   - What must the governance document accomplish?
   - What constraints apply (legal, operational, cultural)?
   - What existing documents relate to this work?

3. Structure Exploration
   - What document type is appropriate (policy, SOP, resolution)?
   - What sections or components are needed?
   - What level of detail is required?

4. Gap Analysis
   - What doesn't exist that needs to?
   - What exists but needs revision?
   - What exists and should be referenced (not duplicated)?

5. Project Plan
   - How will execution be structured?
   - What approvals are needed and when?
   - What is the timeline?

Document R&D outcomes in a Project Guide (TPL-001 / Section 5.1) before 
proceeding to execution.


8.1.3 Using NotebookLM for Governance Research
----------------------------------------------

For organizations with substantial existing governance documentation, 
NotebookLM or equivalent retrieval tools accelerate R&D:

Setup:
Upload governing documents, existing policies, relevant resolutions, and 
current SOPs to a dedicated NotebookLM notebook.

During R&D:
When questions arise about existing governance, query the notebook rather 
than loading full documents into the AI context. Example queries:

- "What does the Operating Agreement say about manager authority to 
  adopt policies?"
- "Which existing SOPs reference client data classification?"
- "What resolution authorized our current signing authorities?"

This preserves AI context for analytical work while enabling precise 
retrieval from the document corpus.

Claude Integration:
When NotebookLM returns relevant passages, provide them to Claude with 
source citation. Claude can then incorporate accurate references into 
drafts without token-expensive full document loading.


8.2 RESOLUTION DRAFTING WORKFLOW
--------------------------------

Resolution drafting is procedural and benefits from systematic workflow.


8.2.1 Pre-Drafting Checklist
----------------------------

Before drafting, confirm:

[ ] Authority identified — which governing document provision authorizes 
    this resolution?
[ ] Resolution type determined — Board, Member, Manager, Unanimous Consent?
[ ] Approval process known — who must sign, what notice required?
[ ] Prior resolutions checked — does this supersede or conflict with 
    existing resolutions?
[ ] Resolution number assigned — per register numbering


8.2.2 Drafting Session
----------------------

Initiate an Execution chat for resolution drafting:

1. Provide Context
   - Resolution type and number
   - Authorizing provision from governing documents
   - Purpose/need for resolution
   - Any specific language requirements

2. Request Draft
   - Ask for complete resolution with all standard components
   - Specify any organization-specific conventions
   - Note if resolution supersedes prior resolution

3. Review Critically
   - Verify recitals accurately state background
   - Confirm resolved clauses accomplish intended purpose
   - Check authorization language matches governing documents
   - Verify signature blocks are appropriate

4. Iterate as Needed
   - AI revision based on feedback
   - Continue until draft is ready for human review


8.2.3 Post-Drafting
-------------------

After AI drafting is complete:

- Route for appropriate approval per governance requirements
- Obtain signatures
- File executed original appropriately
- Update resolution register
- Communicate as required
- Archive drafts with final for audit trail


8.3 SOP DRAFTING WORKFLOW
-------------------------

SOP development is iterative and often spans multiple sessions. The 
workflow accommodates this extended process.


8.3.1 SOP Project Initiation
----------------------------

For new SOP or major revision, complete Project Initiation (TPL-001) with 
SOP-specific considerations:

Scope Definition:
- What process does this SOP cover?
- Where does this SOP begin and end (boundaries)?
- What is explicitly out of scope?

Integration Points:
- What existing SOPs relate to this one?
- What policies does this SOP implement?
- What forms or templates does this SOP reference?

Stakeholder Input:
- Who performs this process currently?
- Who supervises or approves outputs?
- Who will review and approve the SOP?


8.3.2 SOP Drafting Sessions
---------------------------

SOP drafting typically proceeds in phases:

Phase 1: Structure
- Establish section framework
- Define numbering convention
- Identify major process segments

Phase 2: Content Development
- Draft each section sequentially
- Maintain cross-reference integrity
- Flag open questions for resolution

Phase 3: Integration
- Verify cross-references to other documents
- Ensure terminology consistency
- Add forms and templates as appendices

Phase 4: Review Preparation
- Generate clean draft for review
- Prepare revision summary if updating existing SOP
- Identify reviewers and timeline


8.3.3 Managing Long SOP Projects
--------------------------------

Complex SOPs may span many sessions. Apply full project management rigor:

- Maintain Session Logs (TPL-005) for each drafting session
- Log decisions in Decision Log (TPL-004) — especially scope decisions 
  and structural choices
- Bootstrap (TPL-002) between sessions to preserve context
- Use Project Guide to track overall progress and open items

The token exhaustion protocol (Section 4.2.5) is particularly relevant 
for long SOP projects. SOPs are detail-intensive; context accumulates 
quickly. Monitor proactively and bootstrap early rather than risk 
degradation mid-section.


8.3.4 SOP-Specific Quality Checks
---------------------------------

Before finalizing SOP draft:

[ ] All steps numbered sequentially without gaps
[ ] Decision points have clear criteria and branches
[ ] Responsibilities assigned for each action
[ ] Cross-references verified (section numbers, document names)
[ ] Terminology consistent with related documents
[ ] Forms/templates referenced exist and are current
[ ] Revision history updated
[ ] Version number incremented appropriately


8.4 ADOPTION & IMPLEMENTATION
-----------------------------

Drafted governance documents must be formally adopted and effectively 
implemented. AI assists both phases.


8.4.1 Adoption Process
----------------------

Adoption varies by document type:

Policies:
- Draft reviewed by stakeholders
- Revisions incorporated
- Final draft approved by appropriate authority (per governing documents)
- Approval documented (resolution, meeting minutes, or consent)
- Effective date established

SOPs:
- Draft reviewed by process performers and supervisors
- Revisions incorporated
- Final draft approved by SOP owner or designated authority
- Version control updated
- Prior version archived

Resolutions:
- Draft reviewed by signatories
- Execution by appropriate parties
- Filing if required (some resolutions require state filing)
- Register updated


8.4.2 Implementation Support
----------------------------

Adopted documents must be communicated and operationalized:

Communication:
- Notify affected parties of new/revised document
- Highlight key changes if revision
- Provide access to authoritative version
- Set expectation for compliance timeline

Training:
- Develop training materials if complexity warrants
- AI can generate summaries, FAQs, and quick-reference guides
- Conduct training sessions as appropriate

Integration:
- Update related documents to reference new governance
- Revise forms if procedures changed
- Update systems or tools if workflows affected

Monitoring:
- Establish how compliance will be verified
- Schedule first review per document review cycle
- Note in governance calendar


8.4.3 Feedback Loop
-------------------

Implementation reveals gaps and issues. Establish a feedback mechanism:

- How do users report problems or ambiguities?
- Who triages feedback (the document owner)?
- What threshold triggers revision vs. clarification vs. training?

AI can help analyze feedback patterns: "These three questions all relate 
to Section 4.2 — consider clarifying that section."

The Weekly Review (TPL-006) includes governance items, providing regular 
opportunity to surface feedback and schedule revisions.


================================================================================
================================================================================

                          PART IV: TOOLS & FORMATS

================================================================================
================================================================================

This part addresses the tool ecosystem that supports AI-assisted operations. 
Where earlier parts focus on methodology and process, Part IV addresses the 
practical infrastructure: how to separate AI roles for verification, what 
adjunct tools extend AI capabilities, and how file formats affect workflow.

The tool landscape changes rapidly. Rather than prescribing specific products, 
this part establishes functional categories and selection principles. Users 
should evaluate tools against these criteria within their operational context.


================================================================================
SECTION 9: AI ROLE SEPARATION
9.1 THE CASE FOR ROLE SEPARATION

AI platforms vary in their strengths. Some excel at extended reasoning and nuanced judgment; others at rapid iteration, creative generation, or specialized domain tasks. Effective AI-assisted operations leverage these differences rather than forcing a single platform to perform all functions.

Role separation is not about ranking AI platforms. It is about matching task characteristics to platform capabilities, reducing single-point-of-failure risk, and creating verification layers that catch errors no single system would identify on its own.

The framework distinguishes three operational roles: Primary Execution, Secondary Validation, and Specialized Function. A single project may use one, two, or all three—determined by project complexity, verification requirements, and available platform access.

9.2 PRIMARY EXECUTION ROLE

The Primary Execution AI handles the bulk of substantive work: drafting, analysis, project management, and iterative refinement. Selection criteria for this role include:

Context Window Capacity. Complex projects require platforms that can hold substantial context—project guides, bootstraps, reference documents, and accumulated session history—without degradation. Platforms with larger effective context windows reduce the frequency of session handoffs and the associated continuity risk.

Instruction Following. The primary AI must reliably follow multi-step instructions, maintain constraints across extended interactions, and resist drift toward generic responses. This capacity proves more important than raw capability metrics for sustained project work.

Session Continuity. Platforms that maintain coherent state across long sessions reduce overhead. Those requiring frequent re-grounding create friction and increase error surface.

Reasoning Transparency. For governance-adjacent work, the ability to articulate reasoning—not just conclusions—enables verification and creates audit trails.

In practice, the Primary Execution role demands the most from an AI platform. It is where prescriptive enforcement matters most, where context preservation is critical, and where the bulk of verification effort concentrates.

9.3 SECONDARY VALIDATION ROLE

The Secondary Validation AI provides independent review of Primary AI outputs. This role operationalizes Tier 2 verification (see Section 8.2) and serves as a hedge against systematic blind spots.

The validation role works best when:

Platform Differs from Primary. Cross-platform validation catches errors that stem from shared training biases or architectural limitations. A second instance of the same AI provides less independent verification than a different platform entirely.

Validator Has Fresh Context. The Secondary AI should encounter the work product without the accumulated context of its creation. This "fresh eyes" approach surfaces assumptions that became invisible during iterative development.

Validator Is Prompted Adversarially. Rather than asking "does this look right," effective validation prompts instruct the AI to find problems, identify unsupported claims, and challenge conclusions. See VERIF-001 in the Golden Prompt Library (Section 10.7) for a tested prompt formulation.

Validation is not proofreading. It is structural review: logic gaps, unfounded assertions, hallucinated consistency, and sycophantic agreement with flawed premises. Surface errors matter less than reasoning integrity.

Secondary validation adds overhead. Reserve it for outputs that carry consequence—deliverables, governance documents, external communications—rather than internal working drafts.

9.4 SPECIALIZED FUNCTION ROLE

Some tasks benefit from purpose-built tools rather than general-purpose AI. The Specialized Function role encompasses:

Domain-Specific Platforms. Legal research tools, financial modeling platforms, or industry-specific AI applications may outperform general models within their domains. When available and appropriate, route domain-specific queries to domain-specific tools.

Utility Functions. Tasks like file conversion, formatting, transcription, or calculation often have dedicated tools that execute faster and more reliably than prompting a general AI. Using the right tool for the job preserves AI context capacity for tasks that require reasoning.

Verification Specialists. Some platforms specialize in fact-checking, citation verification, or code review. These can supplement general validation with targeted capability.

The framework treats Specialized Function tools as components in a larger system. They receive inputs, produce outputs, and integrate with the broader workflow—but they do not replace the judgment layer that the Primary AI provides.

9.5 MULTI-AI WORKFLOW PATTERNS

Several patterns emerge for coordinating multiple AI roles:

Sequential Handoff. Primary AI produces draft; Secondary AI validates; Human reviews findings and decides. This pattern suits deliverable-focused work with clear quality gates.

Parallel Consultation. User queries both Primary and Secondary simultaneously on the same question, compares responses, and synthesizes. This pattern suits high-stakes decisions where diverse perspectives add value.

Specialized Insertion. Primary AI handles most work; specific subtasks route to Specialized tools; outputs return to Primary for integration. This pattern suits projects requiring capabilities the Primary AI lacks.

Escalation Cascade. Work proceeds with Primary AI until verification flags an issue; Secondary AI investigates; if unresolved, Human Review (Tier 3) engages. This pattern suits ongoing operational work with graduated oversight.

No pattern fits all projects. Select based on risk profile, available platforms, and operational constraints.

9.6 PLATFORM SELECTION CONSIDERATIONS

The framework is tool-agnostic by design. Platform capabilities shift; new entrants emerge; access constraints vary by organization. Rather than prescribing specific platforms, the framework offers selection criteria:

For Primary Execution, prioritize: context capacity, instruction fidelity, reasoning transparency, session stability.

For Secondary Validation, prioritize: independence from Primary (different platform preferred), adversarial prompting compatibility, structured output capability.

For Specialized Functions, prioritize: domain fit, integration ease, reliability, speed.

Document platform selections in the Project Initiation Form (TPL-001) and revisit if performance degrades or superior options become available.

9.7 ROLE SEPARATION AND GOVERNANCE

Role separation creates natural checkpoints for governance integration. The handoff between Primary and Secondary AI is a verification gate. The return from Specialized tools is an integration checkpoint. Each transition is an opportunity for human oversight.

For governance-sensitive work, consider requiring human approval at role transitions—not just at final output. This distributed oversight catches issues earlier and creates clearer audit trails than end-stage review alone.

Cross-reference: See Section 7 (Governance Stack Integration) for positioning AI outputs within approval hierarchies. See Section 8 (Quality Assurance) for verification tier alignment.


================================================================================
SECTION 10: ADJUNCT TOOLS
AI platforms do not operate in isolation. A constellation of supporting tools extends their capabilities, captures their outputs, and bridges gaps in their functionality. This section catalogs tool categories that integrate with AI-assisted operations.

The goal is not to prescribe specific products—these change rapidly—but to identify functional categories that support effective workflows. Select tools within each category based on your operational context, platform compatibility, and organizational constraints.

10.1 CAPTURE TOOLS

Capture tools record inputs for AI processing. Their value lies in reducing friction between information sources and AI platforms.

Voice-to-Text Applications. Dictation tools allow verbal input when typing is impractical—during commutes, while reviewing physical documents, or when thinking benefits from speaking aloud. Look for tools that integrate with your AI platform's input methods (direct paste, file upload, or API connection).

Screenshot and Screen Recording. Visual information often requires capture before AI processing. Screenshots of error messages, interface states, or document sections feed image-capable AI models. Screen recordings document processes for later analysis.

Document Scanning. Physical documents enter digital workflows through scanning. Mobile scanning apps with OCR produce searchable text that AI can process; image-only scans limit AI to visual interpretation.

Clipboard Managers. Extended clipboard history prevents loss of copied content during multi-step workflows. When moving information between applications, documents, and AI chats, clipboard managers reduce re-work.

The common thread: capture tools reduce the gap between where information exists and where AI can process it. Evaluate based on output quality, format compatibility, and workflow integration.

10.2 CALENDAR AND SCHEDULING

AI-assisted project management intersects with time-based coordination. Calendar tools serve several functions:

Deadline Tracking. Project milestones, review dates, and deliverable deadlines benefit from calendar visibility. Some practitioners maintain dedicated project calendars separate from personal scheduling.

Session Scheduling. For projects spanning multiple sessions, scheduling next sessions during closeout (while context is fresh) improves continuity. Calendar entries can include session objectives and bootstrap file references.

Reminder Integration. Calendar reminders prompt action on AI-generated tasks. When AI identifies follow-up items, translating these to calendar entries ensures they surface at appropriate times.

Time Blocking. Dedicated blocks for AI-assisted work—distinct from meetings or reactive tasks—improve focus quality. The depth required for effective AI collaboration benefits from protected time.

Cross-reference: See Section 10.8 for additional scheduling workflow tips.

10.3 RETRIEVAL AND QUERY TOOLS

Context management is a persistent challenge. Retrieval tools help surface relevant information from accumulated project materials.

NotebookLM and Similar Platforms. Document-grounded query tools allow natural language questions against uploaded materials. For large projects, this creates a queryable knowledge base from project guides, bootstraps, session logs, and reference documents.

Workflow Integration: NotebookLM functions as an active retrieval partner within AI-assisted projects. Rather than loading extensive reference material directly into AI context (consuming token capacity), the Primary AI can prompt the user to query NotebookLM for specific information. The workflow operates as follows:

1. Primary AI identifies information need from project materials
2. AI provides the specific query for user to execute in NotebookLM
3. User runs query in NotebookLM against uploaded project corpus
4. User returns the cited response to Primary AI
5. AI integrates retrieved information and proceeds

This pattern preserves Primary AI context capacity for reasoning while leveraging NotebookLM's retrieval capability across large document sets. The cited responses from NotebookLM provide source attribution that would otherwise require AI verification.

Search and Indexing Tools. Desktop search applications index local files for rapid retrieval. When AI work references historical documents, fast search reduces friction.

Reference Managers. For research-intensive projects, reference management tools (Zotero, Mendeley, or similar) organize source materials with metadata that supports later retrieval and citation.

The retrieval tool category addresses a fundamental constraint: AI context windows, however large, cannot hold everything. Strategic use of retrieval tools extends effective context beyond platform limits.

Cross-reference: See Section 10.8 for bootstrap-first upload guidance and retrieval workflow tips.

10.4 CONVERSION AND UTILITY TOOLS

Format mismatches create friction. Conversion tools bridge gaps between what you have and what you need.

File Format Converters. AI platforms vary in accepted input formats. Conversion tools transform documents between formats (PDF to text, DOCX to Markdown, image to searchable PDF) to match platform requirements.

Markdown Processors. Markdown serves as a lightweight formatting language compatible with many AI platforms. Converters that transform Markdown to other formats (HTML, PDF, DOCX) and vice versa support flexible workflows. Browser extensions and online tools handle common conversions.

Text Cleanup Utilities. Content from various sources arrives with formatting artifacts—extra line breaks, inconsistent spacing, special characters. Cleanup tools normalize text before AI processing.

Merge and Split Tools. Large documents may require splitting for AI processing; multiple outputs may require merging for delivery. PDF tools and text utilities handle these operations.

Match tools to your common format pathways. If most inputs arrive as PDFs and most outputs leave as DOCX, ensure that conversion pathway is frictionless.

10.5 STORAGE AND SYNC TOOLS

Project materials require accessible, organized, persistent storage. This category encompasses:

Cloud Storage Platforms. Services like Google Drive, Dropbox, or OneDrive provide accessible storage with sync across devices. AI project folders benefit from consistent structure (see naming conventions in Section 6).

Version-Aware Storage. Beyond basic storage, some platforms maintain version history, enabling rollback to earlier states. For iterative AI projects, version history provides safety nets when revisions go wrong.

Backup Systems. AI-generated work represents invested effort. Backup systems—automated and tested—protect against loss. The 3-2-1 rule (three copies, two media types, one offsite) applies to AI project materials as to any valued data.

Sync Considerations. When working across devices, sync timing matters. Starting work before sync completes risks operating on stale materials. Verify sync status before beginning sessions, particularly for bootstraps and project guides.

Cross-reference: See Section 10.8 for device-switching workflow guidance.

10.6 CHAT EXPORT AND ARCHIVAL TOOLS

AI chat sessions contain valuable content that platforms may not preserve indefinitely. Export and archival tools capture this content for reference, compliance, and project continuity.

Platform-Native Export. Some AI platforms offer built-in export functions. Familiarize yourself with available options—format choices, content scope, and any limitations (e.g., image handling, code block formatting).

Third-Party Export Tools. Extensions and external tools supplement native export. Claude Exporter (claudexporter.com) and similar tools convert chat transcripts to downloadable formats. Evaluate export fidelity—ensuring code blocks, formatting, and structure survive the export process.

Export Timing. Export at session closeout, before content ages out of easy access. Waiting risks loss if platform changes, account issues arise, or memory fades about which sessions contain needed content.

Archival Organization. Exported transcripts benefit from consistent naming and storage. Align export file names with chat naming conventions and store in project folders alongside bootstraps and logs.

Chat Naming Protocol. Effective archival begins with systematic chat naming. At session start, establish a project-consistent name following the convention in Section 6.1 (typically: [###] [DEPT]-[PROJ]-[PHASE] (notes)). At session closeout, refine with a semantic label capturing the session's primary contribution (e.g., "(Parts III-IV draft)" or "(stakeholder review)"). Include the refined name in both the bootstrap and session log for the next instance to inherit.

This naming discipline supports retrieval months later when searching for specific session content. Vague names ("AI project chat 4") create friction; semantic names ("004 GOV-AIPM-EX (template development)") surface in searches.

Cross-reference: The bootstrap template (TPL-002) includes chat naming fields. See Section 6.1 for chat strategy and naming conventions. See Section 10.8 for export workflow tips.

10.7 GOLDEN PROMPT LIBRARY

Effective prompts represent distilled expertise. Prompts that work well once often work well repeatedly—yet without systematic capture, they scatter across chat transcripts and fade from memory. The Golden Prompt Library addresses this through structured reuse.

Concept and Purpose. The library is a curated collection of high-performing prompts, each documented with context, usage guidance, and performance notes. Rather than reinventing prompts each session or losing successful formulations, practitioners build a reusable asset that compounds over time.

Library Structure. Each prompt entry includes:

- Prompt ID: Category-based identifier (e.g., ALIGN-001, VERIF-001)
- Name: Descriptive title for quick reference
- Category: Alignment, Drafting, Verification, Recovery, Pivot, or Closeout
- Context: When to use—the situation or trigger that calls for this prompt
- Model Affinity: Which platforms the prompt is tested/optimized for
- The Prompt: Exact text with {{VARIABLES}} marking substitution points
- What to Avoid: Known failure modes or anti-patterns
- Performance Notes: Observed success patterns and variations
- Related Prompts: Cross-references within the library
- Source and Version: Origin and update history

Starting the Library. Begin with prompts that recur across projects. Common candidates:

- Session opening alignment ("Review these materials and state your assumptions...")
- Verification requests ("Review this document adversarially...")
- Bootstrap generation ("Generate a bootstrap for session continuation...")
- Pivot framing ("We need to change direction. Current state is...")
- Closeout summary ("Summarize this session's accomplishments...")

Maintenance. The library is a living document. Add prompts that prove effective; retire those that don't perform. Version prompts when significant refinements occur. Review periodically to cull prompts that have become stale or superseded.

See Appendix B for the initial Golden Prompt Library with four foundational prompts.

10.8 USER-SIDE WORKFLOW TIPS

Beyond tools, effective AI collaboration depends on user-side practices. These tips consolidate operational wisdom from extended AI project work.

Bootstrap-First Upload. When starting a continuation session, upload the bootstrap before other materials. This establishes project context before the AI processes additional content, reducing misinterpretation risk.

Markdown Conversion Workflow. For outputs destined for formatted delivery:
1. Draft in plain text within AI chat
2. Export to Markdown
3. Convert to target format (HTML, DOCX, PDF)
4. Review in target format for conversion artifacts
5. Final polish in native application if required

Browser extensions (Markdown Preview Plus, Markdown Viewer) and online tools (Dillinger, StackEdit) handle conversion without installing software.

NotebookLM Integration. Beyond passive document storage, NotebookLM can serve as an active retrieval partner (detailed in Section 10.3). Key practices:

- Upload complete project corpus: guides, bootstraps, logs, templates, drafts
- Update after each session with new artifacts
- When Primary AI needs reference information, accept its suggested query
- Return NotebookLM's cited response for AI integration
- Use for "what did we decide about X?" questions that span sessions

Pre-Session Checklist. Before starting a substantive AI session:

- [ ] Bootstrap uploaded (for continuing projects)
- [ ] Project guide accessible
- [ ] Relevant reference documents ready
- [ ] Previous session log reviewed (for context on open items)
- [ ] Decision log current (know the last decision number)
- [ ] Time block protected (minimize interruptions)
- [ ] Export tool ready (for session closeout)

Session Hygiene. During sessions:

- Save incrementally (don't wait for AI to generate large outputs before copying)
- Flag exceptional prompts in notes for later library addition
- Update decision log in real-time (easier than reconstructing later)
- Monitor for drift indicators (see Section 14.3 when drafted)
- Note open items as they emerge (for closeout capture)

Closeout Discipline. At session end:

- Generate bootstrap before context fades
- Update decision log with any undocumented decisions
- Export chat transcript
- Rename chat with semantic label
- Upload new artifacts to NotebookLM
- Schedule next session if project continues
- Note any action items requiring external follow-up

These practices compound. Individually, each adds marginal overhead. Together, they create a workflow that sustains project coherence across sessions, devices, and extended timelines.

Cross-reference: Individual tool sections (10.1-10.7) reference specific tips relevant to their domains. See TPL-007 (Session Log) for structured session documentation.


================================================================================
SECTION 11: FILE TYPES AND FORMATTING
11.1 THE ROLE OF FILE TYPES IN AI WORKFLOWS

File formats shape what AI can do with content. Some formats preserve structure and meaning; others flatten information into raw text. Some enable direct AI processing; others require conversion. Understanding format characteristics informs workflow design.

The framework does not mandate specific formats—context dictates appropriate choices. Rather, it establishes principles for format selection and documents common pathways.

11.2 INPUT FORMAT CONSIDERATIONS

When preparing materials for AI processing, format affects both capability and quality.

Plain Text (.txt). Maximum compatibility, minimum formatting. Text files work with virtually all AI platforms but carry no structural metadata. Use for content where structure is implicit or unimportant.

Markdown (.md). Lightweight formatting with broad AI compatibility. Markdown preserves headers, lists, emphasis, and basic structure while remaining human-readable in raw form. Well-suited for project documentation, drafts, and working materials.

PDF (.pdf). Ubiquitous but variable in AI compatibility. Text-based PDFs process reasonably well; image-based PDFs (scanned documents) require OCR. Complex layouts, multi-column formats, and embedded elements may not parse cleanly. When possible, obtain or create text-based originals rather than PDF conversions.

Word Documents (.docx). Common interchange format with reasonable AI compatibility. Formatting generally survives, though complex features (track changes, comments, embedded objects) may not process as expected. For AI input, simpler formatting produces more reliable results.

Spreadsheets (.xlsx, .csv). Tabular data formats. CSV offers maximum compatibility as plain text; XLSX preserves formulas and formatting but may require conversion for some AI platforms. For data analysis tasks, consider what structure the AI needs to interpret the data correctly.

Images (.png, .jpg, .webp). Visual content for image-capable AI models. Screenshots, diagrams, and document images process with varying accuracy depending on content complexity. Text-heavy images benefit from OCR preprocessing when text content is the goal.

The principle: simpler formats generally process more reliably. When source materials exist in complex formats, consider whether conversion to simpler formats serves the workflow better than direct processing.

11.3 OUTPUT FORMAT CONSIDERATIONS

AI-generated content requires delivery in appropriate formats. Format selection depends on audience, purpose, and downstream use.

Working Drafts. Plain text or Markdown suits iterative development. These formats allow easy AI re-processing if revision cycles continue. Premature formatting creates rework when content changes.

Internal Documentation. Markdown or simple HTML works for materials that remain within technical workflows. These formats render well in many environments without requiring specific applications.

Formal Deliverables. Client-facing or governance documents typically require polished formats—DOCX for editable documents, PDF for fixed-form delivery. The transition from working draft to formal deliverable is a natural quality gate.

Presentation Materials. Slide decks, visual summaries, and formatted reports may require application-specific formats (PPTX, formatted PDF). AI can generate content and structure; final formatting often benefits from human refinement in native applications.

Code and Technical Outputs. Programming languages, configuration files, and technical artifacts each have native formats. AI-generated code should output in appropriate formats with correct extensions.

The principle: match format to purpose and audience. Working formats prioritize flexibility; delivery formats prioritize presentation and compatibility.

11.4 FORMAT CONVERSION PATHWAYS

Common conversion needs and approaches:

Markdown → HTML. Direct rendering via browser preview, conversion tools, or online processors. Preserves structure; styling depends on CSS applied at rendering.

Markdown → DOCX. Pandoc or similar tools handle conversion with configurable styling. Online converters offer simpler paths with less control. Some AI platforms generate DOCX directly.

Markdown → PDF. Often via HTML intermediary (Markdown → HTML → PDF print/export). Direct Markdown-to-PDF tools exist but may handle edge cases differently.

PDF → Text. Extraction quality varies by PDF source. Text-based PDFs extract cleanly; scanned documents require OCR. Complex layouts may scramble content order.

DOCX → Text/Markdown. Word's save-as-text function or dedicated conversion tools. Some formatting survives as Markdown equivalents; complex formatting flattens.

The principle: know your common pathways and have reliable tools for each. Format conversion friction should not slow routine operations.

11.5 FORMATTING STANDARDS FOR PROJECT ARTIFACTS

Consistency in formatting supports usability and professionalism. While final format decisions depend on context, baseline standards guide development work.

Document Identification. All project documents include clear identification: project code, document type, version number, and date. Header or footer placement ensures visibility regardless of how documents are excerpted.

Version Notation. Follow semantic versioning for documents: major.minor (e.g., 1.0, 1.1, 2.0). Major versions indicate structural changes or significant revisions; minor versions indicate incremental updates. Draft status precedes version (e.g., "Draft v0.1" before initial approval, "v1.0" after).

Section Numbering. Hierarchical numbering (1, 1.1, 1.1.1) supports cross-referencing and navigation. Maintain consistent depth—don't alternate between numbered and unnumbered sections without reason.

Cross-Reference Style. Internal references cite section numbers (e.g., "See Section 8.2") or template codes (e.g., "per TPL-001"). Avoid page number references in working documents, as these shift with formatting changes.

Template Codes. Project templates follow the convention established in Section 5: GOV-AIPM-TPL-### with sequential numbering. Template references throughout documentation use the short form (TPL-001).

For this manual's internal development, TXT format serves all drafting phases. HTML formatting with specified typography (Calibri 11pt for OH-internal version) applies during finalization (Phase 7+).

11.6 FORMAT AND THE DEVELOPMENT WORKFLOW

The relationship between format and workflow phase:

Phase 1-4 (Development). TXT or Markdown. Substance focus; formatting deferred. Easy AI re-processing for iteration.

Phase 5 (Appendices/Technical). TXT or Markdown. Consistent with earlier phases.

Phase 6 (Integration). Markdown or transitional HTML. Structure solidifying; formatting decisions maturing.

Phase 7 (Bifurcation). HTML for OH-internal (per formatting spec); Markdown or appropriate format for external version.

Phase 8 (Finalization). Final formats for each version. Conversion to delivery formats (PDF, DOCX) if required.

This progression reflects a principle: invest in formatting proportional to content stability. Early phases change frequently; formatting investment is wasted on content that will revise. Later phases stabilize; formatting investment pays off in final quality.

Cross-reference: See Project Guide Section 4 (Document Structure) for formatting specifications. See Section 6.2 (Log Hygiene) for artifact naming conventions.

11.7 PLATFORM-SPECIFIC CONSIDERATIONS

Different AI platforms handle formats differently. General patterns:

Claude. Accepts text, Markdown, code, PDFs, images. Handles multiple file uploads. Code block formatting generally preserves in output. Long documents may benefit from chunking for upload.

GPT-4/ChatGPT. Similar text and image handling. Plugin and custom GPT configurations affect available formats. Code interpreter features enable direct file manipulation.

Other Platforms. Vary significantly. Consult platform documentation for supported formats and known limitations.

When establishing workflows, test format handling with your specific platforms. Assumptions about "standard" format support often prove wrong in edge cases.

The principle: verify format compatibility early. Discovering format issues mid-project creates avoidable friction.



================================================================================
================================================================================

                        PART V: ADVANCED PATTERNS

================================================================================
================================================================================

This part addresses advanced operational patterns that emerge in sustained 
AI-assisted work. Where Parts II-IV establish foundational workflows, Part V 
addresses the complexity that accumulates across extended engagements: 
multi-session continuity, specialized project types, and systematic failure 
response.

These patterns assume fluency with core framework elements. Users should be 
comfortable with the template ecosystem (Section 5), the three-tier 
verification model (Section 8), and AI role separation (Section 9) before 
applying the advanced patterns described here.


================================================================================
SECTION 12: EXTENDED CONTEXT
AI sessions are bounded by token limits, but projects are not. Extended 
context management bridges this gap, maintaining coherent project state 
across sessions that may span days, weeks, or months. This section addresses 
the patterns that enable long-duration AI-assisted work without cumulative 
context degradation.


--------------------------------------------------------------------------------
12.1 Multi-Session Continuity
--------------------------------------------------------------------------------

Multi-session continuity is the central challenge of extended AI work. Each 
new session begins with no memory of prior sessions. Without deliberate 
continuity mechanisms, every session restart imposes a reconstruction cost 
that compounds over time.

The framework addresses this through three interconnected mechanisms:

Bootstrap Documents (TPL-002)
The bootstrap is the primary continuity vehicle. It captures project state, 
decision history, and next actions in a format optimized for AI ingestion. 
A well-constructed bootstrap enables a new AI instance to resume work with 
minimal context-setting overhead.

Bootstrap quality determines continuity quality. A bootstrap that captures 
"what" without "why" forces the next session to re-derive rationale. A 
bootstrap that omits open questions allows them to resurface as surprises. 
The bootstrap should enable the receiving instance to understand not just 
current state but the trajectory that produced it.

See Section 4.2 for bootstrap generation protocol and Section 10.8 for the 
bootstrap-first workflow tip.

Decision Logs (TPL-004)
Decisions accumulate across sessions. Without systematic logging, later 
sessions may revisit settled questions or make inconsistent choices. The 
Decision Log provides cumulative memory that bootstraps reference but don't 
fully contain.

Each decision entry should include rationale sufficient for a future reader 
to understand why the decision was made, not just what was decided. This 
prevents the "we decided X but I don't remember why" problem that leads to 
decision churn.

Log entries should be numbered sequentially across sessions. The bootstrap 
references decision numbers; the full Decision Log file provides the 
authoritative record. See Section 6.3.2 for Decision Log structure.

Chat Transcript Archives
The exported chat transcript (see Section 6.2.1 insert on Claude Exporter) 
serves as the raw record for verification and recovery. While bootstraps 
and logs are curated summaries, transcripts preserve the full exchange.

Transcripts are not for routine reference—they're too verbose. They serve 
two purposes: Tier 2/3 verification audits, and context recovery when 
bootstraps prove insufficient. Archive every session transcript. The 
storage cost is trivial; the recovery value is significant.

Continuity Anti-Patterns:

Relying on memory. Users sometimes assume AI will "remember" prior sessions 
through some mechanism other than explicit documents. It will not. Every 
session starts fresh. Treat session boundaries as complete memory wipes.

Verbal continuity. Explaining prior context conversationally rather than 
providing documents wastes tokens and introduces paraphrase errors. Provide 
the bootstrap; don't narrate it.

Bootstrap-only handoffs. Bootstraps are necessary but may not be sufficient 
for complex projects. Supplement with the Decision Log, relevant prior 
outputs, and the Project Guide (TPL-001) for projects extending beyond 
three sessions.


--------------------------------------------------------------------------------
12.2 Complex Project Management
--------------------------------------------------------------------------------

Complex projects are distinguished by: multiple parallel workstreams, 
extended duration (5+ sessions), multiple stakeholders, or dependencies 
between deliverables. These projects require management patterns beyond 
single-thread execution.

Project Architecture

Complex projects benefit from explicit architecture defined early:

Phase Structure. Divide the project into discrete phases with defined 
deliverables. Each phase should be achievable within 2-4 sessions. Phase 
boundaries are natural bootstrap points and progress checkpoints.

Workstream Mapping. Identify parallel workstreams and their dependencies. 
Document which workstreams can proceed independently and which have 
blocking dependencies. This informs session planning—work blocked streams 
when dependencies resolve, not before.

Deliverable Registry. Maintain a list of all expected deliverables with 
status tracking. This prevents the "what did we promise?" problem and 
provides a checklist for project completion verification.

The Project Guide (TPL-001) should capture this architecture. For complex 
projects, the Project Guide is a living document updated as understanding 
evolves.

Session Planning

Complex projects require deliberate session allocation:

Scope each session. Before beginning, define what the session will 
accomplish. "Continue working on the project" is insufficient. "Draft 
Sections 12-14 and generate bootstrap for Phase 5" is actionable.

Respect phase boundaries. Avoid starting new phases mid-session unless 
headroom is clearly sufficient. Phase transitions are natural closeout 
points.

Build in buffer. Complex projects generate unexpected complexity. Plan 
sessions at 70% of theoretical capacity to accommodate emergence.

Track cumulative progress. The Session Log (see Section 5.3) should 
reference what percentage of overall project is complete, not just what 
the session accomplished. This maintains perspective on trajectory.

Cross-Session Dependency Management

When Session N depends on output from Session M:

Explicit handoff. The bootstrap from Session M should flag dependencies 
for Session N. "Session N requires: [specific deliverable] before 
proceeding with [specific task]."

Verification before proceeding. Session N should confirm receipt of 
dependencies before substantive work. Missing dependencies discovered 
mid-session waste tokens on partial work that can't complete.

Dependency documentation. The Decision Log should capture dependency 
decisions: "Decided to proceed with X assuming Y from prior session; if 
Y proves incorrect, X requires revision."

Parallel Workstream Coordination

When multiple workstreams proceed simultaneously (potentially across 
different chat sessions):

Workstream isolation. Each session should focus on one workstream. 
Attempting to advance multiple workstreams in a single session creates 
context switching overhead and increases error risk.

Integration checkpoints. Schedule sessions specifically for integration—
combining outputs from parallel workstreams into coherent wholes. 
Integration sessions should not generate new content; they reconcile 
existing content.

Conflict resolution. Parallel work may produce inconsistencies. Establish 
a conflict resolution protocol: which workstream's decisions take 
precedence? How are conflicts surfaced and resolved? Document in the 
Project Guide.


================================================================================
SECTION 13: SPECIALIZED USE CASES
Different project types impose different demands on AI-assisted workflows. 
This section addresses patterns specific to research, creative, and 
technical projects. The core framework applies throughout; these patterns 
adapt it to domain-specific requirements.


--------------------------------------------------------------------------------
13.1 Research Projects
--------------------------------------------------------------------------------

Research projects are characterized by: evolving understanding, source 
management complexity, synthesis requirements, and citation integrity 
demands. The AI serves primarily as research assistant and synthesis 
engine rather than primary author.

Source Management

Research projects accumulate sources. Without systematic management, 
sources become a liability rather than an asset.

Source registry. Maintain a running list of sources consulted, with 
status indicators: reviewed, pending review, cited, discarded. This 
prevents duplicate retrieval and tracks coverage.

Source ingestion protocol. When introducing a source to the AI:
- Provide the source content (or relevant excerpts)
- Specify what you need from this source (extraction, analysis, synthesis)
- Indicate how this source relates to prior sources (confirms, contradicts, 
  extends)

The AI cannot access external sources independently in most configurations. 
User must provide source material. Adjunct tools like NotebookLM (see 
Section 10.3) can assist with source corpus management.

Citation integrity. AI-generated text that incorporates sources must 
maintain citation accuracy. Verify citations before finalizing—AI may 
paraphrase in ways that drift from source meaning, or conflate sources. 
This is a Tier 1 verification priority for research outputs.

Synthesis Patterns

Research often requires synthesizing across multiple sources:

Incremental synthesis. Don't ask AI to synthesize everything at once. 
Build synthesis incrementally: synthesize sources 1-3, then add source 4 
and re-synthesize, then add source 5, etc. This produces more coherent 
results than all-at-once synthesis.

Explicit synthesis instructions. Specify synthesis type: summarize 
consensus, identify disagreements, extract common themes, trace 
chronological development. "Synthesize these sources" is ambiguous; 
"Identify where these three sources agree and disagree on [specific 
question]" is actionable.

Synthesis verification. Cross-check synthesis claims against sources. 
AI may produce plausible-sounding synthesis that doesn't accurately 
reflect source content. See Section 8 for verification protocols.

Research-Specific Failure Modes

Hallucinated citations. AI may generate citations to sources that don't 
exist or misattribute claims to wrong sources. Verify every citation.

Overconfident synthesis. AI may present tentative findings as established 
consensus. Watch for hedging erosion—qualifiers in sources that disappear 
in synthesis.

Source conflation. When working with multiple sources, AI may blend 
content inappropriately. Maintain source boundaries in prompts: "From 
Source A specifically, what does the author argue about X?"


--------------------------------------------------------------------------------
13.2 Creative Projects
--------------------------------------------------------------------------------

Creative projects are characterized by: iterative refinement, subjective 
quality criteria, voice/style consistency requirements, and the need to 
balance AI capability with human creative direction. The AI serves as 
creative collaborator, not autonomous creator.

Creative Direction

The user retains creative authority. AI generates options; user directs 
selection and refinement.

Voice definition. Before generative work, establish voice parameters: 
tone, style, register, audience. Provide examples of target voice. The 
more specific the voice definition, the more consistent the output.

Iteration protocol. Creative work rarely succeeds in first draft. 
Establish iteration expectations:
- First pass: exploratory, multiple options
- Second pass: refinement of selected direction
- Third pass: polish and consistency check

Feedback specificity. "I don't like it" is insufficient feedback for 
AI iteration. Identify what specifically isn't working: pacing, word 
choice, structure, tone. AI responds well to specific critique; it 
struggles with vague dissatisfaction.

Consistency Management

Longer creative works require consistency across sessions:

Style guide. For extended projects, develop a style guide capturing: 
vocabulary preferences, structural patterns, voice markers, things to 
avoid. Provide this guide in each session's rooting.

Character/element bible. For fiction or complex content, maintain 
reference documents for recurring elements. Update as elements evolve. 
AI cannot maintain this across sessions without explicit documentation.

Continuity review. Before generating new content, have AI review recent 
prior content to re-establish voice and continuity. This is particularly 
important after session breaks.

Creative-Specific Failure Modes

Voice drift. Over extended sessions, AI voice may drift toward generic 
patterns. Monitor for homogenization; re-anchor to style guide when 
detected.

Sycophantic iteration. AI may interpret all feedback as request for 
change, even when feedback is observation. Be explicit: "This is 
observation, not change request" vs. "Please revise based on this 
feedback."

Creative flattening. AI may smooth away distinctive elements in 
pursuit of "improvement." Protect intentional stylistic choices by 
flagging them as non-negotiable.


--------------------------------------------------------------------------------
13.3 Technical Projects
--------------------------------------------------------------------------------

Technical projects are characterized by: precision requirements, 
testability, integration complexity, and the need for executable 
correctness. The AI serves as technical collaborator with verification 
emphasis.

Specification Discipline

Technical work requires precise specification:

Input/output definition. Before generating technical content, define 
expected inputs, outputs, and behavior. Ambiguous specifications produce 
ambiguous implementations.

Constraint documentation. Technical constraints (platform limitations, 
performance requirements, compatibility needs) should be documented in 
the Project Guide and referenced in session rooting.

Edge case enumeration. Explicitly identify edge cases that must be 
handled. AI may implement the happy path well but miss edge cases that 
weren't specified.

Verification Emphasis

Technical outputs require rigorous verification:

Test-first patterns. Where possible, define test cases before 
implementation. AI can generate tests from specifications; these tests 
then verify AI-generated implementations.

Incremental verification. Don't generate large technical outputs without 
intermediate verification. Generate component, verify, proceed. Large 
unverified generations compound errors.

Cross-platform validation. If output must work across platforms or 
environments, verify in each target environment. AI-generated code may 
work in one context and fail in another.

Integration Patterns

Technical outputs often integrate with existing systems:

Context provision. Provide relevant existing code, schemas, or 
interfaces. AI cannot infer integration requirements from description 
alone.

Integration testing. Test integration specifically, not just component 
functionality. Interfaces are where failures concentrate.

Rollback planning. Before deploying AI-generated technical changes, 
establish rollback procedures. Assume issues may emerge post-deployment.

Technical-Specific Failure Modes

Plausible but incorrect. Technical outputs may appear correct on 
inspection but fail in execution. Test; don't just review.

Outdated patterns. AI training has a knowledge cutoff. For rapidly 
evolving technical domains, verify that generated patterns reflect 
current best practices.

Over-engineering. AI may generate unnecessarily complex solutions. 
Simpler implementations are often more reliable and maintainable. 
Push back on complexity that isn't justified by requirements.


================================================================================
SECTION 14: FAILURE MODES
Failure in AI-assisted work is not exceptional; it is routine. The question 
is not whether failures will occur but how quickly they are detected and 
how effectively they are addressed. This section documents common failure 
patterns, recovery strategies, and the specific challenge of cognitive 
divergence.


--------------------------------------------------------------------------------
14.1 Common Pitfalls
--------------------------------------------------------------------------------

Failures cluster around predictable patterns. Recognizing these patterns 
enables proactive prevention and early detection.

Context Failures

Insufficient rooting. Starting substantive work before the AI has 
internalized project parameters. Symptoms: output that ignores stated 
constraints, questions about information already provided. Prevention: 
follow rooting protocol (Section 4.1); confirm alignment before drafting.

Context overload. Providing so much context that the AI cannot prioritize. 
Symptoms: responses that address tangential elements while missing core 
requirements. Prevention: curate context; prioritize what's essential 
for the immediate task.

Stale context. Working from outdated project documents or bootstraps. 
Symptoms: references to superseded decisions, work on abandoned 
directions. Prevention: verify document currency at session start; 
date-stamp all project artifacts.

Communication Failures

Ambiguous instructions. Requests that can be interpreted multiple ways. 
Symptoms: output that's technically responsive but misses intent. 
Prevention: be specific; provide examples of desired output; ask AI to 
confirm understanding before executing.

Implicit assumptions. Assuming AI knows things it cannot know without 
being told. Symptoms: outputs that seem inexplicably off-target. 
Prevention: make assumptions explicit; when something "goes without 
saying," say it anyway.

Feedback-iteration mismatch. Providing feedback that AI interprets 
differently than intended. Symptoms: iterations that don't address the 
feedback, or that address it too literally. Prevention: be explicit 
about feedback type (observation vs. change request) and scope (word 
choice vs. structural revision).

Process Failures

Skipped verification. Treating AI output as correct without checking. 
Symptoms: errors discovered downstream, often compounded. Prevention: 
maintain Tier 1 verification discipline; assume output contains errors 
until verified.

Inadequate logging. Failing to document decisions and rationale. 
Symptoms: revisiting settled questions, inconsistent decisions across 
sessions. Prevention: maintain Decision Log; treat logging as mandatory 
workflow step.

Session overrun. Continuing past optimal session length, causing 
quality degradation. Symptoms: warning signs from Section 4.2.4 
(instruction drift, repetitive loops, shallow responses). Prevention: 
monitor headroom; initiate closeout proactively.

Output Failures

Hallucination. AI generates false information presented as fact. 
Especially prevalent with: citations, specific claims about external 
entities, numerical precision. Prevention: verify factual claims; 
treat AI output as hypothesis, not established fact.

Sycophancy. AI agrees with user assertions even when they're incorrect, 
or shapes output to match perceived user preferences rather than 
accuracy. Symptoms: lack of pushback, excessive agreement, tone matching 
that sacrifices substance. Prevention: explicitly request critique; 
use adversarial validation (VERIF-001).

Format compliance failures. Output that doesn't match specified format 
requirements. Symptoms: wrong file type, incorrect structure, missing 
required elements. Prevention: provide format specifications and 
examples; verify format before content review.


--------------------------------------------------------------------------------
14.2 Recovery Strategies
--------------------------------------------------------------------------------

When failures occur, systematic recovery minimizes damage and restores 
productive workflow.

Immediate Response

Stop and assess. Don't continue work that's building on faulty output. 
Pause to understand what went wrong before proceeding.

Scope the failure. Is this a local error (fixable within current 
context) or a systemic issue (requiring session reset or approach 
change)? Local errors can often be corrected through targeted revision; 
systemic issues require more significant intervention.

Preserve the failure. Before attempting recovery, capture the failure 
state. This enables analysis, prevents loss of diagnostic information, 
and provides material for post-mortem.

Local Recovery

For errors that don't require session reset:

Targeted correction. Identify the specific error and request specific 
correction. "In paragraph 3, you stated X; this should be Y because Z. 
Please revise."

Constraint reinforcement. If the error resulted from constraint 
violation, restate the constraint before requesting revision. "Recall 
that all outputs must follow format F. Your output deviated at point P. 
Please regenerate with format compliance."

Verification before proceeding. After correction, verify the fix 
before building on it. Corrections sometimes introduce new errors.

Session-Level Recovery

When current session cannot productively continue:

Tag the exhausted session. Mark the chat name with EXH indicator per 
Section 4.2.5.

Export transcript. Capture full session record before context degrades 
further.

Initiate recovery branch. Start separate chat for context reconstruction 
(see Token Exhaustion Recovery Protocol, Section 4.2.5).

Fresh start with recovery bootstrap. New session begins with original 
bootstrap plus recovery bootstrap, explicitly noting the failure and 
its resolution.

Project-Level Recovery

When failures indicate systemic issues:

Project Guide review. Does the Project Guide still accurately capture 
project parameters? Failures sometimes indicate that project understanding 
has evolved beyond documentation.

Process audit. Review recent Session Logs and Decision Logs for patterns. 
Are failures concentrated in particular areas? Is a specific workflow 
step consistently problematic?

Framework adjustment. If the framework itself is producing failures, 
adjust. Add verification steps, modify templates, change session 
structure. Document changes in the Project Guide.


--------------------------------------------------------------------------------
14.3 Cognitive Divergence Detection
--------------------------------------------------------------------------------

Cognitive divergence occurs when the AI's understanding of the task 
systematically departs from the user's intent. Unlike simple errors 
(which are local and correctable), divergence is a state mismatch that 
corrupts ongoing work. Detecting divergence early is essential; 
undetected divergence produces outputs that are internally consistent 
but externally wrong.

This section fulfills forward references from Section 10.8 and the 
QA-001 prompt (see Golden Prompt Library).

Understanding Divergence

Divergence differs from error:

Error: The AI makes a mistake within a correct understanding. 
Correction is straightforward because the shared frame is intact.

Divergence: The AI is operating from a different understanding than 
the user. Corrections may not land because user and AI are addressing 
different mental models.

Divergence sources include:
- Ambiguous instructions interpreted differently than intended
- Context that the AI prioritized differently than the user expected
- Accumulated drift over long sessions as small mismatches compound
- AI making assumptions that weren't validated

Detection Signals

Early detection prevents divergence from compounding. Monitor for:

Persistent misses. Corrections don't stick. The AI acknowledges feedback 
but subsequent output exhibits the same issues. This suggests the 
correction addressed symptoms, not root cause.

Unexpected coherence. Outputs are internally consistent but don't 
connect to user expectations. The AI is building a logical structure, 
but from wrong premises.

Vocabulary mismatch. The AI uses terms slightly differently than the 
user, or interprets user terms in unexpected ways. This is often the 
first observable symptom of deeper divergence.

Over-elaboration in wrong directions. The AI expands extensively on 
aspects the user considers minor while underdeveloping aspects the 
user considers central. The priority hierarchy has diverged.

Confirmatory responses without confirmation. The AI says "I understand" 
or "Got it" but subsequent behavior suggests otherwise. Verbal 
acknowledgment is not evidence of actual alignment.

Three-Strike Protocol

When divergence is suspected:

Strike One: Direct correction. State the discrepancy explicitly. 
"You interpreted X as Y, but I meant Z." Request acknowledgment of 
the distinction.

Strike Two: Alignment audit. If correction doesn't resolve, use the 
Cognitive Divergence Audit prompt (QA-001): force the AI to identify 
where logic branched and what assumption caused divergence.

Strike Three: Session reset. If two correction attempts fail, the 
divergence may be too embedded to correct within session. Initiate 
closeout, generate bootstrap that explicitly documents the divergence, 
and restart with fresh context.

The three-strike threshold prevents both premature escalation (resetting 
when simple correction would work) and delayed escalation (continuing 
to iterate when the frame is broken).

Response Protocol

When divergence is detected and confirmed:

Halt generative work. Don't continue producing outputs that build on 
divergent understanding. Stop and re-align first.

Identify the branch point. Work backward: at what point did 
understanding diverge? What input or exchange triggered the split? 
This is diagnostic information for prevention.

Validate root assumptions. Before re-aligning, surface and check the 
foundational assumptions the AI is operating from. Divergence often 
originates in assumptions, not instructions.

Explicit re-rooting. Don't assume the AI will self-correct from 
discussion. Provide explicit, fresh rooting that replaces the 
divergent frame. Reference the Project Guide; restate core constraints; 
confirm alignment before resuming.

Document the incident. Log the divergence event: when detected, 
probable cause, resolution steps. This builds institutional knowledge 
for future prevention.

Escalation Triggers

Escalate beyond current session when:
- Three-strike protocol fails to resolve divergence
- Divergence has been present for significant portion of session 
  (substantial output is potentially compromised)
- Divergence points to Project Guide ambiguity requiring update
- Multiple recent sessions have exhibited divergence (pattern issue)

Escalation means: session closeout, explicit divergence documentation 
in bootstrap, fresh session with revised rooting, and Project Guide 
review for systemic issues.

Prevention

Reduce divergence likelihood through:

Explicit alignment checks. Don't assume alignment; verify it. 
Periodically ask AI to summarize current understanding. Use ALIGN-001 
prompt for major task transitions.

Assumption surfacing. At project start and after pivots, ask AI to 
state its assumptions. Correct misalignments before they embed.

Incremental validation. Verify alignment on small outputs before 
scaling. Divergence discovered in a paragraph is cheaper than 
divergence discovered in a chapter.

Clear negative space. Specify what you don't want, not just what you 
want. "Don't interpret X as Y" prevents common misreadings.


================================================================================
================================================================================

                    PART VI: MAIL & EXTERNAL COMMUNICATIONS

================================================================================
================================================================================

This part addresses AI integration with email and external communications. 
Where earlier parts focus on AI-assisted internal work, Part VI addresses 
workflows where AI output enters external channels—email to clients, 
communications to stakeholders, and other outward-facing contexts.

External communications carry higher stakes than internal work. Errors 
reach external parties. Tone mismatches damage relationships. The 
verification discipline from Section 8 applies with amplified importance.


================================================================================
SECTION 15: EMAIL INTEGRATION
Email remains a primary business communication channel. AI integration 
with email workflows requires attention to both input processing (using 
email as AI input) and output processing (using AI to generate email 
content).


--------------------------------------------------------------------------------
15.1 Email-to-AI Workflow
--------------------------------------------------------------------------------

Email content frequently requires AI processing: summarization, response 
drafting, information extraction, or action item identification. This 
subsection addresses patterns for bringing email content into AI workflows.

Email Ingestion

Email enters AI context through several paths:

Direct paste. For individual emails, copying content directly into the 
chat is often sufficient. Strip headers unless they're relevant to the 
task.

Batch processing. For multiple emails, compile into a structured 
document with clear email boundaries. Format:

  --- EMAIL 1 ---
  From: [sender]
  Date: [date]
  Subject: [subject]
  
  [body]
  
  --- EMAIL 2 ---
  ...

Attachment handling. Email attachments should be processed separately 
and provided to AI in appropriate format (see Section 11 for file type 
guidance). Don't assume AI can access attachments from email paste.

Privacy Considerations

Email content often contains sensitive information:

Redaction protocol. Before providing email to AI, review for content 
that shouldn't be processed externally. Redact personal information, 
confidential business data, or privileged content as appropriate.

Context limitation. Email context may persist in AI conversation 
history (depending on platform configuration). Consider whether email 
content is appropriate for the persistence policies of your AI platform.

Third-party content. Emails from external parties may contain 
confidential information about their operations. Exercise judgment 
about processing third-party content through AI.

Common Email Processing Tasks

Summarization. "Summarize the key points from this email thread" or 
"Identify the sender's main request and any deadlines mentioned."

Response drafting. "Draft a response that addresses points A, B, and C 
with tone appropriate for [relationship context]." See Section 15.2 for 
response generation patterns.

Information extraction. "Extract all dates, action items, and named 
parties from this email thread." Useful for populating task trackers or 
CRM systems.

Thread synthesis. For long email threads, "Summarize the discussion 
arc and current status" helps recover context before responding.

Sentiment analysis. "What is the sender's apparent emotional tone? Are 
there concerns or frustrations I should address directly?"


--------------------------------------------------------------------------------
15.2 AI-to-Email Workflow
--------------------------------------------------------------------------------

AI-generated email content requires careful handling before sending. 
Email reaches external parties; errors and tone mismatches have 
relational consequences.

Generation Protocol

When generating email content:

Context provision. Provide relationship context, not just message 
content. "This is a client who prefers direct communication" or 
"This is a first contact; err toward formality" shapes tone 
appropriately.

Prior thread. If responding to an existing thread, provide the thread. 
AI cannot craft appropriate response without knowing what it's 
responding to.

Constraints and goals. Be explicit about what the email needs to 
accomplish and what it needs to avoid. "The goal is to decline the 
meeting while preserving the relationship. Don't suggest alternative 
times."

Verification Requirements

All AI-generated email content requires review before sending:

Accuracy check. Verify factual claims. AI may generate plausible but 
incorrect details about dates, figures, or commitments.

Tone calibration. Read aloud. Does the tone match the relationship 
and situation? AI defaults may not match your communication style.

Commitment review. Identify anything that could be read as a 
commitment or promise. Confirm you intend to make that commitment.

Recipient perspective. Read as the recipient would. Are there 
ambiguities they might misinterpret? Phrases that might land 
differently than intended?

Template Development

For recurring email types, develop templates:

Identify high-frequency patterns. What email types do you send 
repeatedly with variation? These are template candidates.

Generate template with AI. Have AI draft a generalized template with 
clear variable markers: {{CLIENT_NAME}}, {{PROJECT_REFERENCE}}, 
{{SPECIFIC_REQUEST}}.

Refine through use. After several uses, review template performance. 
What variations have you made? Should those become part of the base 
template?

Store for reuse. Maintain email templates in accessible location. 
Templates are single-use prompts that have graduated to reusable assets.

Email-Specific Failure Modes

Register mismatch. AI defaults to a register that doesn't match your 
relationship with the recipient. Too formal, too casual, too 
corporate. Specify register explicitly.

Over-length. AI-generated emails often run longer than necessary. 
Request concision; specify approximate length; edit for density.

Hollow politeness. AI may include politeness phrases that read as 
insincere or formulaic. "I hope this email finds you well" is often 
better removed or personalized.

Missing human judgment. AI cannot assess whether sending a particular 
email is wise, only how to compose it well. Strategic judgment remains 
human responsibility.


================================================================================
SECTION 16: EXTERNAL COMMUNICATIONS
External communications extend beyond email to include client 
deliverables, stakeholder updates, public-facing content, and any 
material that represents you or your organization to outside parties. 
AI-assisted external communications require heightened attention to 
quality, accuracy, and appropriateness.


--------------------------------------------------------------------------------
16.1 Client Communications
--------------------------------------------------------------------------------

Client communications include: formal deliverables, progress reports, 
meeting materials, and ongoing correspondence. These communications 
shape client relationships and represent professional capability.

Quality Thresholds

Client-facing materials should meet higher quality standards than 
internal work:

Elevated verification. Consider Tier 2 verification (cross-AI 
validation) for significant client deliverables. The adversarial 
validator prompt (VERIF-001) is particularly relevant.

Style consistency. Client materials should be stylistically 
consistent with your established professional voice. AI-generated 
content may need adjustment to match organizational style.

Error intolerance. Errors in client materials damage credibility 
disproportionately. Proofread beyond standard levels; consider 
dedicated editing passes.

Client-Specific Customization

Adapt AI-generated content to client context:

Relationship history. Long-term clients have established 
communication patterns. New clients require different framing. 
Provide this context to AI when generating.

Client preferences. If a client prefers detailed technical 
explanations, or brief executive summaries, or visual presentations, 
specify these preferences. AI can adapt style if informed.

Political awareness. Some content may be sensitive given client 
internal dynamics. AI cannot know these dynamics; user must guide 
away from problematic framing.

Approval Workflows

For significant client communications, implement approval workflow:

Draft generation. AI produces initial draft.

Internal review. User reviews and revises draft.

Quality verification. Tier 1 minimum; Tier 2 for significant 
deliverables.

Approval gate. Designated approver confirms communication is 
appropriate before sending.

Send and archive. Document what was sent, when, to whom.

AI should not send client communications directly. AI generates 
content; human approves and sends. This gate is non-negotiable.


--------------------------------------------------------------------------------
16.2 Stakeholder Communications
--------------------------------------------------------------------------------

Stakeholder communications address parties with interest in your work 
beyond direct clients: partners, investors, regulators, media, public. 
These communications often have broader visibility and longer-term 
consequences.

Audience Analysis

Stakeholder communications require audience-specific framing:

Identify stakeholders. Who will read this? What do they already know? 
What do they care about? What concerns might they have?

Tailored framing. The same information may need different framing for 
different audiences. A technical update for engineers differs from an 
executive summary for investors.

Multi-audience considerations. If communication will reach multiple 
audiences, craft for the most sensitive audience while remaining 
accessible to others.

Risk Management

Stakeholder communications carry elevated risk:

Public record potential. Assume anything written may become public. 
Craft accordingly.

Commitment scrutiny. Statements to stakeholders may be treated as 
commitments. Be precise about what is promised vs. anticipated vs. 
aspirational.

Regulatory awareness. Communications to regulators or in regulated 
contexts have compliance implications. AI is not aware of regulatory 
nuance; user must verify compliance.

Media awareness. Communications that might reach media require 
additional care. Quotable phrases may be quoted. Ambiguities may be 
resolved against your interests.

Consistency Management

Across stakeholder communications, maintain consistency:

Messaging alignment. Stakeholder communications should align with each 
other and with public statements. Inconsistencies create credibility 
problems.

Position documentation. Document your stated positions on key topics. 
Reference this documentation when generating new communications to 
ensure consistency.

Temporal consistency. What you say now should be consistent with what 
you've said before. If positions have changed, acknowledge the change 
rather than pretending continuity.


================================================================================
SECTION 17: REFERENCE
This section provides reference resources for framework implementation: 
a quick reference guide for daily operations, a glossary of 
framework-specific terms, and an index for navigation (index to be 
completed in Phase 6).


--------------------------------------------------------------------------------
17.1 Quick Reference Guide
--------------------------------------------------------------------------------

This condensed reference supports daily operations. For detailed 
guidance, see the full sections referenced.

SESSION START CHECKLIST

[ ] Gather materials: bootstrap, Project Guide, relevant prior outputs
[ ] Root the session: provide context, confirm AI alignment (ALIGN-001)
[ ] State session objectives: specific deliverables for this session
[ ] Verify headroom: confirm token capacity for planned work
[ ] Initialize logs: Decision Log and Session Log accessible

MID-SESSION CHECKPOINTS

[ ] Monitor for warning signs (Section 4.2.4):
    - Instruction drift
    - Repetitive loops
    - Hallucinated references
    - Shallow responses
[ ] Log decisions as made (don't batch at session end)
[ ] Check alignment after major task completions
[ ] Request headroom assessment if in doubt

SESSION CLOSEOUT CHECKLIST

[ ] Complete deliverables or reach stable stopping point
[ ] Generate bootstrap if session continues (RECOV-001)
[ ] Update Decision Log with any unlogged decisions
[ ] Complete Session Log entry
[ ] Export chat transcript (Claude Exporter or equivalent)
[ ] Tag chat name if exhausted (EXH)
[ ] Archive to project folder structure

VERIFICATION QUICK REFERENCE

Tier 1 (Minimum - Every Output):
- Accuracy: Factual claims correct?
- Completeness: All requirements addressed?
- Coherence: Internal consistency?
- Format: Matches specifications?

Tier 2 (Elevated - Significant Deliverables):
- Secondary AI validation (VERIF-001)
- Cross-reference check
- Adversarial review

Tier 3 (Intensive - Critical/External):
- Human expert review
- Audit trail preparation
- Compliance verification if applicable

TEMPLATE QUICK REFERENCE

| Template | Code     | Purpose                              |
|----------|----------|--------------------------------------|
| Project Guide | TPL-001 | Project parameters and scope        |
| Bootstrap     | TPL-002 | Session handoff document            |
| Project Initiation | TPL-003 | New project setup               |
| Decision Log  | TPL-004 | Cumulative decision tracking         |
| Session Closeout | TPL-005 | Session completion protocol       |
| Weekly Review | TPL-006 | Periodic reflection                  |
| Session Log   | TPL-007 | Per-session activity record         |
| Validation Protocol | TPL-008 | Tier 2 verification script     |
| Failure Mode Handler | TPL-009 | Failure response procedures    |

COMMON PROMPTS

Alignment (session start):
"Review the provided materials. State your assumptions. Ask 3-5 
clarifying questions before we proceed."

Cognitive divergence check:
"I think we may be misaligned. State your current understanding of 
[X]. What assumptions are you making?"

Headroom check:
"Assess your current token headroom. Can you complete [specific 
task] with quality maintained?"

Bootstrap generation:
"Generate a bootstrap for the next session including: current state, 
decisions made, open items, and warning signs."


--------------------------------------------------------------------------------
17.2 Glossary
--------------------------------------------------------------------------------

Framework-specific terms. For common AI/ML terminology, consult 
standard references.

ALIGNMENT
The state of shared understanding between user and AI regarding task 
parameters, constraints, and expected outputs. See Section 4.1 for 
alignment protocols, ALIGN-001 for alignment prompt.

BOOTSTRAP
A document capturing project state, decisions, and next actions for 
handoff to a new AI session. The primary vehicle for multi-session 
continuity. See TPL-002, Section 12.1.

COGNITIVE DIVERGENCE
A systematic departure between AI's understanding and user's intent 
that corrupts ongoing work. Distinguished from simple error by its 
embedded nature. See Section 14.3.

DECISION LOG
Cumulative record of decisions made across sessions, including 
rationale. Prevents decision churn and supports bootstraps. See 
TPL-004, Section 6.3.2.

EXHAUSTION (Token Exhaustion)
State where AI session has consumed context capacity, leading to 
quality degradation. See Section 4.2.4 for warning signs, Section 
4.2.5 for recovery protocol.

GOLDEN PROMPT
A refined, high-performing prompt captured for reuse. See Section 
10.7, Golden Prompt Library.

GOVERNANCE STACK
The layered document architecture (constitutional documents, enabling 
resolutions, policies, forms, transactions) that provides authority 
framework for organizational decisions. See Section 7.

HEADROOM
Remaining token capacity in a session. Monitored to prevent exhaustion 
and ensure quality maintenance.

LOGIC STATE
The current "mental model" the AI is working from—its understanding of 
project parameters, decisions made, and task context. Captured in the 
Bootstrap for transfer to subsequent sessions. See TPL-002, Section 12.1.

PRIMARY AI
The AI instance performing main execution work on a task. See Section 
9.1 for role definition.

ROOT / ROOTING
The process of establishing AI context at session start. Providing 
necessary materials, confirming alignment, and preparing for 
substantive work. See Section 4.1.

SECONDARY AI
An AI instance used for validation, verification, or alternative 
perspective. Distinct from primary to prevent shared blind spots. 
See Section 9.2.

SESSION
A continuous period of AI interaction bounded by token limits or 
natural stopping points. Sessions are the primary unit of AI work 
organization.

STATIC REFERENCE
A specific label assigned to a source document to prevent the AI from 
conflating multiple documents during processing. Enables precise 
attribution and maintains source boundaries. See Section 13.1 on 
source management.

SYCOPHANCY
The tendency of an AI to agree with a user's assertions or leading 
questions, even when the user is incorrect. A failure mode that 
compromises output reliability. See Section 14.1, VERIF-001 prompt.

THREE-TIER VERIFICATION
The framework's quality assurance model: Tier 1 (self-verification), 
Tier 2 (cross-AI validation), Tier 3 (human expert review). See 
Section 8.

TOKEN
The fundamental unit of context in AI models. Approximately 3/4 of 
a word for English text. Token limits constrain session capacity.


--------------------------------------------------------------------------------
17.3 Index
--------------------------------------------------------------------------------

[PLACEHOLDER - Comprehensive index to be compiled in Phase 6a or Phase 7 
after all sections are finalized. Index will reference sections, templates, 
prompts, and key concepts with section numbers.]

Index categories will include:
- Templates (by code and by name)
- Prompts (from Golden Prompt Library)
- Concepts (alphabetical)
- Procedures (by type)
- Failure modes (by category)
- Cross-references (internal links)


================================================================================
================================================================================
================================================================================
================================================================================

APPENDIX A: TEMPLATE REFERENCE
This appendix contains the complete template suite for AI-assisted project 
management. All eleven templates are presented in full, directly usable form.

Templates are numbered sequentially (TPL-001 through TPL-011) and organized 
by priority tier as established during development.

--------------------------------------------------------------------------------
TEMPLATE INDEX
--------------------------------------------------------------------------------

HIGH PRIORITY (Core Operations):
  TPL-001  Project Initiation Form
  TPL-002  Bootstrap Template
  TPL-003  Session Closeout Checklist

MEDIUM PRIORITY (Ongoing Management):
  TPL-004  Decision Log Entry
  TPL-005  Session Log
  TPL-006  Weekly Review Agenda
  TPL-010  Project Roadmap
  TPL-022  Project Closure Report
  TPL-023  Backlog Register

LOWER PRIORITY (Specialized Use):
  TPL-007  Re-Entry Checklist
  TPL-008  Cross-AI Validation Protocol
  TPL-009  Failure Recovery Worksheet

REFERENCE (AI Quick Lookup):
  TPL-011  Dialog Intake Quick Reference

--------------------------------------------------------------------------------
USAGE GUIDANCE
--------------------------------------------------------------------------------

Template Selection:
- Every project: TPL-001 (initiation), TPL-002 (bootstraps), TPL-003 (closeout)
- Every session: TPL-005 (session log)
- When decisions made: TPL-004 (decision log)
- Multi-phase projects: TPL-010 (project roadmap)
- Phase transitions: TPL-010 (update roadmap, generate new phase detail)
- Weekly: TPL-006 (weekly review)
- After absence: TPL-007 (re-entry)
- Major deliverables: TPL-008 (cross-AI validation)
- When failures occur: TPL-009 (failure recovery)
- Dialog intake: TPL-011 (AI quick reference for intake procedures)
- Deferred items: TPL-023 (backlog register, maintain throughout project)
- Project completion: TPL-022 (project closure report)

Customization:
Templates may be adapted to specific project needs. When adapting:
- Preserve core structure and required fields
- Document adaptations in Project Guide
- Maintain consistency within a project
- Consider creating project-specific variants for recurring patterns


================================================================================
TPL-001: PROJECT INITIATION FORM
Template ID:     GOV-AIPM-TPL-001
Version:         1.0
Purpose:         User completes before starting AI project; provides AI with 
                 project parameters and establishes shared expectations
When to Use:     At the start of any new AI-assisted project or major workstream

--------------------------------------------------------------------------------
INSTRUCTIONS
--------------------------------------------------------------------------------

Complete this form before initiating your first AI chat for a new project. 
Provide this completed form to the AI at the start of the root chat to 
establish shared context, expectations, and working parameters.

Tips:
- Be specific about scope boundaries—what's OUT is as important as what's IN
- Define success criteria that are measurable or clearly verifiable
- Specify file output expectations so the AI knows what to produce
- Include any constraints the AI should know about (timeline, format, etc.)

DIALOG COMPLETION OPTION:
This template can be completed through dialog with the AI. Say:
"Help me complete TPL-001" or "Walk me through Project Initiation"
See Section 3.5 for the Dialog Intake System.

--------------------------------------------------------------------------------
PROJECT INITIATION FORM
--------------------------------------------------------------------------------

DATE: _______________

--------------------------------------------------------------------------------
1. PROJECT IDENTIFICATION
--------------------------------------------------------------------------------

Project Name (Full): 
_____________________________________________________________________________

Project Name (Abbreviated): 
_____________________________________________________________________________

Project Code: 
[###] [DEPT]-[PROJ]
Example: 001 GOV-AIPM

_____________________________________________________________________________

Root Chat Name:
[###] [DEPT]-[PROJ]-[PHASE] ([semantic notes])
Example: 001 GOV-AIPM-RD (root alignment)

_____________________________________________________________________________

--------------------------------------------------------------------------------
2. PROJECT OBJECTIVE
--------------------------------------------------------------------------------

State the primary objective in 1-2 sentences. What is this project meant to 
accomplish?

_____________________________________________________________________________

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
3. SCOPE
--------------------------------------------------------------------------------

IN SCOPE (What this project WILL address):

1. ___________________________________________________________________________

2. ___________________________________________________________________________

3. ___________________________________________________________________________

4. ___________________________________________________________________________

5. ___________________________________________________________________________


OUT OF SCOPE (What this project will NOT address):

1. ___________________________________________________________________________

2. ___________________________________________________________________________

3. ___________________________________________________________________________

--------------------------------------------------------------------------------
4. KEY CONSTRAINTS
--------------------------------------------------------------------------------

List any constraints the AI should be aware of (timeline, budget, format 
requirements, regulatory considerations, dependencies, etc.):

1. ___________________________________________________________________________

2. ___________________________________________________________________________

3. ___________________________________________________________________________

4. ___________________________________________________________________________

--------------------------------------------------------------------------------
5. DELIVERABLES
--------------------------------------------------------------------------------

What specific outputs will this project produce?

| Deliverable | Description | Format | Audience |
|-------------|-------------|--------|----------|
|             |             |        |          |
|             |             |        |          |
|             |             |        |          |
|             |             |        |          |

--------------------------------------------------------------------------------
6. SUCCESS CRITERIA
--------------------------------------------------------------------------------

How will you know this project is complete and successful? List measurable or 
clearly verifiable criteria:

1. ___________________________________________________________________________

2. ___________________________________________________________________________

3. ___________________________________________________________________________

4. ___________________________________________________________________________

--------------------------------------------------------------------------------
7. NAMING CONVENTION
--------------------------------------------------------------------------------

Confirm the naming convention to be used for this project:

Chat Naming Format:
[ ] Standard: [###] [DEPT]-[PROJ]-[PHASE] ([semantic notes])
[ ] Custom: __________________________________________________________________

Department Code: _____________

Project Abbreviation: _____________

File Naming Prefix: _____________

--------------------------------------------------------------------------------
8. FILE OUTPUT EXPECTATIONS
--------------------------------------------------------------------------------

What files should the AI generate at the end of each chat session?

[ ] Session Log
[ ] Decision Log (if decisions made)
[ ] Bootstrap (if continuation needed)
[ ] Chat transcript export reminder
[ ] Deliverable drafts
[ ] Other: ___________________________________________________________________

Where should files be organized?

Primary Chat Folder: __________________________________________________________

Functional Folder(s) for Duplication: _________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
9. LOG & BOOTSTRAP REQUIREMENTS
--------------------------------------------------------------------------------

SESSION LOGGING:
[ ] Full session log each chat
[ ] Brief session notes only
[ ] Log only on significant sessions

DECISION LOGGING:
[ ] Log all decisions with full rationale
[ ] Log major decisions only
[ ] Decision log not required for this project

BOOTSTRAP REQUIREMENTS:
[ ] Bootstrap required at end of every session
[ ] Bootstrap only when continuing to new chat
[ ] Bootstrap only for branch chats
[ ] AI should assess and recommend

Bootstrap Detail Level:
[ ] Comprehensive (full context transfer)
[ ] Standard (key points and next actions)
[ ] Minimal (next actions only)

--------------------------------------------------------------------------------
10. SPECIAL INSTRUCTIONS
--------------------------------------------------------------------------------

Any additional guidance, preferences, or context the AI should know:

_____________________________________________________________________________

_____________________________________________________________________________

_____________________________________________________________________________

_____________________________________________________________________________

_____________________________________________________________________________

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
11. REFERENCE MATERIALS
--------------------------------------------------------------------------------

List any documents, files, or resources the AI should reference:

| Document/Resource | Location | Purpose |
|-------------------|----------|---------|
|                   |          |         |
|                   |          |         |
|                   |          |         |
|                   |          |         |

--------------------------------------------------------------------------------
12. QUALITY ASSURANCE LEVEL
--------------------------------------------------------------------------------

What level of verification is appropriate for this project?

[ ] Tier 1 Only: In-session verification (standard for routine work)
[ ] Tier 1 + 2: Cross-AI validation for deliverables
[ ] Tier 1 + 2 + 3: Human review required for critical outputs

Specific QA notes: ___________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
13. CONFIDENTIALITY CLASSIFICATION
--------------------------------------------------------------------------------

Default data classification for this project:

[ ] Tier 1 - Public/General: No restrictions
[ ] Tier 2 - Internal: Business-sensitive, not client-specific
[ ] Tier 3 - Confidential: Client-identifiable or legally sensitive
[ ] Tier 4 - Restricted: Contains restricted data (requires anonymization)

Notes on data handling: ______________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
FORM COMPLETE
--------------------------------------------------------------------------------

Provide this completed form to the AI at the start of your root chat.

After providing this form, ask the AI:
- "Do you have any questions about this project before we begin?"
- "Please restate your understanding of the objective and scope."

--------------------------------------------------------------------------------
END OF TPL-001
--------------------------------------------------------------------------------


================================================================================
TPL-002: BOOTSTRAP TEMPLATE
Template ID:     GOV-AIPM-TPL-002
Version:         1.1
Purpose:         Standard structure for session transitions; enables clean 
                 handoff between AI instances with full context preservation
When to Use:     - End of any chat session that will continue in a new chat
                 - When branching to a parallel workstream
                 - When approaching token limits
                 - When shifting focus requires fresh context

--------------------------------------------------------------------------------
INSTRUCTIONS
--------------------------------------------------------------------------------

Complete this bootstrap at the end of a chat session before transitioning to 
a new chat. The bootstrap captures everything the next AI instance needs to 
continue work without context loss.

AI Coordinator Role: When operating within this framework, the AI generates 
bootstraps proactively at session end as part of its Continuity Assurance 
Duty (see Section 2.7.6). Users should expect the AI to initiate bootstrap 
generation without being asked.

Tips:
- Write for an AI that has no memory of previous chats
- Be explicit about what's decided vs. what's still open
- Include enough context to orient, but not so much it consumes token budget
- List files the next session should request if it needs more detail
- Distinguish between "must have" and "nice to have" context

Bootstrap Types:
- CONTINUATION: Same project, sequential work, picking up where left off
- BRANCH: Parallel workstream, may rejoin main trunk later
- NEW INSTANCE: Fresh start with new AI instance, full context transfer

DIALOG COMPLETION OPTION:
This template can be completed through dialog with the AI. Say:
"Help me complete TPL-002" or "Walk me through Bootstrap Template"
See Section 3.5 for the Dialog Intake System.

--------------------------------------------------------------------------------
BOOTSTRAP TEMPLATE
--------------------------------------------------------------------------------

================================================================================
BOOTSTRAP: [Project Name] [Phase/Purpose]
PROJECT IDENTIFICATION
----------------------
Project Name (Full):    [Full project name]
Project Name (Abbrev):  [Abbreviated name]
Project Code:           [Code, e.g., GOV-AIPM]

SOURCE CHAT
-----------
Chat:                   [Chat name, e.g., 001 GOV-AIPM-RD (root alignment)]
Date:                   [Date of source chat]
Type:                   [R&D / Execution / Review / Branch]

TARGET CHAT
-----------
Chat:                   [Target chat name, e.g., 002 GOV-AIPM-EX (template dev)]
Bootstrap Type:         [CONTINUATION / BRANCH / NEW INSTANCE]
Phase:                  [Phase number and name]

================================================================================
SESSION REQUIREMENTS
MANDATORY OUTPUTS:
- [ ] Session Log (TPL-005) generated at closeout
- [ ] Decision Log (TPL-004) generated if trigger criteria met
- [ ] Bootstrap (TPL-002) generated if project continues

DECISION LOG TRIGGER CRITERIA:
Generate a dedicated Decision Log file when ANY of the following apply:
- Three or more decisions were made during the session
- Any decision affects project scope, timeline, or approach
- Any decision changes template design or manual structure
- Any decision will need to be referenced in future sessions
- User explicitly requests decision documentation

AI SELF-CHECK: Before closeout, evaluate: "Were decisions made this session 
that future sessions will need to reference, or that affect the project's 
direction?" If yes, generate Decision Log. If uncertain, ask user.

USER CLOSEOUT DUTIES (AI to prompt):
- [ ] Export chat transcript
- [ ] Download AI-produced files
- [ ] Organize files per folder convention
- [ ] Update personal task tracking as needed

================================================================================
PROJECT CONTEXT
[Provide 2-4 sentences describing the project. Include enough context for the 
AI to understand what this project is about and why it matters. Reference the 
Project Guide for full details.]

Key Constraints Still in Effect:
- [Constraint 1]
- [Constraint 2]
- [Constraint 3]

================================================================================
CURRENT STATE
[Describe where the project stands. What phase are we in? What's been 
established? What decisions are locked in? What's working well?]

Active Parameters:
- [Parameter 1, e.g., Naming convention: [###] [DEPT]-[PROJ]-[PHASE]]
- [Parameter 2, e.g., Output format: TXT for development]
- [Parameter 3]

--------------------------------------------------------------------------------
MINI PROJECT MAP
--------------------------------------------------------------------------------

┌─────────────────────────────────────────────────────────────────────────────┐
│  YOU ARE HERE: Phase [#], Session [###]                                     │
│                                                                             │
│  STATUS:       [One-line summary, e.g., "3/8 issues complete, on track"]   │
│                                                                             │
│  JUST COMPLETED: [What was finished this session]                          │
│                                                                             │
│  NEXT UP:      [Immediate next work for target session]                    │
└─────────────────────────────────────────────────────────────────────────────┘

PHASE PROGRESS:
- [Prior phases status summary]
- [Current phase status]
- [Upcoming phases preview]

================================================================================
COMPLETED WORK
[Summarize what was accomplished in the source chat. Be specific enough that 
the next AI can build on this work without re-doing it.]

Summary:
- [Accomplishment 1]
- [Accomplishment 2]
- [Accomplishment 3]

Key Decisions Made:
[Reference decision log for full rationale; list decisions briefly here]

| Decision | Summary |
|----------|---------|
| #        |         |
| #        |         |
| #        |         |

================================================================================
END-OF-SESSION FILES FROM SOURCE CHAT
[List all files generated at the end of the source chat. Include brief 
descriptions so the next AI knows what to request if additional context needed.]

| File Name | Description | Location |
|-----------|-------------|----------|
|           |             |          |
|           |             |          |
|           |             |          |
|           |             |          |

Note: Request these files if you need additional context beyond this bootstrap.

================================================================================
OPEN ITEMS
[List what remains to be done. Include known issues, blockers, or questions 
that need resolution.]

Remaining Work:
- [ ] [Item 1]
- [ ] [Item 2]
- [ ] [Item 3]
- [ ] [Item 4]

Known Issues/Blockers:
- [Issue 1, if any]
- [Issue 2, if any]

Questions Requiring Resolution:
- [Question 1, if any]
- [Question 2, if any]

================================================================================
IMMEDIATE NEXT ACTIONS
[Prioritized list of what to do first in the target chat. Be specific and 
actionable.]

1. [First action]
2. [Second action]
3. [Third action]
4. [Fourth action]

================================================================================
CONSTRAINTS & PARAMETERS
Token Management:
- [Note any token concerns or guidance for the target session]
- [e.g., "Monitor headroom—9 templates to complete; bootstrap if >70% consumed"]

Format Requirements:
- [e.g., "TXT format for all templates"]
- [e.g., "HTML versions deferred to Phase 5"]

Quality Thresholds:
- [e.g., "Tier 1 verification on all outputs"]
- [e.g., "Templates must be immediately usable"]

================================================================================
FILES TO REFERENCE
[List files the target chat should have access to. Distinguish essential from 
supplementary.]

ESSENTIAL (provide at session start):
- [File 1, e.g., GOV-AIPM_Project_Guide_v1.01.txt]
- [File 2]

SUPPLEMENTARY (request if needed):
- [File 3]
- [File 4]

================================================================================
SPECIAL INSTRUCTIONS
[Any session-specific guidance, deviations from standard process, or context 
the AI should keep in mind.]

- [Instruction 1]
- [Instruction 2]
- [Instruction 3]

================================================================================
QUESTIONS TO ADDRESS AT SESSION START
[List any questions the AI should ask or confirm before proceeding.]

Before proceeding, confirm:
- [Question 1]
- [Question 2]
- [Question 3]

================================================================================
USER ACTIONS REQUIRED
Before starting target session:
- [ ] Export source chat transcript
- [ ] Download all source session files:
      - [File 1]
      - [File 2]
      - [File 3]
- [ ] Organize files in project folder
- [ ] Review outputs if desired

At target session start:
- Provide this bootstrap
- Provide essential files listed above
- State readiness to proceed

================================================================================
NEXT SESSION QUICK-START
Copy this prompt to start your next session:

"I'm continuing [Project Name] [Phase/Purpose]. Attached:
- Bootstrap: [Bootstrap filename]
- [Essential file 1]
- [Essential file 2]

Please confirm receipt and alignment."

Files to attach:
1. [This bootstrap filename]
2. [Essential file 1]
3. [Essential file 2]
4. [Optional file, if needed]

================================================================================
END OF BOOTSTRAP
--------------------------------------------------------------------------------
END OF TPL-002
--------------------------------------------------------------------------------


================================================================================
TPL-003: SESSION CLOSEOUT CHECKLIST
Template ID:     GOV-AIPM-TPL-003
Version:         1.0
Purpose:         Ensure consistent session endings with complete documentation 
                 and proper handoff preparation
When to Use:     At the end of every AI chat session, before closing the chat

--------------------------------------------------------------------------------
INSTRUCTIONS
--------------------------------------------------------------------------------

Use this checklist at the end of every AI chat session. Work through each 
section systematically to ensure nothing is missed. The AI can help complete 
this checklist as part of the session closeout process.

Tip: Request this checklist from the AI near the end of a session:
"Let's run through the session closeout checklist before we wrap up."

DIALOG COMPLETION OPTION:
This template can be completed through dialog with the AI. Say:
"Help me complete TPL-003" or "Walk me through Session Closeout"
See Section 3.5 for the Dialog Intake System.

--------------------------------------------------------------------------------
SESSION CLOSEOUT CHECKLIST
--------------------------------------------------------------------------------

SESSION IDENTIFICATION
----------------------
Chat Name:        ___________________________________________________________
Date:             _______________
Session Duration: _______________ (approximate)

--------------------------------------------------------------------------------
1. WORK COMPLETED THIS SESSION
--------------------------------------------------------------------------------

List major accomplishments:

- [ ] ________________________________________________________________________

- [ ] ________________________________________________________________________

- [ ] ________________________________________________________________________

- [ ] ________________________________________________________________________

- [ ] ________________________________________________________________________


Deviations from planned work (if any):

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
2. DECISIONS MADE
--------------------------------------------------------------------------------

List decisions made this session (transfer to Decision Log with full rationale):

| # | Decision | Rationale Summary |
|---|----------|-------------------|
|   |          |                   |
|   |          |                   |
|   |          |                   |
|   |          |                   |

- [ ] All decisions captured above
- [ ] Rationale documented for each decision
- [ ] Decision log entry created/updated

--------------------------------------------------------------------------------
3. FILES GENERATED
--------------------------------------------------------------------------------

List all files created this session:

| File Name | Purpose | Location |
|-----------|---------|----------|
|           |         |          |
|           |         |          |
|           |         |          |
|           |         |          |
|           |         |          |

- [ ] All files listed above
- [ ] File purposes clear

--------------------------------------------------------------------------------
4. CHAT TRANSCRIPT EXPORT
--------------------------------------------------------------------------------

- [ ] Chat exported via Claude Exporter (https://www.claudexporter.com)

Export format(s) saved:
- [ ] MD (Markdown) — recommended for primary archive
- [ ] TXT (Plain text)
- [ ] PDF (Presentation/formal archive)

- [ ] Export saved to chat folder

File name: ___________________________________________________________________

--------------------------------------------------------------------------------
5. BOOTSTRAP STATUS
--------------------------------------------------------------------------------

Bootstrap needed?
- [ ] YES — Continuation to next session
- [ ] YES — Branch to parallel workstream
- [ ] NO — Project complete or natural pause point

If YES:

- [ ] Bootstrap generated

- [ ] Bootstrap type indicated (Continuation / Branch / New Instance)

- [ ] End-of-session files documented in bootstrap

- [ ] Next actions clearly specified

- [ ] Files to reference listed

Bootstrap file name: _________________________________________________________

--------------------------------------------------------------------------------
6. FILE MANAGEMENT
--------------------------------------------------------------------------------

CHAT FOLDER:
- [ ] All session files saved to chat folder
- [ ] Folder structure follows convention:
      [Chat Name]/
      ├── bootstrap_in.txt
      ├── bootstrap_out.txt
      ├── session_log.txt
      ├── decision_log.txt
      ├── chat_transcript.md
      ├── drafts/
      ├── deliverables/
      └── notes/

FUNCTIONAL FOLDERS:
- [ ] Deliverables duplicated to appropriate functional folders
- [ ] Project guide updated (if applicable)
- [ ] Decision log duplicated to project folder (if applicable)

Functional folders updated:

_____________________________________________________________________________

_____________________________________________________________________________

NAMING CONVENTIONS:
- [ ] All files follow naming convention
- [ ] No orphaned or mislabeled files

--------------------------------------------------------------------------------
7. SESSION DOCUMENTATION
--------------------------------------------------------------------------------

SESSION LOG:
- [ ] Session log entry created
- [ ] Session objective recorded
- [ ] Work accomplished documented
- [ ] Issues encountered noted
- [ ] Files produced listed
- [ ] Bootstrap status recorded
- [ ] Chat export status recorded
- [ ] Next session notes included

DECISION LOG:
- [ ] All decisions from Section 2 logged with full rationale
- [ ] Decision IDs assigned
- [ ] Related documents referenced

--------------------------------------------------------------------------------
8. NEXT SESSION PREPARATION
--------------------------------------------------------------------------------

Next session focus:

_____________________________________________________________________________

_____________________________________________________________________________

Required inputs for next session:

- [ ] ________________________________________________________________________

- [ ] ________________________________________________________________________

- [ ] ________________________________________________________________________

Blockers or dependencies:

_____________________________________________________________________________

_____________________________________________________________________________

Questions to resolve before next session:

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
9. QUALITY CHECK
--------------------------------------------------------------------------------

- [ ] Session objective achieved (or deviation documented)
- [ ] All outputs reviewed for accuracy
- [ ] No incomplete work left undocumented
- [ ] Handoff is clean (next session can proceed without confusion)
- [ ] AI stability verified (no signs of context drift or degradation)

--------------------------------------------------------------------------------
10. FINAL CONFIRMATION
--------------------------------------------------------------------------------

- [ ] All checklist items complete
- [ ] Ready to close chat

Closeout completed by: _______________________________________________________

Date/Time: ___________________________________________________________________

--------------------------------------------------------------------------------
SESSION CLOSEOUT COMPLETE
--------------------------------------------------------------------------------

Chat may now be closed. All artifacts preserved for continuity.

--------------------------------------------------------------------------------
END OF TPL-003
--------------------------------------------------------------------------------


================================================================================
TPL-004: DECISION LOG ENTRY
Template ID:     GOV-AIPM-TPL-004
Version:         1.0
Purpose:         Capture decisions with full context for future reference, 
                 audit trail, and prevention of decision churn
When to Use:     Whenever a significant decision is made during AI-assisted work

--------------------------------------------------------------------------------
INSTRUCTIONS
--------------------------------------------------------------------------------

Use this template to log decisions as they are made. A decision log is 
append-only at the raw level—entries are added but never deleted or modified. 
If a decision is revisited, add a new entry that references the original.

What constitutes a "significant decision"?
- Choices that affect project direction or scope
- Selections between meaningful alternatives
- Commitments that constrain future options
- Anything you might need to explain or justify later

Tips:
- Log decisions as close to when they're made as possible
- Be explicit about alternatives considered—this prevents re-litigation
- Include rationale even if it seems obvious now; it won't be later
- Reference related documents so context can be reconstructed

DIALOG COMPLETION OPTION:
This template can be completed through dialog with the AI. Say:
"Help me complete TPL-004" or "Walk me through Decision Log Entry"
See Section 3.5 for the Dialog Intake System.

--------------------------------------------------------------------------------
DECISION LOG
--------------------------------------------------------------------------------

PROJECT: ____________________________________________________________________

LOG MAINTAINED BY: ___________________________________________________________

--------------------------------------------------------------------------------
DECISION ENTRY
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
DECISION #[___]
--------------------------------------------------------------------------------

DATE/TIME:          ___________________________________________________________

SESSION:            ___________________________________________________________
                    (Chat name where decision was made)

--------------------------------------------------------------------------------
DECISION STATEMENT
--------------------------------------------------------------------------------

[State the decision clearly and actionably. What was decided?]

_____________________________________________________________________________

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
CONTEXT / TRIGGER
--------------------------------------------------------------------------------

[What prompted this decision? What question or issue required resolution?]

_____________________________________________________________________________

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
ALTERNATIVES CONSIDERED
--------------------------------------------------------------------------------

[List at least 2 alternatives that were considered. Include the option that 
was NOT selected and why.]

Alternative A: _______________________________________________________________

_____________________________________________________________________________

Why not selected: ____________________________________________________________

_____________________________________________________________________________


Alternative B: _______________________________________________________________

_____________________________________________________________________________

Why not selected: ____________________________________________________________

_____________________________________________________________________________


Alternative C (if applicable): _______________________________________________

_____________________________________________________________________________

Why not selected: ____________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
RATIONALE FOR SELECTION
--------------------------------------------------------------------------------

[Explain why this decision was made. What factors were most important? What 
trade-offs were accepted?]

_____________________________________________________________________________

_____________________________________________________________________________

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
EXPECTED OUTCOME / IMPACT
--------------------------------------------------------------------------------

[What do you expect to happen as a result of this decision? How will it 
affect the project?]

_____________________________________________________________________________

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
RELATED DOCUMENTS / REFERENCES
--------------------------------------------------------------------------------

[List any documents, prior decisions, or resources that informed this decision]

| Document/Reference | Relevance |
|--------------------|-----------|
|                    |           |
|                    |           |
|                    |           |

--------------------------------------------------------------------------------
REVIEW TRIGGER (if applicable)
--------------------------------------------------------------------------------

[Under what conditions should this decision be revisited? Leave blank if the 
decision is final.]

Revisit this decision if:

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
DECISION STATUS
--------------------------------------------------------------------------------

[ ] ACTIVE — Decision is in effect
[ ] SUPERSEDED — Replaced by Decision #[___]
[ ] DEFERRED — Postponed; revisit on [date/trigger]

--------------------------------------------------------------------------------
END OF ENTRY
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
TEMPLATE USAGE NOTES
--------------------------------------------------------------------------------

DECISION LOG FILE NAMING:
- Per project: [PROJECT-CODE]_Decision_Log.txt
- Example: GOV-AIPM_Decision_Log.txt

DECISION NUMBERING:
- Sequential within project: #1, #2, #3...
- If superseding a prior decision, note: "Supersedes Decision #X"

LOG MAINTENANCE:
- Append new entries at the end of the log file
- Never delete or modify existing entries
- If correcting an error, add a new entry noting the correction

INTEGRATION WITH SESSION CLOSEOUT:
- At session end, transfer decisions from checklist to this log format
- Brief version goes in session log; full version goes here

--------------------------------------------------------------------------------
END OF TPL-004
--------------------------------------------------------------------------------


================================================================================
TPL-005: SESSION LOG
Template ID:     GOV-AIPM-TPL-005
Version:         1.0
Purpose:         Record what occurred in a chat session for continuity, 
                 project tracking, and audit trail
When to Use:     At the end of every AI chat session

--------------------------------------------------------------------------------
INSTRUCTIONS
--------------------------------------------------------------------------------

Complete a session log entry at the end of every AI chat session. This log 
provides a running record of project activity and serves as a quick reference 
for understanding project history.

Session logs are distinct from decision logs:
- Session Log: Records what happened (activities, outputs, issues)
- Decision Log: Records what was decided (choices, rationale, alternatives)

Tips:
- Complete the session log as part of the Session Closeout Checklist
- Keep entries concise—this is a summary, not a transcript
- Reference the decision log for decision details rather than duplicating
- Note issues and blockers even if unresolved; they inform future sessions
- Export chat transcript via Claude Exporter before closing

DIALOG COMPLETION OPTION:
This template can be completed through dialog with the AI. Say:
"Help me complete TPL-005" or "Walk me through Session Log"
See Section 3.5 for the Dialog Intake System.

--------------------------------------------------------------------------------
SESSION LOG
--------------------------------------------------------------------------------

PROJECT: ____________________________________________________________________

PROJECT CODE: _______________________________________________________________

LOG MAINTAINED BY: ___________________________________________________________

--------------------------------------------------------------------------------
SESSION ENTRY
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
SESSION: [Chat Name]
--------------------------------------------------------------------------------

DATE:               ___________________________________________________________

DURATION:           ___________________________________________________________ 
                    (approximate)

SESSION TYPE:       [ ] R&D    [ ] Execution    [ ] Review    [ ] Branch

--------------------------------------------------------------------------------
SESSION OBJECTIVE
--------------------------------------------------------------------------------

[What was this session intended to accomplish?]

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
WORK ACCOMPLISHED
--------------------------------------------------------------------------------

[What was actually done? List major activities and outputs.]

1. ___________________________________________________________________________

2. ___________________________________________________________________________

3. ___________________________________________________________________________

4. ___________________________________________________________________________

5. ___________________________________________________________________________

Objective achieved?  [ ] Yes    [ ] Partial    [ ] No

If partial or no, explain: ___________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
DECISIONS MADE
--------------------------------------------------------------------------------

[List decisions briefly; reference Decision Log for full rationale]

| Decision # | Brief Description |
|------------|-------------------|
|            |                   |
|            |                   |
|            |                   |

Total decisions this session: _____

--------------------------------------------------------------------------------
ISSUES ENCOUNTERED
--------------------------------------------------------------------------------

[Note any problems, blockers, or unexpected challenges]

| Issue | Status | Notes |
|-------|--------|-------|
|       |        |       |
|       |        |       |
|       |        |       |

Status options: Resolved / Open / Deferred

--------------------------------------------------------------------------------
FILES PRODUCED
--------------------------------------------------------------------------------

[List files generated this session]

| File Name | Type | Description |
|-----------|------|-------------|
|           |      |             |
|           |      |             |
|           |      |             |
|           |      |             |

Total files: _____

--------------------------------------------------------------------------------
BOOTSTRAP STATUS
--------------------------------------------------------------------------------

Bootstrap generated?    [ ] Yes    [ ] No

If yes:
- Type:                 [ ] Continuation    [ ] Branch    [ ] New Instance
- File name:            _____________________________________________________
- Target chat:          _____________________________________________________

--------------------------------------------------------------------------------
CHAT EXPORT STATUS
--------------------------------------------------------------------------------

Chat exported?          [ ] Yes    [ ] No

If yes:
- Format(s):            [ ] MD    [ ] TXT    [ ] PDF
- File name:            _____________________________________________________
- Saved to:             _____________________________________________________

--------------------------------------------------------------------------------
NEXT SESSION NOTES
--------------------------------------------------------------------------------

Recommended focus for next session:

_____________________________________________________________________________

_____________________________________________________________________________

Inputs needed:

_____________________________________________________________________________

Open questions to resolve:

_____________________________________________________________________________

--------------------------------------------------------------------------------
SESSION METRICS (Optional)
--------------------------------------------------------------------------------

Token efficiency:       [ ] Completed planned work within session
                        [ ] Required bootstrap before completion
                        [ ] Session ended early (reason: ___________________)

Alignment verification: [ ] "Any questions?" asked before major work
                        [ ] AI restated understanding before proceeding

Quality tier applied:   [ ] Tier 1 only    [ ] Tier 1+2    [ ] Tier 1+2+3

--------------------------------------------------------------------------------
END OF ENTRY
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
SESSION LOG FILE STRUCTURE
--------------------------------------------------------------------------------

Maintain session logs as a single file per project with entries appended 
chronologically.

FILE NAMING:
[PROJECT-CODE]_Session_Log.txt
Example: GOV-AIPM_Session_Log.txt

ENTRY ORDER:
- Newest entries at the bottom (append-only)
- Entries separated by clear dividers

ALTERNATIVE STRUCTURE:
For long-running projects, consider:
- One session log file per phase
- Example: GOV-AIPM_Session_Log_Phase1.txt

--------------------------------------------------------------------------------
END OF TPL-005
--------------------------------------------------------------------------------


================================================================================
TPL-006: WEEKLY REVIEW AGENDA
Template ID:     GOV-AIPM-TPL-006
Version:         1.0
Purpose:         Structured weekly reflection on AI-assisted work; ensures 
                 cognitive sustainability, process improvement, and strategic 
                 alignment
When to Use:     Weekly, at a consistent time (recommended: end of week)

--------------------------------------------------------------------------------
INSTRUCTIONS
--------------------------------------------------------------------------------

Conduct a weekly review to step back from tactical work and assess patterns, 
progress, and process health. This review can be done solo or with AI 
assistance.

The weekly review serves multiple purposes:
- Prevents cognitive fatigue through enforced reflection
- Identifies process improvements while details are fresh
- Ensures projects stay aligned with objectives
- Catches drift before it becomes problematic
- Maintains sustainable work pace

Tips:
- Schedule this as a recurring calendar event
- Keep it consistent—same day/time each week builds habit
- Be honest in the cognitive state check; fatigue compounds
- Use insights to adjust next week's approach

DIALOG COMPLETION OPTION:
This template can be completed through dialog with the AI. Say:
"Help me complete TPL-006" or "Walk me through Weekly Review"
See Section 3.5 for the Dialog Intake System.

--------------------------------------------------------------------------------
WEEKLY REVIEW AGENDA
--------------------------------------------------------------------------------

WEEK ENDING:        ___________________________________________________________

REVIEW DATE:        ___________________________________________________________

REVIEWED BY:        ___________________________________________________________

--------------------------------------------------------------------------------
1. PROJECTS ACTIVE THIS WEEK
--------------------------------------------------------------------------------

[List all projects with AI-assisted activity this week]

| Project Code | Project Name | Status | Sessions This Week |
|--------------|--------------|--------|--------------------|
|              |              |        |                    |
|              |              |        |                    |
|              |              |        |                    |
|              |              |        |                    |

Status options: Active / On Hold / Completed / Blocked

Total active projects: _____

--------------------------------------------------------------------------------
PROJECT HEALTH CHECK
--------------------------------------------------------------------------------

Any projects at risk?    [ ] Yes    [ ] No

If yes, which and why:

_____________________________________________________________________________

_____________________________________________________________________________

Any projects requiring scope adjustment?    [ ] Yes    [ ] No

If yes, which and why:

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
2. MAJOR DECISIONS MADE
--------------------------------------------------------------------------------

[Review decision logs from this week; note significant decisions]

| Date | Project | Decision Summary | Impact Level |
|------|---------|------------------|--------------|
|      |         |                  |              |
|      |         |                  |              |
|      |         |                  |              |

Impact Level: High / Medium / Low

Any decisions requiring follow-up or reconsideration?

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
3. ISSUES & BLOCKERS
--------------------------------------------------------------------------------

[Review session logs; consolidate issues encountered]

RESOLVED THIS WEEK:
| Issue | Resolution | Lessons Learned |
|-------|------------|-----------------|
|       |            |                 |
|       |            |                 |

STILL OPEN:
| Issue | Owner | Next Action | Target Date |
|-------|-------|-------------|-------------|
|       |       |             |             |
|       |       |             |             |

ESCALATIONS NEEDED:
| Issue | Escalate To | Reason |
|-------|-------------|--------|
|       |             |        |

--------------------------------------------------------------------------------
4. UPCOMING PRIORITIES
--------------------------------------------------------------------------------

[Plan focus for next week]

PRIORITY 1 (Must complete):

_____________________________________________________________________________

_____________________________________________________________________________

PRIORITY 2 (Should complete):

_____________________________________________________________________________

_____________________________________________________________________________

PRIORITY 3 (Nice to complete):

_____________________________________________________________________________

_____________________________________________________________________________

DEPENDENCIES OR INPUTS NEEDED:

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
5. SOP & GOVERNANCE ITEMS
--------------------------------------------------------------------------------

[Note any governance-related activities triggered or needed]

ITEMS TRIGGERED THIS WEEK:
| Item Type | Description | Status |
|-----------|-------------|--------|
|           |             |        |
|           |             |        |

Item types: Resolution needed / SOP update / Policy review / KB entry / Other

ITEMS PENDING:
| Item | Due Date | Notes |
|------|----------|-------|
|      |          |       |
|      |          |       |

Any governance gaps identified?

_____________________________________________________________________________

--------------------------------------------------------------------------------
6. COGNITIVE STATE CHECK
--------------------------------------------------------------------------------

[Honest self-assessment of cognitive sustainability]

FATIGUE INDICATORS:
[ ] Feeling well-rested and focused
[ ] Mild fatigue—manageable
[ ] Moderate fatigue—affecting quality
[ ] Significant fatigue—need recovery time

Signs observed:
_____________________________________________________________________________

OVERLOAD INDICATORS:
[ ] Workload sustainable
[ ] Slightly stretched but manageable
[ ] Overloaded—quality suffering
[ ] Critical—unsustainable pace

Signs observed:
_____________________________________________________________________________

QUALITY CONCERNS:
[ ] Quality maintained throughout week
[ ] Some quality slippage noted
[ ] Significant quality issues
[ ] Rework required on deliverables

Specific concerns:
_____________________________________________________________________________

DECISION QUALITY:
[ ] Decisions feel sound and well-considered
[ ] Some rushed decisions noted
[ ] Decision fatigue affecting choices
[ ] Deferred decisions accumulating

Notes:
_____________________________________________________________________________

--------------------------------------------------------------------------------
COGNITIVE SUSTAINABILITY ACTIONS
--------------------------------------------------------------------------------

Based on the above, what adjustments are needed?

[ ] No adjustments needed—sustainable pace
[ ] Reduce workload next week
[ ] Schedule recovery time
[ ] Delegate or defer items
[ ] Seek support/assistance
[ ] Other: ___________________________________________________________________

Specific actions:

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
7. PROCESS & TOOL OBSERVATIONS
--------------------------------------------------------------------------------

[Note any insights about AI-assisted work process]

WHAT WORKED WELL:

_____________________________________________________________________________

_____________________________________________________________________________

WHAT DIDN'T WORK WELL:

_____________________________________________________________________________

_____________________________________________________________________________

PROCESS IMPROVEMENTS TO CONSIDER:

_____________________________________________________________________________

_____________________________________________________________________________

TOOL OBSERVATIONS:
[Any notes on AI tool behavior, new capabilities, issues]

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
8. METRICS SUMMARY (Optional)
--------------------------------------------------------------------------------

[If tracking metrics, summarize here]

| Metric | This Week | Target | Trend |
|--------|-----------|--------|-------|
| Sessions conducted |  |  |  |
| Decisions logged |  |  |  |
| Bootstrap success rate |  |  |  |
| Files produced |  |  |  |
| Issues resolved |  |  |  |

--------------------------------------------------------------------------------
9. ADJUSTMENTS FOR NEXT WEEK
--------------------------------------------------------------------------------

[Based on this review, what changes for next week?]

PROCESS ADJUSTMENTS:

_____________________________________________________________________________

_____________________________________________________________________________

PRIORITY ADJUSTMENTS:

_____________________________________________________________________________

_____________________________________________________________________________

RESOURCE ADJUSTMENTS:

_____________________________________________________________________________

_____________________________________________________________________________

SCHEDULE ADJUSTMENTS:

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
10. REVIEW COMPLETE
--------------------------------------------------------------------------------

[ ] All sections reviewed
[ ] Action items identified
[ ] Next week priorities set
[ ] Cognitive state addressed

REVIEW DURATION:    ___________________________________________________________

NEXT REVIEW DATE:   ___________________________________________________________

Notes for next review:

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
END OF WEEKLY REVIEW
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
TEMPLATE USAGE NOTES
--------------------------------------------------------------------------------

FILE NAMING:
Weekly_Review_[YYYY-MM-DD].txt
Example: Weekly_Review_2025-12-20.txt

STORAGE:
Store in a dedicated Reviews folder or within project folder if 
project-specific.

FREQUENCY:
- Standard: Weekly
- High-intensity periods: Consider bi-weekly check-ins
- Low-activity periods: Can extend to bi-weekly reviews

INTEGRATION:
- Input: Session logs, decision logs from the week
- Output: Priority adjustments, process improvements, action items

--------------------------------------------------------------------------------
END OF TPL-006
--------------------------------------------------------------------------------


================================================================================
TPL-007: RE-ENTRY CHECKLIST
Template ID:     GOV-AIPM-TPL-007
Version:         1.0
Purpose:         Protocol for returning to AI-assisted work after extended 
                 absence; prevents regression and ensures proper reorientation
When to Use:     After any absence long enough to cause context loss
                 (typically 2+ weeks, but use judgment)

--------------------------------------------------------------------------------
INSTRUCTIONS
--------------------------------------------------------------------------------

Use this checklist when returning to AI-assisted work after an extended 
absence. The goal is to rebuild context systematically rather than jumping 
back in and risking confusion, contradictory work, or wheel reinvention.

When to use this checklist:
- After vacation or extended time away
- After working on other projects exclusively
- When picking up a dormant project
- Whenever you feel "out of the loop" on a project

The principle: If your absence caused context loss, invest in re-entry before 
resuming work. This investment pays off in avoided rework and confusion.

Tips:
- Don't skip steps even if you feel confident—memory is unreliable
- Use NotebookLM or similar tools to query your own documents
- The reorientation chat is not optional; it catches gaps

DIALOG COMPLETION OPTION:
This template can be completed through dialog with the AI. Say:
"Help me complete TPL-007" or "Walk me through Re-Entry Checklist"
See Section 3.5 for the Dialog Intake System.

--------------------------------------------------------------------------------
RE-ENTRY CHECKLIST
--------------------------------------------------------------------------------

PROJECT: ____________________________________________________________________

PROJECT CODE: _______________________________________________________________

LAST ACTIVE DATE: ___________________________________________________________

TODAY'S DATE: _______________________________________________________________

TIME ELAPSED: _______________________________________________________________

RE-ENTRY INITIATED BY: ______________________________________________________

--------------------------------------------------------------------------------
1. ABSENCE ASSESSMENT
--------------------------------------------------------------------------------

Duration of absence:
[ ] Less than 1 week — Re-entry checklist optional
[ ] 1-2 weeks — Abbreviated re-entry recommended
[ ] 2-4 weeks — Full re-entry required
[ ] 1+ month — Extended re-entry required
[ ] 3+ months — Project resumption protocol (see Section 5)

Context retention self-assessment:
[ ] High — Remember most details clearly
[ ] Medium — Remember general direction, fuzzy on details
[ ] Low — Remember project exists, little detail
[ ] Minimal — Need full reorientation

What changed during absence?
[ ] Nothing significant expected
[ ] External factors may have changed (regulations, market, etc.)
[ ] Team/stakeholder changes
[ ] Related projects progressed
[ ] Unknown—need to investigate

--------------------------------------------------------------------------------
2. MATERIALS TO REVIEW
--------------------------------------------------------------------------------

[Check off as you review each item]

ESSENTIAL REVIEW:
- [ ] Project Guide (current version)
      Notes: ________________________________________________________________

- [ ] Recent Session Logs (last 3-5 sessions)
      Notes: ________________________________________________________________

- [ ] Recent Decision Logs (last 2 weeks before absence)
      Notes: ________________________________________________________________

- [ ] Last Bootstrap (if applicable)
      Notes: ________________________________________________________________

- [ ] Last Session Closeout Checklist
      Notes: ________________________________________________________________

EXTENDED REVIEW (for longer absences):
- [ ] Full Decision Log scan
      Notes: ________________________________________________________________

- [ ] Project deliverables produced
      Notes: ________________________________________________________________

- [ ] Related project updates
      Notes: ________________________________________________________________

- [ ] External updates (if applicable)
      Notes: ________________________________________________________________

REVIEW COMPLETE:    [ ] Yes    [ ] Partial (note gaps below)

Gaps or missing materials:

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
3. NOTEBOOKLM / RETRIEVAL QUERIES
--------------------------------------------------------------------------------

[Use NotebookLM or similar tool to query project documents]

QUERIES TO RUN:

- [ ] "What is the current status of [project]?"
      Result summary: ________________________________________________________

- [ ] "What were the most recent decisions made?"
      Result summary: ________________________________________________________

- [ ] "What are the open items or blockers?"
      Result summary: ________________________________________________________

- [ ] "What was the last work completed?"
      Result summary: ________________________________________________________

- [ ] Custom query: __________________________________________________________
      Result summary: ________________________________________________________

- [ ] Custom query: __________________________________________________________
      Result summary: ________________________________________________________

INSIGHTS FROM QUERIES:

_____________________________________________________________________________

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
4. REORIENTATION CHAT
--------------------------------------------------------------------------------

Conduct a dedicated reorientation chat with AI before resuming work.

REORIENTATION OBJECTIVES:
- [ ] Verify understanding of project status
- [ ] Confirm active parameters and constraints
- [ ] Identify any changes needed based on time elapsed
- [ ] Establish focus for resumption

CHAT SETUP:
- Chat name: [###] [DEPT]-[PROJ]-RD (reorientation)
- Provide: Project Guide, recent logs, last bootstrap

PROMPTS TO USE:

"I'm returning to this project after [time period]. Based on the materials 
I've provided, please summarize:
1. Current project status
2. Last completed work
3. Immediate next steps
4. Any decisions that may need revisiting given the time elapsed"

"What assumptions are currently in effect that I should reconfirm?"

"Are there any time-sensitive elements I should be aware of?"

REORIENTATION CHAT COMPLETED:    [ ] Yes    [ ] No

Chat name used: _____________________________________________________________

Key insights from reorientation:

_____________________________________________________________________________

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
5. VERIFICATION QUESTIONS
--------------------------------------------------------------------------------

[Answer these questions to confirm you're ready to resume]

Can you clearly state the project objective?
[ ] Yes    [ ] No — Review Project Guide Section 1

_____________________________________________________________________________

Can you identify the current phase and immediate next actions?
[ ] Yes    [ ] No — Review last bootstrap and session log

_____________________________________________________________________________

Do you know what decisions have been made and their rationale?
[ ] Yes    [ ] No — Review decision log

_____________________________________________________________________________

Are you aware of any open issues or blockers?
[ ] Yes    [ ] No — Review session logs for issues

_____________________________________________________________________________

Do you understand the project constraints and parameters?
[ ] Yes    [ ] No — Review Project Guide constraints section

_____________________________________________________________________________

Has anything changed externally that affects this project?
[ ] Yes — Document below    [ ] No    [ ] Unknown — Investigate

External changes:

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
6. RESUME CRITERIA
--------------------------------------------------------------------------------

[All criteria must be met before resuming substantive work]

- [ ] Materials review complete
- [ ] Retrieval queries run
- [ ] Reorientation chat completed
- [ ] All verification questions answered "Yes"
- [ ] External changes assessed (if applicable)
- [ ] Cognitive state appropriate (not fatigued, focused)

READY TO RESUME:    [ ] Yes    [ ] No — Address gaps first

If No, what's needed before resuming:

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
7. RESUMPTION PLAN
--------------------------------------------------------------------------------

First session focus:

_____________________________________________________________________________

_____________________________________________________________________________

Priority items:

1. ___________________________________________________________________________

2. ___________________________________________________________________________

3. ___________________________________________________________________________

Any adjustments needed based on time elapsed:

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
8. RE-ENTRY COMPLETE
--------------------------------------------------------------------------------

- [ ] All checklist sections complete
- [ ] Ready to resume work
- [ ] First session planned

RE-ENTRY DURATION:      _____________________________________________________

RESUMPTION CHAT:        _____________________________________________________
                        (Name of first working session after re-entry)

Notes for future re-entries:

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
ABBREVIATED RE-ENTRY (1-2 week absence)
--------------------------------------------------------------------------------

For shorter absences, use this abbreviated version:

- [ ] Review last session log
- [ ] Review last bootstrap
- [ ] Scan recent decisions
- [ ] Quick reorientation prompt to AI
- [ ] Confirm immediate next actions
- [ ] Resume work

--------------------------------------------------------------------------------
EXTENDED RE-ENTRY (3+ months / Project Resumption)
--------------------------------------------------------------------------------

For very long absences or dormant project resumption:

1. Treat as near-new project initiation
2. Review ALL project materials, not just recent
3. Conduct comprehensive NotebookLM audit
4. Consider whether project parameters are still valid
5. May require updated Project Initiation Form
6. Reorientation chat should be extensive
7. First session should include explicit "project health check"

--------------------------------------------------------------------------------
END OF TPL-007
--------------------------------------------------------------------------------


================================================================================
TPL-008: CROSS-AI VALIDATION PROTOCOL
Template ID:     GOV-AIPM-TPL-008
Version:         1.0
Purpose:         Structured approach to using multiple AI tools for validation; 
                 implements Tier 2 quality assurance
When to Use:     Before finalizing major deliverables; when outputs require 
                 additional verification; when Tier 2 QA is specified

--------------------------------------------------------------------------------
INSTRUCTIONS
--------------------------------------------------------------------------------

Cross-AI validation uses multiple AI tools to identify gaps, assumptions, and 
potential errors that a single AI might miss. This protocol provides a 
structured approach to conducting validation effectively.

When to use cross-AI validation:
- Major deliverables before finalization
- Complex analyses where errors have significant impact
- Novel situations without established precedent
- When primary AI output "feels off" but you can't pinpoint why
- When Tier 2 QA is specified in project parameters

Principle: Different AI tools have different training, biases, and blind spots. 
Using multiple tools increases the likelihood of catching errors.

Tips:
- Frame validation prompts to encourage critical assessment, not agreement
- Document conflicts rather than immediately resolving them
- Use NotebookLM for document-grounded validation
- Be specific about what aspects to check

DIALOG COMPLETION OPTION:
This template can be completed through dialog with the AI. Say:
"Help me complete TPL-008" or "Walk me through Cross-AI Validation"
See Section 3.5 for the Dialog Intake System.

--------------------------------------------------------------------------------
CROSS-AI VALIDATION PROTOCOL
--------------------------------------------------------------------------------

VALIDATION INITIATED:   ______________________________________________________

DATE:                   ______________________________________________________

PROJECT:                ______________________________________________________

--------------------------------------------------------------------------------
1. SOURCE IDENTIFICATION
--------------------------------------------------------------------------------

OUTPUT BEING VALIDATED
----------------------

Source AI:              ______________________________________________________
                        (e.g., Claude, ChatGPT, Gemini)

Session/Chat:           ______________________________________________________

Output Type:
[ ] Draft document
[ ] Analysis/recommendation
[ ] Code/technical output
[ ] Research summary
[ ] Strategic plan
[ ] Process/procedure
[ ] Other: ___________________________________________________________________

Output Description:

_____________________________________________________________________________

_____________________________________________________________________________

Output Location/File:

_____________________________________________________________________________

--------------------------------------------------------------------------------
WHY VALIDATION IS NEEDED
--------------------------------------------------------------------------------

[ ] Major deliverable requiring quality assurance
[ ] Complex analysis with significant impact
[ ] Novel situation without precedent
[ ] Primary output seems questionable
[ ] Standard Tier 2 QA requirement
[ ] Other: ___________________________________________________________________

Specific concerns (if any):

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
2. VALIDATION AI SELECTION
--------------------------------------------------------------------------------

Select validation AI(s) based on the output type and validation needs:

PRIMARY VALIDATION AI:

Tool:                   ______________________________________________________

Rationale for selection:

_____________________________________________________________________________

SECONDARY VALIDATION AI (if needed):

Tool:                   ______________________________________________________

Rationale for selection:

_____________________________________________________________________________

DOCUMENT-GROUNDED VALIDATION:

[ ] NotebookLM validation required
    Source documents to upload: ______________________________________________

--------------------------------------------------------------------------------
AI SELECTION GUIDANCE
--------------------------------------------------------------------------------

| Output Type | Recommended Validators | Rationale |
|-------------|------------------------|-----------|
| Document/Writing | ChatGPT, Gemini | Different style/clarity perspectives |
| Technical/Code | ChatGPT, specialized tools | Error detection, alternatives |
| Analysis | Gemini, ChatGPT | Different reasoning approaches |
| Research | Perplexity, NotebookLM | Source verification, grounding |
| Governance/Policy | NotebookLM + ChatGPT | Document alignment + critique |

--------------------------------------------------------------------------------
3. VALIDATION PROMPT STRUCTURE
--------------------------------------------------------------------------------

Use structured prompts that encourage critical assessment:

--------------------------------------------------------------------------------
PRIMARY VALIDATION PROMPT
--------------------------------------------------------------------------------

"I'm seeking critical review of the following [output type]. Please:

1. Identify any logical gaps or inconsistencies
2. Note unstated assumptions
3. Flag potential errors or inaccuracies  
4. Suggest alternative approaches or perspectives
5. Rate your confidence in the overall quality (High/Medium/Low)

Be critical rather than agreeable. I need honest assessment, not validation.

[Paste or attach output]

Context: [Brief context about the project and purpose]"

--------------------------------------------------------------------------------
NOTEBOOKLM VALIDATION PROMPT
--------------------------------------------------------------------------------

"Based on the source documents provided, please assess:

1. Does this [output type] align with the established [policies/procedures/
   guidelines] in the source documents?
2. Are there any contradictions between this output and the source material?
3. What source document sections are most relevant to validating this output?
4. Are there gaps where the output goes beyond what sources support?

[Paste or attach output]"

--------------------------------------------------------------------------------
SPECIFIC ASPECT PROMPTS
--------------------------------------------------------------------------------

For logical consistency:
"Analyze the logical structure of this argument. Identify any:
- Non-sequiturs
- Unsupported conclusions
- Circular reasoning
- Missing steps in the logic chain"

For factual accuracy:
"Review the factual claims in this output. For each significant claim:
- Assess likelihood of accuracy
- Note where verification is recommended
- Flag any claims that seem questionable"

For completeness:
"Assess the completeness of this [output type]. Identify:
- Topics that should be covered but aren't
- Sections that seem underdeveloped
- Important considerations that are missing"

--------------------------------------------------------------------------------
4. AREAS TO CHECK
--------------------------------------------------------------------------------

[Select areas relevant to this validation]

LOGICAL CONSISTENCY:
- [ ] Arguments follow logically
- [ ] Conclusions supported by premises
- [ ] No internal contradictions
- [ ] Reasoning chain complete

FACTUAL ACCURACY:
- [ ] Claims appear accurate
- [ ] Sources cited appropriately
- [ ] No obvious errors
- [ ] Verification recommended where needed

COMPLETENESS:
- [ ] All required elements present
- [ ] No significant gaps
- [ ] Appropriate depth of coverage
- [ ] Nothing important omitted

ASSUMPTIONS:
- [ ] Assumptions explicitly stated
- [ ] Assumptions reasonable
- [ ] Hidden assumptions identified
- [ ] Alternative assumptions considered

ALIGNMENT:
- [ ] Aligns with project objectives
- [ ] Consistent with established parameters
- [ ] Matches governing documents (if applicable)
- [ ] Follows required format/structure

CLARITY:
- [ ] Clear and understandable
- [ ] Appropriate for audience
- [ ] Well-organized
- [ ] Actionable (if applicable)

ALTERNATIVE PERSPECTIVES:
- [ ] Other approaches considered
- [ ] Counterarguments addressed
- [ ] Limitations acknowledged
- [ ] Bias checked

--------------------------------------------------------------------------------
5. VALIDATION FINDINGS
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
PRIMARY VALIDATOR FINDINGS
--------------------------------------------------------------------------------

Validator:              ______________________________________________________

Confidence Rating:      [ ] High    [ ] Medium    [ ] Low

ISSUES IDENTIFIED:

| # | Issue | Severity | Category |
|---|-------|----------|----------|
| 1 |       |          |          |
| 2 |       |          |          |
| 3 |       |          |          |
| 4 |       |          |          |
| 5 |       |          |          |

Severity: Critical / Major / Minor
Category: Logic / Accuracy / Completeness / Assumption / Alignment / Clarity

STRENGTHS NOTED:

_____________________________________________________________________________

_____________________________________________________________________________

SUGGESTIONS PROVIDED:

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
SECONDARY VALIDATOR FINDINGS (if applicable)
--------------------------------------------------------------------------------

Validator:              ______________________________________________________

Confidence Rating:      [ ] High    [ ] Medium    [ ] Low

ISSUES IDENTIFIED:

| # | Issue | Severity | Category |
|---|-------|----------|----------|
| 1 |       |          |          |
| 2 |       |          |          |
| 3 |       |          |          |

ADDITIONAL PERSPECTIVES:

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
NOTEBOOKLM FINDINGS (if applicable)
--------------------------------------------------------------------------------

ALIGNMENT ASSESSMENT:

[ ] Fully aligned with source documents
[ ] Mostly aligned with minor gaps
[ ] Partially aligned with significant gaps
[ ] Misaligned with source documents

Specific alignment issues:

_____________________________________________________________________________

_____________________________________________________________________________

Source references cited:

_____________________________________________________________________________

--------------------------------------------------------------------------------
6. CONFLICT RESOLUTION
--------------------------------------------------------------------------------

[If validators disagree, document and resolve]

CONFLICTS IDENTIFIED:

| Conflict | Validator 1 Says | Validator 2 Says |
|----------|------------------|------------------|
|          |                  |                  |
|          |                  |                  |
|          |                  |                  |

--------------------------------------------------------------------------------
RESOLUTION APPROACH
--------------------------------------------------------------------------------

For each conflict:

[ ] Accept Validator 1 position
    Rationale: ______________________________________________________________

[ ] Accept Validator 2 position
    Rationale: ______________________________________________________________

[ ] Synthesize both positions
    Resolution: _____________________________________________________________

[ ] Seek additional validation (Tier 3 / human review)
    Escalation reason: ______________________________________________________

[ ] Accept uncertainty and document
    Documentation: __________________________________________________________

--------------------------------------------------------------------------------
TIE-BREAKER GUIDANCE
--------------------------------------------------------------------------------

When validators conflict and no clear resolution emerges:

1. Privilege source-grounded validation (NotebookLM) over general AI opinion
2. Privilege factual claims that can be verified over interpretive claims
3. When both are interpretive, note the disagreement and escalate to Tier 3
4. Document the conflict for future reference regardless of resolution
5. If time-sensitive, make a provisional decision and flag for later review

--------------------------------------------------------------------------------
7. VALIDATION OUTCOME
--------------------------------------------------------------------------------

OVERALL ASSESSMENT:

[ ] VALIDATED — Output acceptable with no changes
[ ] VALIDATED WITH REVISIONS — Output acceptable after addressing issues
[ ] REQUIRES SIGNIFICANT REVISION — Major issues identified
[ ] FAILED VALIDATION — Output not acceptable; restart recommended
[ ] ESCALATE TO TIER 3 — Human review required

--------------------------------------------------------------------------------
ISSUES REQUIRING ACTION
--------------------------------------------------------------------------------

| Issue | Action Required | Priority | Owner |
|-------|-----------------|----------|-------|
|       |                 |          |       |
|       |                 |          |       |
|       |                 |          |       |

Priority: Immediate / Before finalization / Future consideration

--------------------------------------------------------------------------------
REVISIONS TO MAKE
--------------------------------------------------------------------------------

1. ___________________________________________________________________________

2. ___________________________________________________________________________

3. ___________________________________________________________________________

4. ___________________________________________________________________________

--------------------------------------------------------------------------------
ESCALATION (if applicable)
--------------------------------------------------------------------------------

Escalate to:            ______________________________________________________

Reason:                 ______________________________________________________

Information to provide:

_____________________________________________________________________________

--------------------------------------------------------------------------------
8. DOCUMENTATION
--------------------------------------------------------------------------------

VALIDATION RECORD:

- [ ] Findings documented above
- [ ] Conflicts and resolutions recorded
- [ ] Actions identified
- [ ] Outcome determined

FILE THIS VALIDATION:

Location:               ______________________________________________________

File name:              ______________________________________________________

LINK TO DECISION LOG (if decisions made):

Decision #:             ______________________________________________________

--------------------------------------------------------------------------------
9. POST-VALIDATION
--------------------------------------------------------------------------------

After addressing validation findings:

- [ ] Revisions completed
- [ ] Re-validation needed?    [ ] Yes — repeat protocol    [ ] No
- [ ] Ready for finalization
- [ ] Output updated and saved

FINAL DISPOSITION:

_____________________________________________________________________________

VALIDATION COMPLETED:   ______________________________________________________
                        (Date/Time)

--------------------------------------------------------------------------------
QUICK REFERENCE: VALIDATION PROMPTS
--------------------------------------------------------------------------------

STANDARD CRITICAL REVIEW:
"Please critically review this [output]. Identify gaps, assumptions, errors, 
and alternatives. Be critical, not agreeable."

LOGIC CHECK:
"Analyze the logical structure. Find non-sequiturs, unsupported conclusions, 
and missing reasoning steps."

FACT CHECK:
"Review factual claims. Assess accuracy, flag questionable claims, note where 
verification is needed."

COMPLETENESS CHECK:
"Assess completeness. What's missing, underdeveloped, or needs more coverage?"

ALIGNMENT CHECK (NotebookLM):
"Does this align with the source documents? Find contradictions and gaps."

ASSUMPTION SURFACE:
"What assumptions does this output make? Which are stated, which are hidden, 
and are they reasonable?"

--------------------------------------------------------------------------------
END OF TPL-008
--------------------------------------------------------------------------------


================================================================================
TPL-009: FAILURE RECOVERY WORKSHEET
Template ID:     GOV-AIPM-TPL-009
Version:         1.0
Purpose:         Structured response when a failure case is identified; guides 
                 recovery and captures lessons learned
When to Use:     When recognizing signs of a failure case from the AI Operations 
                 Manual Appendix A (or similar operational failure)

--------------------------------------------------------------------------------
INSTRUCTIONS
--------------------------------------------------------------------------------

Use this worksheet when you recognize that you've entered a failure case. The 
worksheet guides you through stopping, assessing, recovering, and learning 
from the failure.

Failure cases covered:
1. No Logs → Decision Churn
2. SOP Drift
3. Unchecked AI Assumptions
4. No Branching → Token Collapse
5. Cognitive Fatigue
6. No Re-Entry Protocol → Regression
7. Cognitive Divergence (AI not tracking user intent)

General Recovery Principle:
STOP → DOCUMENT current state → STEP BACK to last known-good position → 
PROCEED with explicit acknowledgment of what was lost or uncertain

Tips:
- Don't continue working while completing this worksheet
- Be honest about severity—underestimating delays recovery
- The lessons learned section is not optional; failures should improve process
- Some context loss may be unrecoverable; acknowledge and move forward

DIALOG COMPLETION OPTION:
This template can be completed through dialog with the AI. Say:
"Help me complete TPL-009" or "Walk me through Failure Recovery"
See Section 3.5 for the Dialog Intake System.

--------------------------------------------------------------------------------
FAILURE RECOVERY WORKSHEET
--------------------------------------------------------------------------------

DATE/TIME IDENTIFIED:   ______________________________________________________

PROJECT:                ______________________________________________________

SESSION:                ______________________________________________________

IDENTIFIED BY:          ______________________________________________________

--------------------------------------------------------------------------------
1. FAILURE CASE IDENTIFICATION
--------------------------------------------------------------------------------

FAILURE TYPE:

[ ] NO LOGS → DECISION CHURN
    Re-debating settled issues; contradictory directions; no record of why 
    decisions were made

[ ] SOP DRIFT
    Actual practice has diverged from documented procedures; doing things 
    differently than specified

[ ] UNCHECKED AI ASSUMPTIONS
    AI output seems confident but feels "off"; contradicts known facts; 
    accepted without verification

[ ] NO BRANCHING → TOKEN COLLAPSE
    Chat sluggish; AI losing early context; output quality degrading; 
    approaching or exceeding token limits

[ ] COGNITIVE FATIGUE
    Declining decision quality; irritability; taking shortcuts; reduced 
    attention to detail

[ ] NO RE-ENTRY PROTOCOL → REGRESSION
    Confusion after absence; producing contradictory work; reinventing 
    solutions that already exist

[ ] COGNITIVE DIVERGENCE
    AI consistently misinterpreting intent; outputs not matching expectations;
    repeated corrections not sticking; Three-Strike threshold reached

[ ] OTHER: __________________________________________________________________

--------------------------------------------------------------------------------
RECOGNITION SIGNS OBSERVED
--------------------------------------------------------------------------------

What signs indicated this failure case?

_____________________________________________________________________________

_____________________________________________________________________________

_____________________________________________________________________________

When did signs first appear (in retrospect)?

_____________________________________________________________________________

What triggered recognition of the failure?

_____________________________________________________________________________

--------------------------------------------------------------------------------
2. CURRENT STATE ASSESSMENT
--------------------------------------------------------------------------------

WHAT'S AFFECTED:

[ ] Current session only
[ ] Current project
[ ] Multiple projects
[ ] Established processes/procedures
[ ] Deliverables already produced
[ ] Decisions already made

Specific impacts:

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
SEVERITY ASSESSMENT
--------------------------------------------------------------------------------

[ ] MINOR
    - Limited scope
    - Easy to correct
    - No downstream effects
    - Recovery time: < 1 hour

[ ] MODERATE
    - Broader scope
    - Requires deliberate correction
    - Some downstream effects
    - Recovery time: 1-4 hours

[ ] SEVERE
    - Extensive scope
    - Significant correction needed
    - Major downstream effects
    - Recovery time: 4+ hours or multiple sessions

--------------------------------------------------------------------------------
URGENCY ASSESSMENT
--------------------------------------------------------------------------------

[ ] IMMEDIATE
    - Stop all work now
    - Cannot proceed without recovery
    - Active harm if continued

[ ] SOON
    - Complete current atomic task
    - Address before next major work
    - Risk increases with delay

[ ] SCHEDULED
    - Can be addressed in planned time
    - Document and schedule recovery
    - Monitor for escalation

--------------------------------------------------------------------------------
3. LAST KNOWN-GOOD POSITION
--------------------------------------------------------------------------------

What was the last point before this failure occurred?

_____________________________________________________________________________

_____________________________________________________________________________

What artifacts exist from that point?

| Artifact | Location | Status |
|----------|----------|--------|
|          |          |        |
|          |          |        |
|          |          |        |

Status: Available / Partial / Missing

Can we roll back to the last known-good position?

[ ] Yes — fully recoverable
[ ] Partially — some work will be lost
[ ] No — must reconstruct from available information

What will be lost if we roll back?

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
4. FAILURE-SPECIFIC RECOVERY PROTOCOL
--------------------------------------------------------------------------------

Select the protocol matching your failure type:

--------------------------------------------------------------------------------
PROTOCOL A: NO LOGS → DECISION CHURN
--------------------------------------------------------------------------------

[ ] STOP making new decisions

[ ] Reconstruct decision history:
    - Review chat transcripts
    - Check any partial logs
    - Interview memory (what do you remember deciding?)
    - Document reconstructed decisions with [RECONSTRUCTED] tag

[ ] Create decision log going forward:
    - Use Template 4 (Decision Log Entry)
    - Log all new decisions immediately

[ ] Accept some history may be unrecoverable:
    - Document what's unknown
    - Make fresh decisions where needed
    - Note: "Decision made due to unrecoverable history"

Notes:

_____________________________________________________________________________

--------------------------------------------------------------------------------
PROTOCOL B: SOP DRIFT
--------------------------------------------------------------------------------

[ ] STOP current work

[ ] Conduct SOP audit:
    - Compare documented procedure to actual practice
    - Identify specific divergences
    - Assess which approach is actually correct

[ ] Decide resolution path:
    [ ] Update SOP to match reality (practice is better)
    [ ] Retrain to match SOP (procedure is better)
    [ ] Hybrid (some of each)

[ ] Document the decision:
    - Log rationale for resolution
    - Update relevant documents
    - Communicate changes if others affected

Notes:

_____________________________________________________________________________

--------------------------------------------------------------------------------
PROTOCOL C: UNCHECKED AI ASSUMPTIONS
--------------------------------------------------------------------------------

[ ] PAUSE accepting AI outputs

[ ] Explicitly surface assumptions:
    - Ask AI: "What assumptions are you making?"
    - List assumptions you've been making
    - Identify which are verified vs. assumed

[ ] Cross-validate:
    - Use second AI source (Tier 2 validation)
    - Check against primary sources
    - Query NotebookLM against governing documents

[ ] Re-evaluate affected outputs:
    - Which outputs were based on unchecked assumptions?
    - Do they need revision?
    - What verification is needed going forward?

Notes:

_____________________________________________________________________________

--------------------------------------------------------------------------------
PROTOCOL D: NO BRANCHING → TOKEN COLLAPSE
--------------------------------------------------------------------------------

[ ] STOP new drafting immediately

[ ] Generate emergency bootstrap:
    - Capture current state NOW before more context is lost
    - Use Template 2 (Bootstrap Template)
    - Be comprehensive—you may be losing context as you write

[ ] Save all files immediately:
    - Download/export everything produced
    - Don't rely on chat history

[ ] Accept some context loss:
    - Document what you remember
    - Note what seems uncertain
    - Previous chat may be unreliable for details

[ ] Resume in fresh chat:
    - Create new chat with tighter scope
    - Provide bootstrap and essential files
    - Verify AI understanding before proceeding

Notes:

_____________________________________________________________________________

--------------------------------------------------------------------------------
PROTOCOL E: COGNITIVE FATIGUE
--------------------------------------------------------------------------------

[ ] STOP decision-making immediately

[ ] Take mandatory break:
    - Minimum 30 minutes
    - Step away from screen
    - Do not make significant decisions

[ ] Assess state after break:
    - Am I actually recovered?
    - What's my capacity for the rest of day?
    - Should I stop for the day?

[ ] Review recent decisions:
    - Look at decisions made while fatigued
    - Flag any that seem questionable
    - Consider re-evaluation when fresh

[ ] Resume with fresh review:
    - Start next session with current state review
    - Explicitly verify recent decisions
    - Reduce workload if needed

Notes:

_____________________________________________________________________________

--------------------------------------------------------------------------------
PROTOCOL F: NO RE-ENTRY PROTOCOL → REGRESSION
--------------------------------------------------------------------------------

[ ] STOP new work immediately

[ ] Execute full re-entry protocol:
    - Use Template 7 (Re-Entry Checklist)
    - Do not skip steps

[ ] Audit work done since return:
    - What have I produced since returning?
    - Does it contradict earlier work?
    - Is it reinventing existing solutions?

[ ] Reconcile contradictions:
    - Identify conflicts between old and new work
    - Determine correct resolution
    - Document the reconciliation

[ ] Do not proceed until oriented:
    - Complete re-entry checklist fully
    - Verify with reorientation chat
    - Confirm understanding before new work

Notes:

_____________________________________________________________________________

--------------------------------------------------------------------------------
PROTOCOL G: COGNITIVE DIVERGENCE
--------------------------------------------------------------------------------

[ ] STOP current task immediately

[ ] Document the divergence:
    - State your intended logic path clearly
    - Note AI's actual execution path
    - Identify the specific point where paths diverged

[ ] Use QA-001 prompt (Cognitive Divergence Audit):
    - Provide AI with intended logic and misaligned outputs
    - Request analysis of where logic branched
    - Get explicit correction path

[ ] If QA-001 fails to correct:
    - Consider fresh chat with clean context
    - Use different AI tool for the task
    - Escalate to Tier 3 (human review) if critical

[ ] Prevent recurrence:
    - Add explicit checkpoints for this task type
    - Consider whether bootstrap or instructions need clarification
    - Log pattern for future reference

Notes:

_____________________________________________________________________________

--------------------------------------------------------------------------------
5. RECOVERY STEPS TAKEN
--------------------------------------------------------------------------------

Document specific actions taken to recover:

| Step | Action Taken | Outcome |
|------|--------------|---------|
| 1    |              |         |
| 2    |              |         |
| 3    |              |         |
| 4    |              |         |
| 5    |              |         |

Overall recovery outcome:

[ ] Fully recovered — back to known-good state
[ ] Partially recovered — some losses accepted
[ ] Stabilized — bleeding stopped, full recovery ongoing
[ ] Ongoing — recovery in progress

What was unrecoverable?

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
6. VERIFICATION
--------------------------------------------------------------------------------

How do you know recovery succeeded?

_____________________________________________________________________________

_____________________________________________________________________________

Verification checks performed:

- [ ] Current state matches last known-good (where applicable)
- [ ] No ongoing symptoms of failure case
- [ ] Affected work has been addressed
- [ ] Documentation is current
- [ ] Process gaps have been closed

Remaining concerns:

_____________________________________________________________________________

--------------------------------------------------------------------------------
7. LESSONS LEARNED
--------------------------------------------------------------------------------

ROOT CAUSE:

What caused this failure case to occur?

_____________________________________________________________________________

_____________________________________________________________________________

Could it have been prevented? How?

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
PREVENTION MEASURES
--------------------------------------------------------------------------------

What will prevent this failure case in the future?

| Measure | Implementation | Owner | Timeline |
|---------|----------------|-------|----------|
|         |                |       |          |
|         |                |       |          |
|         |                |       |          |

Process updates needed:

_____________________________________________________________________________

_____________________________________________________________________________

Template or tool updates needed:

_____________________________________________________________________________

_____________________________________________________________________________

--------------------------------------------------------------------------------
EARLY WARNING SIGNS
--------------------------------------------------------------------------------

What signs should trigger earlier intervention next time?

1. ___________________________________________________________________________

2. ___________________________________________________________________________

3. ___________________________________________________________________________

--------------------------------------------------------------------------------
8. UPDATES NEEDED
--------------------------------------------------------------------------------

Based on this failure and recovery:

PROCESS UPDATES:
- [ ] Update to AI Operations Manual needed
      Section: _______________________________________________________________
      Change: ________________________________________________________________

- [ ] Update to SOP needed
      Section: _______________________________________________________________
      Change: ________________________________________________________________

- [ ] Update to Project Guide needed
      Change: ________________________________________________________________

TEMPLATE UPDATES:
- [ ] Template update needed
      Template: ______________________________________________________________
      Change: ________________________________________________________________

TRAINING/AWARENESS:
- [ ] Training update needed
      Topic: _________________________________________________________________

- [ ] Awareness communication needed
      Audience: ______________________________________________________________

--------------------------------------------------------------------------------
9. RECOVERY COMPLETE
--------------------------------------------------------------------------------

- [ ] Recovery protocol executed
- [ ] Verification performed
- [ ] Lessons documented
- [ ] Prevention measures identified
- [ ] Updates scheduled

RECOVERY DURATION:      ______________________________________________________

READY TO RESUME:        [ ] Yes    [ ] No — additional work needed

NEXT ACTIONS:

1. ___________________________________________________________________________

2. ___________________________________________________________________________

3. ___________________________________________________________________________

WORKSHEET COMPLETED:    ______________________________________________________
                        (Date/Time)

--------------------------------------------------------------------------------
QUICK REFERENCE: FAILURE CASE RECOVERY
--------------------------------------------------------------------------------

| Failure Case | First Action | Key Recovery Step |
|--------------|--------------|-------------------|
| No Logs → Decision Churn | STOP deciding | Reconstruct history |
| SOP Drift | STOP work | Audit SOP vs. practice |
| Unchecked AI Assumptions | PAUSE accepting | Surface and validate |
| Token Collapse | STOP drafting | Emergency bootstrap |
| Cognitive Fatigue | STOP decisions | Mandatory 30min break |
| No Re-Entry → Regression | STOP new work | Full re-entry protocol |
| Cognitive Divergence | STOP task | QA-001 audit prompt |

UNIVERSAL PRINCIPLE:
STOP → DOCUMENT → STEP BACK → PROCEED

--------------------------------------------------------------------------------
END OF TPL-009
--------------------------------------------------------------------------------


================================================================================
TPL-010: PROJECT ROADMAP
Template ID:     GOV-AIPM-TPL-010
Version:         1.0
Purpose:         Provides macro-level project visibility and phase-specific 
                 execution planning; serves as the "horizon view" document 
                 for maintaining orientation across multi-phase projects
When to Use:     - At project initiation (create initial roadmap)
                 - At phase transitions (update status, generate new phase detail)
                 - After extended absence (reference for re-orientation)
                 - When strategic visibility is needed mid-phase

--------------------------------------------------------------------------------
INSTRUCTIONS
--------------------------------------------------------------------------------

The Project Roadmap serves two functions:

1. MACRO VIEW (Horizon): Shows the full project arc—where you've been, 
   where you are, where you're going. Updated at each phase transition.

2. PHASE VIEW (Execution Brief): Details the current phase—objectives, 
   expected work, success criteria. Regenerated at each phase boundary.

Creating the Roadmap:
- Generate initial roadmap after completing Project Initiation (TPL-001)
- The AI should draft the roadmap based on the Project Guide's phase structure
- Review and confirm the roadmap before beginning Phase 1 work

Maintaining the Roadmap:
- The AI maintains and updates this document as part of its coordinator role
- At phase transitions, the AI marks prior phase complete and generates 
  new Current Phase Detail
- Users should save updated roadmaps to project files after each transition

Tips:
- The roadmap is not a rigid plan—phases may shift as projects evolve
- When phase structure changes significantly, update the Project Guide first, 
  then regenerate the roadmap
- Keep the Current Phase Detail focused; detailed task tracking belongs in 
  session bootstraps

Cross-reference: See Section 4.3.5 for Phase Transition Protocol. See 
Section 2.7 for AI coordinator duties including roadmap maintenance.

DIALOG COMPLETION OPTION:
This template can be completed through dialog with the AI. Say:
"Help me complete TPL-010" or "Walk me through Project Roadmap"
See Section 3.5 for the Dialog Intake System.

--------------------------------------------------------------------------------
PROJECT ROADMAP TEMPLATE
--------------------------------------------------------------------------------

================================================================================
PROJECT ROADMAP: [Project Name]
PROJECT IDENTIFICATION
----------------------
Project Name:       [Full project name]
Project Code:       [Project code, e.g., GOV-AIPM]
Project Guide:      [Project Guide filename and version]
Roadmap Version:    [Version number, e.g., 1.0]
Last Updated:       [Date of last update]
Updated By:         [Session that produced this update]

================================================================================
PHASE OVERVIEW
[List all project phases with status. Update this section at each phase 
transition. Use status indicators: NOT STARTED | IN PROGRESS | COMPLETE]

| Phase | Name                    | Status       | Sessions | Notes           |
|-------|-------------------------|--------------|----------|-----------------|
| 1     | [Phase 1 name]          | [Status]     | [#/#]    | [Brief note]    |
| 2     | [Phase 2 name]          | [Status]     | [#/#]    | [Brief note]    |
| 3     | [Phase 3 name]          | [Status]     | [#/#]    | [Brief note]    |
| 4     | [Phase 4 name]          | [Status]     | [#/#]    | [Brief note]    |
| ...   | ...                     | ...          | ...      | ...             |

Sessions: [Actual]/[Allocated] — e.g., "3/5" means 3 sessions used of 5 allocated

================================================================================
CURRENT POSITION
┌─────────────────────────────────────────────────────────────────────────────┐
│  YOU ARE HERE: Phase [#], Session [###]                                     │
│                                                                             │
│  STATUS:       [One-line summary of current state]                          │
│                                                                             │
│  JUST COMPLETED: [What was finished most recently]                          │
│                                                                             │
│  NEXT UP:      [Immediate next work]                                        │
└─────────────────────────────────────────────────────────────────────────────┘

PROGRESS SUMMARY:
- Phases Complete: [#] of [Total]
- Current Phase Progress: [Description, e.g., "4 of 6 deliverables complete"]
- Overall Project: [Percentage or fraction complete]

================================================================================
MILESTONE TRACKING
[Track significant project milestones. Update as milestones are reached.]

| Milestone                        | Target      | Actual      | Status    |
|----------------------------------|-------------|-------------|-----------|
| [Milestone 1]                    | [Date/Sess] | [Date/Sess] | [Status]  |
| [Milestone 2]                    | [Date/Sess] | [Date/Sess] | [Status]  |
| [Milestone 3]                    | [Date/Sess] | [Date/Sess] | [Status]  |
| [Milestone 4]                    | [Date/Sess] | [Date/Sess] | [Status]  |

Status: PENDING | ON TRACK | AT RISK | COMPLETE | DEFERRED

================================================================================
CURRENT PHASE DETAIL (Phase Execution Brief)
[This section is regenerated at each phase transition. It provides the 
working plan for the current phase.]

PHASE [#]: [Phase Name]
------------------------

Phase Objective:
[2-3 sentences describing what this phase should accomplish and why it 
matters to the overall project.]

Session Allocation:
- Estimated Sessions: [#]
- Sessions Used: [#]
- Sessions Remaining: [#]

Expected Deliverables:
[ ] [Deliverable 1]
[ ] [Deliverable 2]
[ ] [Deliverable 3]
[ ] [Deliverable 4]

Success Criteria:
The phase is complete when:
- [Criterion 1]
- [Criterion 2]
- [Criterion 3]

Key Considerations:
- [Consideration 1 — dependencies, risks, or notes for this phase]
- [Consideration 2]
- [Consideration 3]

Phase Transition Trigger:
[Describe what signals that this phase is complete and the next should begin]

================================================================================
HORIZON VIEW
[Updated at phase transitions. Provides strategic orientation.]

WHERE WE'VE BEEN:
[Summary of completed phases, key decisions made, and how the project has 
evolved from its original conception.]

WHERE WE ARE:
[Current phase focus, what's being accomplished, key challenges or 
opportunities in the current work.]

WHERE WE'RE GOING:
[Upcoming phases, what remains to complete the project, anticipated 
challenges or decision points ahead.]

================================================================================
ROADMAP HISTORY
[Track significant roadmap updates for audit trail.]

| Version | Date       | Session | Change Summary                            |
|---------|------------|---------|-------------------------------------------|
| 1.0     | [Date]     | [###]   | Initial roadmap created                   |
| 1.1     | [Date]     | [###]   | [Change description]                      |
| 1.2     | [Date]     | [###]   | [Change description]                      |

================================================================================
END OF PROJECT ROADMAP
--------------------------------------------------------------------------------
END OF TPL-010
--------------------------------------------------------------------------------


================================================================================
TPL-011: DIALOG INTAKE QUICK REFERENCE
Template:        Dialog Intake Quick Reference
Version:         1.0
Date:            December 26, 2025
Purpose:         Condensed reference for rapid AI lookup during dialog intake
Reference:       Section 3.5 (Full Dialog Intake System)
                 Appendix F (Complete Failure Mode Catalog)

================================================================================
ENTRY POINT TRIAGE TREE
User's first message → Parse for intent signals:

BOOTSTRAP PROVIDED?
├── Yes + continuation language → Touchpoint 1.2 (Session Continuation)
│   └── Verify bootstrap, confirm context, proceed
└── No bootstrap
    └── Parse message content ↓

FRAMEWORK REFERENCE? ("AI Operations Manual", "GOV-AIPM", "policy manual")
└── Yes → Framework Recognition Mode
    └── Confirm adoption, assess need, route accordingly

NEW PROJECT SIGNALS? ("new project", "start", "begin", "initiate")
├── Clear objective → Touchpoint 1.1 (Project Initiation) → TPL-001
└── Unclear objective → Exploration Session (Section 1.4.5)

CONTINUATION WITHOUT BOOTSTRAP? ("continue", "where we left off")
├── Can reconstruct context → Abbreviated Re-Entry
└── Cannot reconstruct → Full Re-Entry (Touchpoint 1.3) → TPL-007

CONFUSION SIGNALS? ("lost", "confused", "where are we")
└── Touchpoint 1.3 (Re-Entry) → Depth based on severity

PROBLEM SIGNALS? ("not working", "wrong", "stuck", "help")
└── Touchpoint 2.6 (Failure Recovery) → TPL-009

TRANSITION SIGNALS? ("done with", "next phase", "finished")
└── Touchpoint 1.4 (Phase Transition) → Verify completion first

AMBIGUOUS OR UNCLASSIFIABLE?
└── Touchpoint 4.1 (Ambiguous Intent) → Ask: "What are you trying to accomplish today?"

================================================================================
CONFIRMATION CALIBRATION QUICK TABLE
| TIER | NAME | BEHAVIOR | EXAMPLE LANGUAGE |
|------|------|----------|------------------|
| 0 | Silent | Proceed, log, user corrects if needed | "I've logged this session." |
| 1 | Lightweight | Brief request, single ack OK | "Decision captured. Correct?" |
| 2 | Explicit | Summary + substantive response | "Please confirm this scope change..." |
| 3 | Stop-and-Verify | Full stop, explicit "confirmed" | "CONFIRMATION REQUIRED..." |

TIER ASSIGNMENTS:
- Session log entry → Tier 0
- Routine decision capture → Tier 1 (Tier 2 if scope-affecting)
- Bootstrap generation → Tier 1 (Tier 2 if first bootstrap)
- Project Guide minor update → Tier 1 (Tier 3 if scope-affecting)
- Project Guide creation/major update → Tier 3 (Always)
- Phase transition → Tier 3 (Always)
- Scope change → Tier 3 (Always)
- Failure recovery path → Tier 3 (Always)

================================================================================
TOP 10 FAILURE MODES — QUICK RECOVERY
| # | FAILURE | SEVERITY | ONE-LINE RECOVERY |
|---|---------|----------|-------------------|
| 1 | Vague Project Objective | HIGH | Route to Exploration Session; use readiness questions |
| 2 | No Bootstrap at Session End | CRITICAL | Never skip; emergency bootstrap if rushed |
| 3 | Scope Creep Undetected | CRITICAL | Flag ALL out-of-scope requests explicitly |
| 4 | Decision Not Captured | HIGH | Proactively flag apparent decisions |
| 5 | Premature Phase Transition | HIGH | Verify completion criteria before transition |
| 6 | User Resistance to Process | MEDIUM | Offer lightweight alternatives; brief value explanation |
| 7 | Contradictory Information | HIGH | Stop progress until resolved |
| 8 | Context Degradation | HIGH | Regular freshness checks; watch warning signs |
| 9 | Passive Acquiescence (Inertia) | MEDIUM | Pause after 3+ minimal responses; verify engagement |
| 10 | Multi-Project Confusion | MEDIUM | Explicit project naming; separate artifacts |

================================================================================
TOUCHPOINT QUICK INDEX
ENTRY POINTS:
- 1.1 First Contact → TPL-001, Project Guide | Watch for: vague objectives
- 1.2 Session Continuation → Bootstrap validation | Watch for: missing bootstrap
- 1.3 Re-Entry (Extended Absence) → TPL-007 | Watch for: underestimated context loss
- 1.4 Phase Transition → Updated Project Guide | Watch for: premature transition

ARTIFACT CREATION:
- 2.1 Project Guide Creation → Project Guide | Watch for: incomplete constraints
- 2.2 Decision Capture → TPL-004 | Watch for: implicit decisions missed
- 2.3 Bootstrap Generation → TPL-002 | Watch for: rushed/skipped closeout
- 2.4 Session Log Entry → TPL-005 | Watch for: skipped logging
- 2.5 Re-Entry Checklist → TPL-007 | Watch for: abbreviated when full needed
- 2.6 Failure Recovery → TPL-009 | Watch for: wrong failure type

PROCESS EXECUTION:
- 3.1 Scope Change Detection → Project Guide update | Watch for: silent creep
- 3.2 Verification/QA → Quality check | Watch for: verification fatigue
- 3.3 Weekly Review → TPL-006 | Watch for: rushed review

EDGE CASES:
- 4.1 Ambiguous Intent → Triage questions | Watch for: false classification
- 4.2 Contradictory Information → Resolution | Watch for: propagated contradiction
- 4.3 User Resistance → Calibrated response | Watch for: lost benefits
- 4.4 Multi-Project Confusion → Separation | Watch for: cross-contamination
- 4.5 External Stakeholder Info → Verification | Watch for: unverified data
- 4.6 User Inertia → Engagement check | Watch for: rubber-stamping

================================================================================
TRIGGER PHRASES BY CATEGORY
NEW PROJECT:
"I want to start..." | "new project" | "new initiative" | "begin" | "initiate"
"help me get organized on something new" | "I have an idea for..."

CONTINUATION:
"continue" | "pick up where we left off" | "back to the project"
"here's my bootstrap" | "resuming" | "let's keep working on..."

RE-ENTRY:
"I was working on..." | "where were we?" | "it's been a while"
"I think we were doing..." | "what did we decide about..."

PROBLEM/RECOVERY:
"something's wrong" | "not working" | "stuck" | "help"
"we're off track" | "this isn't what I wanted" | "can we fix..."

PHASE TRANSITION:
"done with this phase" | "ready for next phase" | "finished with..."
"what's next?" | "phase complete"

SCOPE CHANGE (flags to watch):
"while we're at it..." | "can we also..." | "I just thought of..."
"we should add..." | "what about including..."

INERTIA/FATIGUE (warning signals):
"ok" | "k" | "yes" | "fine" | "sure" | "go ahead" | "sounds good"
(3+ consecutive minimal responses = pause and check)

================================================================================
CONTEXT FRESHNESS CHECK TEMPLATES
BRIEF (< 2 hours, same day):
"Still working on [X]?"

STANDARD (overnight/next-day):
"Quick context check:
- Project: [name]
- Phase: [current]
- Last session: [summary]
- This session: [planned]
Match where you are?"

EXTENDED (multi-day gap):
"Let me verify our position:
- Project: [name] 
- Current phase: [phase] / Goal: [phase goal]
- Recent decisions: [key decisions]
- Open items: [pending items]
- Today's focus: [planned work]
All accurate, or has anything changed?"

================================================================================
EMERGENCY PROTOCOLS
TOKEN WALL APPROACHING:
1. Announce: "Approaching context limits"
2. Generate emergency bootstrap immediately
3. List files produced this session
4. List user actions required
5. Suggest session close

RUSHED USER EXIT:
1. Generate minimal bootstrap (current state + next actions only)
2. List critical user actions
3. Note "emergency closeout" in log
4. Full closeout can happen next session

UNRECOVERABLE FAILURE:
1. Stop all work
2. Inventory what's salvageable
3. Document failure in session log
4. Generate bootstrap to last known-good state
5. May require new chat to reset context

================================================================================
INSTRUCTIONS
This quick reference is designed for rapid AI lookup during dialog intake.

For full detail on any item, consult:
- Section 3.5: The Dialog Intake System (complete guidance)
- Appendix F: Dialog Intake Failure Mode Catalog (all failure modes)

DIALOG COMPLETION OPTION:
Users can reference this template directly by saying:
"Show me the intake quick reference" or "What's the confirmation tier for [X]?"

================================================================================
END OF TPL-011

================================================================================
TPL-022: PROJECT CLOSURE REPORT
================================================================================

PURPOSE:
Comprehensive documentation template for formally closing AI-assisted projects,
capturing lessons learned, final metrics, asset disposition, and knowledge
transfer requirements. Ensures systematic closure that preserves institutional
knowledge and informs future projects.

WHEN TO USE:
- Project completion (successful delivery)
- Project termination (early ending, strategic pivot)
- Phase completion requiring formal closure before next phase
- Annual/periodic review requiring project status formalization

RELATED SECTIONS:
- Section 5.8: Project Closure Report (lifecycle integration)
- Section 3.2: Phase Transition Standards
- Section 4.4: Decision Logs
- TPL-002: Decision Log Template (for final decisions)
- TPL-003: Session Log Template (for final session)

================================================================================
TEMPLATE CONTENT
================================================================================

--------------------------------------------------------------------------------
PROJECT CLOSURE REPORT
--------------------------------------------------------------------------------

PROJECT IDENTIFICATION
--------------------------------------------------------------------------------
Project Name:        [Full project name]
Project Code:        [Internal reference code if applicable]
Project Manager:     [Name/role]
AI Collaborator:     [Model/version used]
Report Date:         [YYYY-MM-DD]
Report Author:       [Name]
Report Version:      [e.g., v1.0]

CLOSURE CLASSIFICATION
--------------------------------------------------------------------------------
Closure Type:        [ ] Successful Completion
                     [ ] Early Termination
                     [ ] Strategic Pivot
                     [ ] Phase Completion
                     [ ] Administrative Closure

Closure Date:        [YYYY-MM-DD]
Original Target:     [YYYY-MM-DD]
Variance:            [+/- days, explanation if significant]

================================================================================
SECTION 1: EXECUTIVE SUMMARY
================================================================================

1.1 PROJECT OVERVIEW
--------------------------------------------------------------------------------
[2-3 paragraph summary of what the project was, why it was undertaken,
and what it accomplished. Write for stakeholders who may not have been
involved in day-to-day operations.]

Original Objectives:
1. [Primary objective]
2. [Secondary objective]
3. [Additional objectives as applicable]

Final Status:
[Brief statement of achievement level - e.g., "All primary objectives achieved,
one secondary objective deferred to future project."]

1.2 KEY OUTCOMES
--------------------------------------------------------------------------------
[Bullet list of tangible outcomes/deliverables produced]

• [Outcome 1]
• [Outcome 2]
• [Outcome 3]
• [Additional outcomes]

1.3 RECOMMENDATION SUMMARY
--------------------------------------------------------------------------------
[2-3 sentences capturing the most important recommendations for future
similar projects, extracted from detailed lessons learned below]

================================================================================
SECTION 2: SCOPE ANALYSIS
================================================================================

2.1 ORIGINAL VS. FINAL SCOPE
--------------------------------------------------------------------------------

ORIGINAL SCOPE (from project charter/initiation):
[Copy or summarize original scope statement]

FINAL SCOPE (as delivered):
[Document actual scope delivered]

SCOPE CHANGES:
| Change # | Description           | Reason          | Impact    | Approved |
|----------|-----------------------|-----------------|-----------|----------|
| SC-001   | [Change description]  | [Why changed]   | [Impact]  | [Y/N/NA] |
| SC-002   | [Change description]  | [Why changed]   | [Impact]  | [Y/N/NA] |

2.2 SCOPE VARIANCE ANALYSIS
--------------------------------------------------------------------------------
Added Items:     [List items added to scope during project]
Removed Items:   [List items removed from scope]
Modified Items:  [List items significantly modified]

Net Assessment:  [ ] Scope Maintained
                 [ ] Scope Expanded (controlled)
                 [ ] Scope Reduced (controlled)
                 [ ] Scope Creep Occurred

Lessons for Scope Management:
[What would you do differently regarding scope definition and control?]

================================================================================
SECTION 3: TIMELINE ANALYSIS
================================================================================

3.1 MILESTONE ACHIEVEMENT
--------------------------------------------------------------------------------

| Milestone           | Planned      | Actual       | Variance | Notes        |
|---------------------|--------------|--------------|----------|--------------|
| Project Kickoff     | [Date]       | [Date]       | [Days]   | [Notes]      |
| [Phase 1 Complete]  | [Date]       | [Date]       | [Days]   | [Notes]      |
| [Phase 2 Complete]  | [Date]       | [Date]       | [Days]   | [Notes]      |
| [Key Deliverable]   | [Date]       | [Date]       | [Days]   | [Notes]      |
| Project Closure     | [Date]       | [Date]       | [Days]   | [Notes]      |

3.2 TIMELINE VARIANCE ANALYSIS
--------------------------------------------------------------------------------
Total Planned Duration:    [X days/weeks/months]
Total Actual Duration:     [X days/weeks/months]
Overall Variance:          [+/- X days] ([X]%)

Primary Delay Factors:
1. [Factor and impact]
2. [Factor and impact]

Primary Acceleration Factors:
1. [Factor and impact]
2. [Factor and impact]

Lessons for Timeline Management:
[What would you do differently regarding scheduling and time estimation?]

================================================================================
SECTION 4: RESOURCE UTILIZATION
================================================================================

4.1 HUMAN RESOURCES
--------------------------------------------------------------------------------

| Role                | Planned Hours | Actual Hours | Variance | Notes       |
|---------------------|---------------|--------------|----------|-------------|
| Project Manager     | [Hours]       | [Hours]      | [%]      | [Notes]     |
| [Role 2]            | [Hours]       | [Hours]      | [%]      | [Notes]     |
| [Role 3]            | [Hours]       | [Hours]      | [%]      | [Notes]     |

4.2 AI RESOURCES
--------------------------------------------------------------------------------

| Resource Type       | Planned       | Actual       | Variance | Notes       |
|---------------------|---------------|--------------|----------|-------------|
| Total Sessions      | [Count]       | [Count]      | [%]      | [Notes]     |
| Tokens Consumed     | [Estimate]    | [Actual]     | [%]      | [Notes]     |
| Branch Sessions     | [Count]       | [Count]      | [%]      | [Notes]     |

AI Utilization Patterns:
[Describe how AI was used - phases of heavy use, light use, etc.]

4.3 FINANCIAL RESOURCES (if applicable)
--------------------------------------------------------------------------------

| Category            | Budgeted      | Actual       | Variance | Notes       |
|---------------------|---------------|--------------|----------|-------------|
| [Category 1]        | [$Amount]     | [$Amount]    | [%]      | [Notes]     |
| [Category 2]        | [$Amount]     | [$Amount]    | [%]      | [Notes]     |
| Total               | [$Amount]     | [$Amount]    | [%]      | [Notes]     |

4.4 RESOURCE LESSONS LEARNED
--------------------------------------------------------------------------------
[What would you do differently regarding resource planning and allocation?]

================================================================================
SECTION 5: DELIVERABLES ACCEPTANCE
================================================================================

5.1 DELIVERABLE STATUS
--------------------------------------------------------------------------------

| Deliverable ID | Name                | Status     | Accepted By | Date      |
|----------------|---------------------|------------|-------------|-----------|
| DEL-001        | [Deliverable name]  | [Status]   | [Name]      | [Date]    |
| DEL-002        | [Deliverable name]  | [Status]   | [Name]      | [Date]    |
| DEL-003        | [Deliverable name]  | [Status]   | [Name]      | [Date]    |

Status Options: Accepted / Accepted with Conditions / Rejected / Deferred / N/A

5.2 ACCEPTANCE CRITERIA VERIFICATION
--------------------------------------------------------------------------------

Deliverable: [DEL-001 Name]
| Criterion                    | Met? | Evidence/Notes                        |
|------------------------------|------|---------------------------------------|
| [Criterion 1]                | Y/N  | [Evidence]                            |
| [Criterion 2]                | Y/N  | [Evidence]                            |

[Repeat for each major deliverable]

5.3 OUTSTANDING ITEMS
--------------------------------------------------------------------------------
[List any items not delivered, with explanation and disposition]

| Item                | Reason Not Delivered | Disposition                       |
|---------------------|----------------------|-----------------------------------|
| [Item]              | [Reason]             | [Transferred to X / Cancelled]    |

================================================================================
SECTION 6: QUALITY ASSESSMENT
================================================================================

6.1 QUALITY METRICS
--------------------------------------------------------------------------------

| Metric                      | Target       | Actual       | Assessment      |
|-----------------------------|--------------|--------------|-----------------|
| Defect Rate                 | [Target]     | [Actual]     | [Met/Not Met]   |
| Rework Percentage           | [Target]     | [Actual]     | [Met/Not Met]   |
| Stakeholder Satisfaction    | [Target]     | [Actual]     | [Met/Not Met]   |
| [Custom Metric]             | [Target]     | [Actual]     | [Met/Not Met]   |

6.2 QUALITY ISSUES LOG
--------------------------------------------------------------------------------

| Issue ID | Description          | Severity | Resolution      | Prevented By    |
|----------|----------------------|----------|-----------------|-----------------|
| QI-001   | [Issue description]  | H/M/L    | [How resolved]  | [Future prevent]|
| QI-002   | [Issue description]  | H/M/L    | [How resolved]  | [Future prevent]|

6.3 AI-SPECIFIC QUALITY OBSERVATIONS
--------------------------------------------------------------------------------
[Document quality patterns specific to AI collaboration]

Strengths Observed:
• [AI capability that enhanced quality]
• [AI capability that enhanced quality]

Challenges Observed:
• [AI limitation that affected quality]
• [AI limitation that affected quality]

Mitigation Effectiveness:
[How well did your quality controls work for AI-generated content?]

================================================================================
SECTION 7: RISK AND ISSUE RESOLUTION
================================================================================

7.1 RISK REGISTER FINAL STATUS
--------------------------------------------------------------------------------

| Risk ID | Description          | Initial  | Final    | Resolution              |
|---------|----------------------|----------|----------|-------------------------|
| R-001   | [Risk description]   | [Rating] | [Status] | [How resolved/avoided]  |
| R-002   | [Risk description]   | [Rating] | [Status] | [How resolved/avoided]  |

Final Status Options: Closed-Avoided / Closed-Mitigated / Closed-Accepted / 
                      Occurred-Managed / Occurred-Impact / Transferred

7.2 ISSUE LOG FINAL STATUS
--------------------------------------------------------------------------------

| Issue ID | Description          | Severity | Status   | Resolution              |
|----------|----------------------|----------|----------|-------------------------|
| I-001    | [Issue description]  | H/M/L    | [Status] | [Resolution summary]    |
| I-002    | [Issue description]  | H/M/L    | [Status] | [Resolution summary]    |

7.3 UNRESOLVED ITEMS
--------------------------------------------------------------------------------
[Document any risks or issues being transferred or remaining open]

| Item     | Type      | Current Status | Transfer To     | Notes               |
|----------|-----------|----------------|-----------------|---------------------|
| [ID]     | Risk/Issue| [Status]       | [Project/Owner] | [Transition notes]  |

================================================================================
SECTION 8: LESSONS LEARNED
================================================================================

8.1 WHAT WORKED WELL
--------------------------------------------------------------------------------

Category: Project Management
• [Lesson - what specifically worked and why]
• [Lesson - what specifically worked and why]

Category: AI Collaboration
• [Lesson - what specifically worked and why]
• [Lesson - what specifically worked and why]

Category: Technical Execution
• [Lesson - what specifically worked and why]
• [Lesson - what specifically worked and why]

Category: Stakeholder Management
• [Lesson - what specifically worked and why]
• [Lesson - what specifically worked and why]

8.2 WHAT COULD BE IMPROVED
--------------------------------------------------------------------------------

Category: Project Management
• [Issue] → [Recommended improvement]
• [Issue] → [Recommended improvement]

Category: AI Collaboration
• [Issue] → [Recommended improvement]
• [Issue] → [Recommended improvement]

Category: Technical Execution
• [Issue] → [Recommended improvement]
• [Issue] → [Recommended improvement]

Category: Stakeholder Management
• [Issue] → [Recommended improvement]
• [Issue] → [Recommended improvement]

8.3 AI-SPECIFIC LESSONS
--------------------------------------------------------------------------------

Context Management:
[Lessons about managing AI context, session continuity, bootstrap documents]

Prompt Engineering:
[Lessons about effective prompting, what worked, what didn't]

Quality Control:
[Lessons about verifying AI output, catching errors, validation approaches]

Collaboration Patterns:
[Lessons about human-AI workflow, handoffs, decision boundaries]

8.4 RECOMMENDATIONS FOR FUTURE PROJECTS
--------------------------------------------------------------------------------

Must Do (Critical):
1. [Recommendation that future projects must adopt]
2. [Recommendation that future projects must adopt]

Should Do (High Value):
1. [Recommendation that would significantly benefit future projects]
2. [Recommendation that would significantly benefit future projects]

Consider (Situational):
1. [Recommendation that may help depending on context]
2. [Recommendation that may help depending on context]

================================================================================
SECTION 9: ASSET DISPOSITION
================================================================================

9.1 DOCUMENT ARCHIVE
--------------------------------------------------------------------------------

| Asset Type          | Location            | Retention    | Owner           |
|---------------------|---------------------|--------------|-----------------|
| Project Charter     | [Path/URL]          | [Period]     | [Owner]         |
| Session Logs        | [Path/URL]          | [Period]     | [Owner]         |
| Decision Logs       | [Path/URL]          | [Period]     | [Owner]         |
| Deliverables        | [Path/URL]          | [Period]     | [Owner]         |
| Chat Transcripts    | [Path/URL]          | [Period]     | [Owner]         |
| Bootstrap Docs      | [Path/URL]          | [Period]     | [Owner]         |
| [Other Assets]      | [Path/URL]          | [Period]     | [Owner]         |

9.2 KNOWLEDGE ASSETS
--------------------------------------------------------------------------------

Templates Produced:
[List any reusable templates created during project]
• [Template name] - [Description] - [Location]

Prompts Developed:
[List any effective prompts worth preserving]
• [Prompt name/purpose] - [Location in Golden Prompt Library or other]

Patterns Documented:
[List any new patterns or approaches worth capturing]
• [Pattern name] - [Description] - [Location]

9.3 SYSTEM/ACCESS DISPOSITION
--------------------------------------------------------------------------------

| System/Access       | Current Status | Disposition Action | Completion Date |
|---------------------|----------------|--------------------|-----------------| 
| [System access 1]   | [Status]       | [Revoke/Maintain]  | [Date]          |
| [API keys]          | [Status]       | [Rotate/Revoke]    | [Date]          |
| [Shared folders]    | [Status]       | [Archive/Delete]   | [Date]          |

================================================================================
SECTION 10: STAKEHOLDER SIGN-OFF
================================================================================

10.1 FORMAL ACCEPTANCE
--------------------------------------------------------------------------------

Project Closure Acceptance:

By signing below, stakeholders acknowledge:
• All deliverables have been reviewed
• Outstanding items have been documented and dispositioned
• The project may be formally closed

| Role                | Name           | Signature    | Date              |
|---------------------|----------------|--------------|-------------------|
| Project Sponsor     | ______________ | ____________ | _________________ |
| Project Manager     | ______________ | ____________ | _________________ |
| Key Stakeholder     | ______________ | ____________ | _________________ |
| [Additional]        | ______________ | ____________ | _________________ |

10.2 CLOSURE NOTES
--------------------------------------------------------------------------------
[Any final notes, conditions, or comments from stakeholders]

================================================================================
SECTION 11: APPENDICES (as needed)
================================================================================

Appendix A: Final Project Metrics Dashboard
[Include or reference final metrics visualization]

Appendix B: Complete Session Log Index
[List all sessions with links/references]

Appendix C: Decision Log Summary
[Major decisions made during project]

Appendix D: [Project-Specific Appendix]
[Any additional documentation needed]

================================================================================
USAGE INSTRUCTIONS
================================================================================

COMPLETING THIS TEMPLATE:

1. START EARLY
   Begin populating this template during the final phase, not after.
   Gather metrics and lessons while they're fresh.

2. BE HONEST
   The value is in accurate assessment, not positive spin.
   Future projects benefit from candid lessons learned.

3. QUANTIFY WHERE POSSIBLE
   Metrics tell the story better than narrative alone.
   Include actual numbers, dates, and measurements.

4. FOCUS ON ACTIONABLE LESSONS
   "Communication could improve" is not actionable.
   "Weekly stakeholder email with standard format" is actionable.

5. OBTAIN SIGN-OFF
   Formal closure prevents project resurrection and scope disputes.
   Get signatures before archiving.

SCALING GUIDANCE:

Small Projects (< 10 sessions):
- Sections 1, 5, 8, 10 required
- Other sections abbreviated or omitted
- Single-page executive summary may suffice

Medium Projects (10-50 sessions):
- All sections recommended
- Detail level moderate
- Focus on lessons learned

Large Projects (50+ sessions):
- All sections required
- High detail level
- Consider separate lessons learned workshop
- May require multiple reviewers

AI-ASSISTED COMPLETION:

This template can be partially completed by AI using:
• Session logs as source material
• Decision logs for Section 7 and lessons
• Project artifacts for deliverables list

Human verification required for:
• Stakeholder satisfaction assessment
• Political/organizational lessons
• Sign-off sections
• Final recommendations

CROSS-REFERENCES:

Related Templates:
• TPL-001: Project Charter (original scope reference)
• TPL-002: Decision Log (decision history)
• TPL-003: Session Log (session details)
• TPL-023: Backlog Register (deferred items)

Related Sections:
• Section 5.8: Project Closure Report (process guidance)
• Section 3.2: Phase Transition Standards
• Section 6.3: Knowledge Preservation

================================================================================
END OF TPL-022
================================================================================

================================================================================
TPL-023: BACKLOG REGISTER
================================================================================

PURPOSE:
Living document for capturing, organizing, and tracking items deferred during
project execution. Serves as both a parking lot during active work and a
structured input source for future planning. Prevents good ideas from being
lost while maintaining project focus.

WHEN TO USE:
- Throughout project lifecycle to capture deferred items
- During scope discussions to document out-of-scope items
- At phase transitions to review accumulated backlog
- During planning to source future work items
- At project closure to document outstanding items

RELATED SECTIONS:
- Section 5.7: Backlog Register (lifecycle integration)
- Section 3.1: Scope Management Principles
- Section 5.8: Project Closure Report
- TPL-022: Project Closure Report (transfers open items)

================================================================================
TEMPLATE CONTENT
================================================================================

--------------------------------------------------------------------------------
BACKLOG REGISTER
--------------------------------------------------------------------------------

PROJECT IDENTIFICATION
--------------------------------------------------------------------------------
Project Name:        [Full project name]
Project Code:        [Internal reference code if applicable]
Register Owner:      [Name/role responsible for maintenance]
Created Date:        [YYYY-MM-DD]
Last Updated:        [YYYY-MM-DD]
Version:             [e.g., v1.0, v2.3]

REGISTER SUMMARY
--------------------------------------------------------------------------------
Total Items:         [Count]
High Priority:       [Count]
Medium Priority:     [Count]
Low Priority:        [Count]
Completed/Closed:    [Count]

================================================================================
SECTION 1: ACTIVE BACKLOG
================================================================================

Items in this section are confirmed backlog - reviewed, prioritized, and
awaiting future action.

--------------------------------------------------------------------------------
HIGH PRIORITY ITEMS
--------------------------------------------------------------------------------
Items that should be addressed in next planning cycle or represent
significant technical debt.

| ID      | Item Description                    | Type    | Source  | Added   |
|---------|-------------------------------------|---------|---------|---------|
| BL-001  | [Clear description of backlog item] | [Type]  | [Where] | [Date]  |
|         | Context: [Additional context]       |         |         |         |
|         | Dependencies: [Any dependencies]    |         |         |         |
|         | Est. Effort: [T-shirt size or hrs]  |         |         |         |
|---------|-------------------------------------|---------|---------|---------|
| BL-002  | [Clear description of backlog item] | [Type]  | [Where] | [Date]  |
|         | Context: [Additional context]       |         |         |         |
|         | Dependencies: [Any dependencies]    |         |         |         |
|         | Est. Effort: [T-shirt size or hrs]  |         |         |         |

Type Options: Enhancement / Bug-Fix / Technical-Debt / Documentation / 
              Research / Process / Integration / Other

--------------------------------------------------------------------------------
MEDIUM PRIORITY ITEMS
--------------------------------------------------------------------------------
Items that add value but are not urgent. Consider for future phases or
when capacity allows.

| ID      | Item Description                    | Type    | Source  | Added   |
|---------|-------------------------------------|---------|---------|---------|
| BL-003  | [Clear description of backlog item] | [Type]  | [Where] | [Date]  |
|         | Context: [Additional context]       |         |         |         |
|         | Dependencies: [Any dependencies]    |         |         |         |
|         | Est. Effort: [T-shirt size or hrs]  |         |         |         |

--------------------------------------------------------------------------------
LOW PRIORITY ITEMS
--------------------------------------------------------------------------------
Nice-to-have items. May never be implemented but worth tracking.

| ID      | Item Description                    | Type    | Source  | Added   |
|---------|-------------------------------------|---------|---------|---------|
| BL-004  | [Clear description of backlog item] | [Type]  | [Where] | [Date]  |
|         | Context: [Additional context]       |         |         |         |

================================================================================
SECTION 2: INTAKE QUEUE
================================================================================

Items in this section are newly captured and awaiting triage. They have not
yet been prioritized or validated for inclusion in active backlog.

--------------------------------------------------------------------------------
PENDING TRIAGE
--------------------------------------------------------------------------------

| Temp ID | Item Description                    | Captured | Source           |
|---------|-------------------------------------|----------|------------------|
| TMP-001 | [Raw capture of potential item]     | [Date]   | [Session/Source] |
|         | Raw Notes: [Any initial notes]      |          |                  |
|---------|-------------------------------------|----------|------------------|
| TMP-002 | [Raw capture of potential item]     | [Date]   | [Session/Source] |
|         | Raw Notes: [Any initial notes]      |          |                  |

TRIAGE PROCESS:
1. Review item description for clarity
2. Determine if item is valid (not duplicate, not already addressed)
3. Assign type and priority
4. Add context, dependencies, and effort estimate
5. Assign permanent ID (BL-###)
6. Move to appropriate priority section

================================================================================
SECTION 3: COMPLETED/CLOSED ITEMS
================================================================================

Items that have been addressed or intentionally closed without action.
Maintained for historical reference.

--------------------------------------------------------------------------------
COMPLETED ITEMS
--------------------------------------------------------------------------------

| ID      | Item Description            | Completed | Resolution               |
|---------|------------------------------|----------|--------------------------|
| BL-005  | [Description]                | [Date]   | [How it was resolved]    |
|         | Implemented in: [Reference]  |          |                          |

--------------------------------------------------------------------------------
CLOSED WITHOUT ACTION
--------------------------------------------------------------------------------

| ID      | Item Description            | Closed   | Reason                    |
|---------|------------------------------|----------|--------------------------|
| BL-006  | [Description]                | [Date]   | [Why closed without work] |

Closure Reasons: Duplicate / No-Longer-Relevant / Out-of-Scope / 
                 Superseded / Will-Not-Fix / Merged-With-[ID]

================================================================================
SECTION 4: THEMATIC GROUPINGS
================================================================================

Optional section for organizing backlog items by theme, component, or
workstream for planning purposes.

--------------------------------------------------------------------------------
THEME: [Theme Name 1]
--------------------------------------------------------------------------------
Description: [What this theme encompasses]

Related Items:
• BL-001: [Brief description]
• BL-003: [Brief description]

Theme Notes:
[Any planning notes about addressing this theme as a group]

--------------------------------------------------------------------------------
THEME: [Theme Name 2]
--------------------------------------------------------------------------------
Description: [What this theme encompasses]

Related Items:
• BL-002: [Brief description]
• BL-004: [Brief description]

Theme Notes:
[Any planning notes about addressing this theme as a group]

================================================================================
SECTION 5: METRICS AND TRENDS
================================================================================

BACKLOG HEALTH METRICS
--------------------------------------------------------------------------------

| Period          | Added | Completed | Closed | Net Change | Total Active |
|-----------------|-------|-----------|--------|------------|--------------|
| [Month/Sprint]  | [#]   | [#]       | [#]    | [+/-]      | [#]          |
| [Month/Sprint]  | [#]   | [#]       | [#]    | [+/-]      | [#]          |
| [Month/Sprint]  | [#]   | [#]       | [#]    | [+/-]      | [#]          |

AGING ANALYSIS
--------------------------------------------------------------------------------

| Age Bucket      | High Priority | Medium Priority | Low Priority |
|-----------------|---------------|-----------------|--------------|
| < 30 days       | [#]           | [#]             | [#]          |
| 30-90 days      | [#]           | [#]             | [#]          |
| 90-180 days     | [#]           | [#]             | [#]          |
| > 180 days      | [#]           | [#]             | [#]          |

Items older than 180 days should be reviewed for continued relevance.

================================================================================
SECTION 6: CHANGE LOG
================================================================================

| Date       | Changed By  | Change Description                              |
|------------|-------------|-------------------------------------------------|
| [Date]     | [Name]      | [Description of change to register]             |
| [Date]     | [Name]      | [Description of change to register]             |

================================================================================
USAGE INSTRUCTIONS
================================================================================

CAPTURING ITEMS:

During Work:
"That's a good idea but out of scope - let me add it to the backlog."
→ Add to Intake Queue with date and source (e.g., "Session 015")

During Reviews:
"We should eventually do X" or "Future enhancement: Y"
→ Add to Intake Queue for later triage

From Project Closure:
Outstanding items from TPL-022 → Transfer to this register with context

QUICK CAPTURE FORMAT:
[BACKLOG] Brief description | Source: Session X | Type: Enhancement

TRIAGE CADENCE:

Weekly (Active Projects):
- Review Intake Queue
- Triage new items to Active Backlog or Close
- 15-30 minutes

Phase Transitions:
- Full backlog review
- Re-prioritize based on next phase needs
- Identify items to address in upcoming phase
- 1-2 hours

Project Closure:
- Final triage of all pending items
- Transfer active items to successor project or organizational backlog
- Close items that will not be transferred
- Document in closure report

PRIORITIZATION CRITERIA:

High Priority:
• Blocking future work
• Significant technical debt accumulating interest
• Stakeholder commitment made
• Security or compliance related

Medium Priority:
• Clear value but not urgent
• Enables future capabilities
• Improves efficiency or quality
• Requested by stakeholders but not committed

Low Priority:
• Nice-to-have
• Uncertain value
• Significant effort for marginal gain
• Dependent on other uncommitted work

ID NUMBERING:
- BL-### for confirmed backlog items (BL-001, BL-002, etc.)
- TMP-### for intake queue items (temporary, reassigned after triage)
- Maintain sequential numbering; do not reuse IDs

SCALING GUIDANCE:

Small Projects:
- Sections 1-2 sufficient
- Simple list may replace detailed tracking
- Review at project end only

Medium Projects:
- All sections recommended
- Weekly triage
- Phase-end comprehensive review

Large Projects:
- Full tracking with metrics
- May need sub-registers by workstream
- Dedicated backlog grooming sessions
- Integration with project management tools

AI-ASSISTED MAINTENANCE:

Capture Support:
AI can help identify potential backlog items from session discussions:
"Based on our conversation, should any of these be added to backlog?"

Triage Support:
AI can help draft item descriptions and suggest priority:
"Help me triage these raw backlog captures"

Review Support:
AI can help analyze backlog for patterns:
"What themes emerge from current backlog items?"

Human Decision Required:
- Final priority assignment
- Closure decisions
- Commitment to implement

CROSS-REFERENCES:

Related Templates:
• TPL-022: Project Closure Report (source of transferred items)
• TPL-001: Project Charter (scope reference for backlog decisions)

Related Sections:
• Section 5.7: Backlog Register (process guidance)
• Section 3.1: Scope Management Principles

================================================================================
END OF TPL-023
================================================================================

================================================================================
END OF APPENDIX A
================================================================================
================================================================================
================================================================================
================================================================================


================================================================================
APPENDIX B: PROMPT REFERENCE
This appendix contains the Golden Prompt Library—a curated collection of 
refined, high-performing prompts that have proven effective across AI-assisted 
projects. Rather than reinventing prompts each session or losing successful 
formulations in chat transcripts, this library provides a reusable reference.

--------------------------------------------------------------------------------
ABOUT THE PROMPT LIBRARY
--------------------------------------------------------------------------------

PURPOSE:
The prompt library captures prompts that have been tested and refined through 
actual use. These are not theoretical constructs but working tools that have 
demonstrated effectiveness in real project contexts.

USAGE:
- Before starting common tasks, check the library for existing prompts
- Copy and adapt as needed (replace {{VARIABLES}} with specifics)
- Note adaptations that improve performance—these become version updates
- Add new prompts when you develop ones that work exceptionally well

MAINTENANCE:
- Review quarterly for stale or underperforming entries
- Archive deprecated prompts rather than deleting
- Version prompts that undergo significant refinement

INTEGRATION:
- Upload to NotebookLM for queryable access
- Reference during project rooting to select appropriate prompts
- See Section 10.7 of the AI Operations Manual for full guidance

--------------------------------------------------------------------------------
PROMPT CATEGORIES
--------------------------------------------------------------------------------

| Category    | Code   | Description                                        |
|-------------|--------|----------------------------------------------------|
| Alignment   | ALIGN  | Establishing shared understanding, confirming params|
| Drafting    | DRAFT  | Content generation, document creation              |
| Verification| VERIF  | Cross-checking, validation, quality assurance      |
| Recovery    | RECOV  | Context reconstruction, failure recovery, resumption|
| Pivot       | PIVOT  | Redirecting AI when off-track or stuck             |
| Closeout    | CLOSE  | Session ending, bootstrap generation, handoff prep |

--------------------------------------------------------------------------------
PROMPT INDEX
--------------------------------------------------------------------------------

| ID        | Name                          | Category    | Use Case            |
|-----------|-------------------------------|-------------|---------------------|
| ALIGN-001 | Explicit Alignment Grounding  | Alignment   | Project initiation  |
| RECOV-001 | Bootstrap Generator           | Recovery    | Session handoff     |
| QA-001    | Cognitive Divergence Audit    | Verification| Drift detection     |
| VERIF-001 | Adversarial Cross-AI Validator| Verification| Tier 2 validation   |


================================================================================
PROMPT ENTRIES

--------------------------------------------------------------------------------
PROMPT ID: ALIGN-001
--------------------------------------------------------------------------------

NAME:           Explicit Alignment Grounding

CATEGORY:       Alignment

CONTEXT:        Start of Root Chat or after major pivot. Use when establishing 
                shared understanding before substantive work begins.

MODEL AFFINITY: Model-Agnostic (tested: Claude, GPT-4)

THE PROMPT:

    I am initiating a project under the GOV-AIPM framework. I will provide 
    {{PROJECT_INITIATION_FORM or PROJECT_GUIDE}}. Your task is to:

    1. Internalize the scope and constraints
    2. Identify any assumptions you are making about this request
    3. Ask 3-5 clarifying questions that, once answered, will ensure alignment

    Do not begin drafting until I confirm we are aligned.

WHAT TO AVOID:
- Don't skip the clarifying questions step even if the brief seems clear
- Don't provide assumptions as questions—state them as assumptions for 
  user confirmation

PERFORMANCE NOTES:
- High success rate in preventing mid-project pivots
- The "identify assumptions" step catches approximately 80% of alignment gaps 
  early in the process

RELATED PROMPTS: RECOV-001 (for session handoff alignment)

SOURCE:         GOV-AIPM development

VERSION:        1.0    LAST UPDATED: December 21, 2025

--------------------------------------------------------------------------------


--------------------------------------------------------------------------------
PROMPT ID: RECOV-001
--------------------------------------------------------------------------------

NAME:           Bootstrap Generator

CATEGORY:       Recovery / Closeout

CONTEXT:        End of session when preparing handoff to next chat. Use when 
                approaching token limits or completing a session phase.

MODEL AFFINITY: Claude-optimized (structure aligns with TPL-002)

THE PROMPT:

    We are closing this session. Review our conversation and generate a 
    bootstrap for the next AI instance. Include:

    1. Current project state and logic position
    2. All decisions made (reference Decision Log entries by number)
    3. Open items and immediate next actions
    4. Warning signs the next instance should monitor for context drift

    Structure per TPL-002. Prioritize fidelity over brevity, but compress 
    where possible without losing critical context.

WHAT TO AVOID:
- Don't sacrifice decision rationale for compression
- Don't omit warning signs even if session went smoothly

PERFORMANCE NOTES:
- Works best when Decision Log has been maintained during session
- "Warning signs" section catches issues that manifest in subsequent sessions

RELATED PROMPTS: ALIGN-001 (for receiving chat alignment)

SOURCE:         GOV-AIPM development

VERSION:        1.0    LAST UPDATED: December 21, 2025

--------------------------------------------------------------------------------


--------------------------------------------------------------------------------
PROMPT ID: QA-001
--------------------------------------------------------------------------------

NAME:           Cognitive Divergence Audit

CATEGORY:       Verification / Pivot

CONTEXT:        When user senses AI is looping, drifting, or not tracking 
                intent. Use before escalating to Tier 3 Human Review. 
                Typically invoked after Three-Strike threshold (two failed 
                correction attempts).

MODEL AFFINITY: Model-Agnostic

THE PROMPT:

    I believe we are experiencing cognitive divergence. I will provide:
    - My intended logic path: {{USER_INTENT}}
    - Your last 2-3 outputs that seem misaligned

    Analyze the gap between my intent and your execution. Do not apologize 
    or explain defensively. Provide:

    1. Where the logic branched (specific point of divergence)
    2. What assumption or interpretation caused the branch
    3. A correction path to re-align with {{ROOT_OBJECTIVES}}

WHAT TO AVOID:
- Don't accept AI deflection ("I understand now, let me try again" without 
  analysis)
- Don't use if the issue is simple misunderstanding vs. systematic drift

PERFORMANCE NOTES:
- Most effective after 2 failed correction attempts (Three-Strike threshold)
- The "do not apologize" instruction improves diagnostic quality significantly
- If this prompt fails to correct, proceed to fresh chat or different AI tool

RELATED PROMPTS: See TPL-009 Protocol G if divergence persists after correction

CROSS-REFERENCE: Section 14.3 (Cognitive Divergence Detection)

SOURCE:         GOV-AIPM development

VERSION:        1.0    LAST UPDATED: December 21, 2025

--------------------------------------------------------------------------------


--------------------------------------------------------------------------------
PROMPT ID: VERIF-001
--------------------------------------------------------------------------------

NAME:           Adversarial Cross-AI Validator

CATEGORY:       Verification

CONTEXT:        Tier 2 validation using secondary AI. Use when primary AI 
                output requires independent verification per TPL-008.

MODEL AFFINITY: Best used in AI different from the one that produced the 
                work (e.g., GPT-4 validating Claude output, or vice versa)

THE PROMPT:

    You are an adversarial auditor. Review the attached {{DOCUMENT_TYPE}} 
    generated by another AI.

    Identify:
    1. Logic gaps or unsupported claims
    2. Instances of "hallucinated consistency" (conclusions that sound 
       coherent but lack foundation)
    3. Sections where the AI was "too agreeable" (accepted flawed premises)
    4. Tone shifts from technical/instructional to conversational filler

    Provide a Reliability Score (1-10) with specific findings supporting 
    your rating. Do not soften criticism.

WHAT TO AVOID:
- Don't use the same AI that produced the work
- Don't accept high scores without specific supporting evidence

PERFORMANCE NOTES:
- "Too agreeable" check catches sycophantic patterns effectively
- Works best when validator has no context from original session (fresh eyes)
- Reliability Score provides quick triage for further review needs

RELATED PROMPTS: See TPL-008 for full validation protocol

SOURCE:         GOV-AIPM development

VERSION:        1.0    LAST UPDATED: December 21, 2025

--------------------------------------------------------------------------------


================================================================================
ADDITIONAL PROMPT PATTERNS
The following prompt patterns appear throughout the manual and are useful 
for specific situations. They are presented in condensed form for quick 
reference.

--------------------------------------------------------------------------------
ALIGNMENT PROMPTS (Pre-Work)
--------------------------------------------------------------------------------

RESTATE UNDERSTANDING:
"Before proceeding, restate what you understand the task to be."

SURFACE ASSUMPTIONS:
"What assumptions are you making about this request?"

FAILURE ANTICIPATION:
"What would cause this approach to fail?"

SCOPE CONFIRMATION:
"Confirm: What is IN scope and what is OUT of scope for this task?"

--------------------------------------------------------------------------------
DRAFTING PROMPTS
--------------------------------------------------------------------------------

STRUCTURED OUTPUT REQUEST:
"Generate [output type] with the following structure: [structure]. 
Include [required elements]. Omit [excluded elements]."

ITERATIVE REFINEMENT:
"Here is your previous output. Revise to address: [specific issues]. 
Maintain [elements to preserve]. Do not change [protected elements]."

TONE CALIBRATION:
"Adjust the tone of this draft to be more [formal/casual/technical/
accessible]. The audience is [audience description]."

--------------------------------------------------------------------------------
VERIFICATION PROMPTS
--------------------------------------------------------------------------------

SELF-CHECK:
"Review your output for: alignment with objective, gaps in coverage, 
unstated assumptions. Report findings before I proceed."

SOURCE CHECK:
"For each factual claim in your response, indicate your confidence level 
and whether verification is recommended."

CONSISTENCY CHECK:
"Compare this output to [prior output/established parameter]. Identify 
any inconsistencies or contradictions."

--------------------------------------------------------------------------------
RECOVERY PROMPTS
--------------------------------------------------------------------------------

CONTEXT RECONSTRUCTION:
"I need to reconstruct context. Based on [available materials], summarize:
current state, recent decisions, and immediate next actions."

FRESH START:
"Disregard our previous exchange on this topic. Let me reframe the request 
from the beginning: [fresh framing]."

CLARIFICATION REQUEST:
"I'm not confident we're aligned. Please tell me: What do you understand 
the objective to be? What are you trying to accomplish with your response?"

--------------------------------------------------------------------------------
CLOSEOUT PROMPTS
--------------------------------------------------------------------------------

SESSION SUMMARY:
"Summarize this session: objectives, accomplishments, decisions made, 
open items, and recommended next actions."

DECISION EXTRACTION:
"List all decisions made in this session in the format: 
Decision: [statement] | Rationale: [brief rationale]"

FILE INVENTORY:
"List all files we've discussed creating or modifying this session, 
with their current status (created/modified/pending)."


================================================================================
ADDING NEW PROMPTS TO THE LIBRARY
When a prompt works exceptionally well:

1. NOTE IT during session (flag in Session Log under "Process Observations")
2. DRAFT ENTRY at session closeout or weekly review
3. GENERALIZE the prompt (remove project-specific references, add variables)
4. ADD METADATA (context, model affinity, what to avoid, performance notes)
5. ASSIGN ID per category convention: [CATEGORY]-[###]
6. APPEND to this library

--------------------------------------------------------------------------------
NEW PROMPT ENTRY TEMPLATE
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
PROMPT ID: [CATEGORY]-[###]
--------------------------------------------------------------------------------

NAME:           [Descriptive name]

CATEGORY:       [Alignment / Drafting / Verification / Recovery / Pivot / 
                Closeout]

CONTEXT:        [When to use—situation or trigger]

MODEL AFFINITY: [Claude / GPT / Gemini / Model-Agnostic]
                [Notes on cross-model performance if tested]

THE PROMPT:

    [Exact prompt text with {{VARIABLES}} for user substitution]

WHAT TO AVOID:
- [Known failure modes or anti-patterns when using this prompt]

PERFORMANCE NOTES:
- [Success rate or qualitative assessment]
- [Variations for specific contexts]

RELATED PROMPTS: [Cross-references to related entries]

SOURCE:         [Project/session origin]

VERSION:        [#]    LAST UPDATED: [Date]

--------------------------------------------------------------------------------


================================================================================
VERSION HISTORY
| Version | Date       | Changes                                            |
|---------|------------|----------------------------------------------------|
| 1.0     | 2025-12-21 | Initial library with 4 core prompts                |

================================================================================
END OF APPENDIX B

================================================================================
APPENDIX C: TOOL CONFIGURATION GUIDE
This appendix provides setup guidance for tools commonly used in AI-assisted 
project management. The guidance is tool-agnostic in principle—focusing on 
capabilities needed rather than specific products—with concrete examples for 
commonly used platforms.

--------------------------------------------------------------------------------
CONFIGURATION PHILOSOPHY
--------------------------------------------------------------------------------

The GOV-AIPM framework does not mandate specific tools. Tool selection should 
be based on:

1. CAPABILITY MATCH: Does the tool provide the needed functionality?
2. INTEGRATION: Does it work with your existing workflow?
3. SUSTAINABILITY: Will it be available and maintained long-term?
4. SECURITY: Does it meet your data handling requirements?

When configuring any tool, establish these elements:
- Clear purpose (what this tool does in your workflow)
- Integration points (how it connects with other tools)
- Data flow (what information goes in/out)
- Retention policy (how long data is kept)


================================================================================
C.1 PRIMARY AI PLATFORM
PURPOSE:
The primary AI platform handles core project work: drafting, analysis, 
problem-solving, and interactive development.

REQUIRED CAPABILITIES:
- Extended conversation context (ability to maintain context across a session)
- File handling (upload, reference, generate)
- Structured output generation
- Conversation export or transcript access

--------------------------------------------------------------------------------
CLAUDE CONFIGURATION
--------------------------------------------------------------------------------

PLATFORM: Claude (claude.ai or Claude app)

SETUP:
1. Create account at claude.ai
2. Configure conversation settings:
   - Enable conversation history (for continuity)
   - Review data retention settings per your security requirements

WORKFLOW INTEGRATION:
- Use Claude Projects for persistent context across sessions on related work
- Upload Project Guide and key documents to Project Knowledge
- Establish naming convention for chats within projects

PROJECT KNOWLEDGE SETUP:
For ongoing projects, upload to Project Knowledge:
- Project Guide (primary reference)
- Current templates being used
- Governing documents (for governance-related work)
- Recent decision logs

This enables Claude to reference these materials without re-uploading each 
session.

SESSION MANAGEMENT:
- Name chats per naming convention: [###] [DEPT]-[PROJ]-[PHASE] ([notes])
- Monitor response quality for signs of context degradation
- Generate bootstrap when approaching context limits
- Export transcripts before closing chats

DATA CONSIDERATIONS:
- Review Anthropic's data usage policies
- Apply data classification framework (Section 15)
- Never share Tier 4 (Restricted) data

--------------------------------------------------------------------------------
ALTERNATIVE PLATFORMS
--------------------------------------------------------------------------------

GPT-4 / ChatGPT:
- Similar capabilities to Claude
- Use Custom GPTs for persistent instructions (analogous to Claude Projects)
- Export via conversation sharing or third-party tools

Gemini:
- Strong integration with Google Workspace
- Use for validation as secondary AI (see Tier 2 QA)
- Different training may catch errors Claude misses


================================================================================
C.2 RETRIEVAL & QUERY TOOLS
PURPOSE:
Retrieval tools enable querying your own documents and knowledge base, 
providing grounded responses based on uploaded source material.

REQUIRED CAPABILITIES:
- Document upload and indexing
- Natural language query interface
- Source citation in responses
- Multiple document handling

--------------------------------------------------------------------------------
NOTEBOOKLM CONFIGURATION
--------------------------------------------------------------------------------

PLATFORM: Google NotebookLM (notebooklm.google.com)

SETUP:
1. Access via Google account at notebooklm.google.com
2. Create notebooks organized by project or domain

NOTEBOOK ORGANIZATION:
Create separate notebooks for:
- Project-specific sources (per major project)
- Governance documents (governing docs, resolutions, SOPs)
- Reference materials (manuals, guides, standards)
- Prompt library and templates

DOCUMENT UPLOAD:
Supported formats: PDF, Google Docs, text files, web pages
- Upload authoritative source documents
- Include version information in document titles
- Refresh documents when updated versions are available

USAGE IN WORKFLOW:
- Verification: Query to confirm AI outputs against source documents
- Re-entry: Query project documents to rebuild context after absence
- Research: Query across multiple sources for comprehensive answers
- Governance check: Verify alignment with established policies

QUERY STRATEGIES:
- Be specific: "What does Section 4.2 say about token management?"
- Request citations: "Quote the relevant passage from the source"
- Cross-reference: "Do any sources contradict this statement?"

LIMITATIONS:
- Responses limited to uploaded content (by design)
- Cannot access real-time information
- Quality depends on quality of uploaded sources


================================================================================
C.3 CHAT EXPORT & ARCHIVAL
PURPOSE:
Chat export tools create permanent records of AI conversations, enabling 
audit trails, re-entry support, and knowledge preservation.

REQUIRED CAPABILITIES:
- Full conversation capture (both user and AI messages)
- Multiple export formats
- Metadata preservation (dates, context)
- Ease of use within workflow

--------------------------------------------------------------------------------
CLAUDE EXPORTER CONFIGURATION
--------------------------------------------------------------------------------

TOOL: Claude Exporter
URL: https://www.claudexporter.com

SETUP:
1. Visit claudexporter.com
2. Install browser extension (if applicable) or use web interface
3. Authorize access to Claude conversations

EXPORT FORMATS:
| Format | Use Case | Notes |
|--------|----------|-------|
| MD     | Primary archive | Preserves structure, readable, searchable |
| TXT    | Universal compatibility | Plain text, maximum portability |
| PDF    | Formal records | Presentation-ready, archival quality |

RECOMMENDED PRACTICE:
- Export at end of every session (per Session Closeout Checklist)
- Primary format: Markdown (MD) for readability and searchability
- Secondary format: PDF for formal project records
- Save to chat folder per file organization conventions

NAMING CONVENTION:
[Platform]-[ChatName].[format]
Example: Claude-001_GOV-AIPM-RD__root_alignment_.md

STORAGE:
- Save to chat folder immediately after export
- Include in NotebookLM notebook for queryable access
- Backup to cloud storage per retention policy

INTEGRATION WITH RE-ENTRY:
Exported transcripts support re-entry by providing:
- Complete conversation record for context reconstruction
- Searchable history for finding past discussions
- Source material for NotebookLM queries


================================================================================
C.4 FILE ORGANIZATION & STORAGE
PURPOSE:
Cloud storage provides backup, sync, and access across devices for all 
project materials.

REQUIRED CAPABILITIES:
- Folder hierarchy support
- File versioning
- Sharing controls
- Reliable sync

--------------------------------------------------------------------------------
GOOGLE DRIVE CONFIGURATION
--------------------------------------------------------------------------------

PLATFORM: Google Drive

FOLDER STRUCTURE:
Align cloud storage structure with your organizational framework:

[Organization Root]/
├── [Department or Practice Area]/
│   ├── [Project Code] [Project Name]/
│   │   ├── 00_Project_Guide/
│   │   ├── 01_Chat_Sessions/
│   │   │   ├── 001_[Chat Name]/
│   │   │   ├── 002_[Chat Name]/
│   │   │   └── ...
│   │   ├── 02_Decision_Logs/
│   │   ├── 03_Deliverables/
│   │   └── 04_Reference/
│   └── ...
└── ...

CHAT SESSION FOLDER CONTENTS:
Each chat folder should contain:
- bootstrap_in.txt (if received)
- bootstrap_out.txt (if generated)
- session_log.txt
- decision_log.txt (if decisions made)
- chat_transcript.md (exported)
- /drafts/ (working files)
- /deliverables/ (completed outputs)

SYNC CONFIGURATION:
- Enable offline access for active project folders
- Configure selective sync if storage is limited
- Verify sync status before closing work sessions

VERSION MANAGEMENT:
- Use meaningful file names with version indicators
- Enable version history for key documents
- Periodically clean up obsolete versions


================================================================================
C.5 CAPTURE & QUICK NOTE TOOLS
PURPOSE:
Capture tools enable quick preservation of thoughts, tasks, and observations 
without interrupting primary workflow.

REQUIRED CAPABILITIES:
- Quick capture (minimal friction)
- Cross-device sync
- Basic organization (labels, categories)
- Search functionality

--------------------------------------------------------------------------------
GOOGLE KEEP CONFIGURATION
--------------------------------------------------------------------------------

PLATFORM: Google Keep

SETUP:
1. Access via keep.google.com or mobile app
2. Create labels for common capture categories

RECOMMENDED LABELS:
- AI-Insights: Observations about AI behavior or process
- Tasks: Action items to process later
- Prompts: Prompt ideas or refinements to add to library
- Questions: Items requiring research or clarification
- Process-Notes: Workflow improvement ideas

CAPTURE WORKFLOW:
During AI sessions, use capture tool to note:
- Successful prompts worth adding to library
- Unexpected AI behaviors (positive or negative)
- Process improvement ideas
- Questions to research later
- Tasks generated during session

PROCESSING CADENCE:
- Review capture notes during Weekly Review (TPL-006)
- Transfer prompts to Prompt Library
- Convert tasks to project action items
- Archive or delete processed notes


================================================================================
C.6 CALENDAR & SCHEDULING
PURPOSE:
Calendar tools enforce cadences (weekly reviews, project checkpoints) and 
manage time-based commitments.

REQUIRED CAPABILITIES:
- Recurring event support
- Reminder/notification system
- Integration with workflow

--------------------------------------------------------------------------------
CALENDAR CONFIGURATION
--------------------------------------------------------------------------------

PLATFORM: Google Calendar (or equivalent)

RECURRING EVENTS TO ESTABLISH:
| Event | Frequency | Purpose | Duration |
|-------|-----------|---------|----------|
| Weekly Review | Weekly | TPL-006 execution | 30-60 min |
| Project Checkpoint | Per project | Health check | 15-30 min |
| Prompt Library Review | Quarterly | Maintenance | 30 min |
| Process Retrospective | Monthly | Improvement | 45 min |

NOTIFICATION STRATEGY:
- Set reminders for recurring reviews
- Use calendar blocks for focused AI work sessions
- Schedule buffer time between intensive AI sessions

INTEGRATION:
- Link calendar events to relevant project folders
- Include document links in event descriptions
- Track review completion in session logs


================================================================================
C.7 TOOL INTEGRATION SUMMARY
--------------------------------------------------------------------------------
RECOMMENDED TOOL STACK
--------------------------------------------------------------------------------

| Function | Tool Category | Example | Integration Point |
|----------|---------------|---------|-------------------|
| Primary AI | Conversational AI | Claude | Core work execution |
| Validation AI | Secondary AI | GPT-4, Gemini | Tier 2 QA |
| Retrieval | RAG Tool | NotebookLM | Verification, re-entry |
| Export | Chat Archive | Claude Exporter | Session closeout |
| Storage | Cloud Drive | Google Drive | File organization |
| Capture | Quick Notes | Google Keep | Insight preservation |
| Scheduling | Calendar | Google Calendar | Cadence enforcement |

--------------------------------------------------------------------------------
DATA FLOW DIAGRAM
--------------------------------------------------------------------------------

                    ┌─────────────────┐
                    │   Primary AI    │
                    │    (Claude)     │
                    └────────┬────────┘
                             │
              ┌──────────────┼──────────────┐
              │              │              │
              ▼              ▼              ▼
      ┌───────────┐  ┌───────────┐  ┌───────────┐
      │  Export   │  │Validation │  │  Capture  │
      │  (Claude  │  │   AI      │  │  (Keep)   │
      │ Exporter) │  │(GPT/Gemini│  │           │
      └─────┬─────┘  └─────┬─────┘  └─────┬─────┘
            │              │              │
            ▼              │              │
      ┌───────────┐        │              │
      │  Storage  │◄───────┴──────────────┘
      │  (Drive)  │
      └─────┬─────┘
            │
            ▼
      ┌───────────┐
      │ Retrieval │
      │(NotebookLM│
      └───────────┘

--------------------------------------------------------------------------------
CONFIGURATION CHECKLIST
--------------------------------------------------------------------------------

Initial Setup:
- [ ] Primary AI platform account created and configured
- [ ] Claude Projects set up for ongoing work
- [ ] NotebookLM notebooks created for key domains
- [ ] Claude Exporter installed and tested
- [ ] Cloud storage folder structure established
- [ ] Capture tool configured with relevant labels
- [ ] Calendar recurring events created

Per-Project Setup:
- [ ] Project folder created in cloud storage
- [ ] Chat folder structure established
- [ ] Project documents uploaded to NotebookLM
- [ ] Project Guide uploaded to Claude Project Knowledge
- [ ] Calendar events for project checkpoints scheduled

================================================================================
END OF APPENDIX C

================================================================================
APPENDIX D: TROUBLESHOOTING GUIDE
This appendix provides problem-to-solution guidance for common issues 
encountered in AI-assisted work. Use this guide when something isn't working 
as expected before escalating to more intensive recovery protocols.

Organization: Problems are grouped by category. Each entry includes symptoms, 
likely causes, and recommended solutions. Cross-references to relevant manual 
sections and templates are provided for deeper guidance.

--------------------------------------------------------------------------------
HOW TO USE THIS GUIDE
--------------------------------------------------------------------------------

1. Identify the category that best matches your issue
2. Find the specific problem description
3. Verify symptoms match your situation
4. Apply the recommended solution
5. If unresolved, escalate per the guidance provided

Severity Indicators:
- QUICK FIX: Resolvable in minutes with simple action
- MODERATE: Requires deliberate effort, possibly 15-30 minutes
- INVOLVED: May require significant time or new session
- ESCALATE: Use formal recovery protocol (TPL-009)


================================================================================
D.1 SESSION MANAGEMENT ISSUES
--------------------------------------------------------------------------------
PROBLEM: AI seems to have forgotten earlier context
--------------------------------------------------------------------------------

SYMPTOMS:
- AI asks questions already answered
- AI contradicts earlier statements
- Responses become generic or less specific
- AI doesn't reference established decisions

LIKELY CAUSES:
- Approaching token limits
- Context window overflow
- Long session without reinforcement

SOLUTIONS:

[QUICK FIX] Restate key context:
"To confirm our parameters: [restate 2-3 key points]. Please acknowledge 
and continue with this context in mind."

[MODERATE] Provide context summary:
"Here's a summary of our established context: [bulleted summary]. 
Please confirm understanding before we proceed."

[INVOLVED] Bootstrap and restart:
If context loss is severe, generate a bootstrap (TPL-002) capturing current 
state and continue in a fresh chat.

CROSS-REFERENCE: Section 6 (Session Lifecycle), TPL-002 (Bootstrap)

--------------------------------------------------------------------------------
PROBLEM: AI responses are becoming slow or degraded
--------------------------------------------------------------------------------

SYMPTOMS:
- Longer response times
- Incomplete responses
- Responses cut off mid-sentence
- Declining response quality

LIKELY CAUSES:
- Token limit approaching
- Complex context accumulation
- Platform performance issues

SOLUTIONS:

[QUICK FIX] Check token status:
Ask: "Approximately how much of your context window have we used?"
(Note: AI estimates may be rough but directionally useful)

[MODERATE] Reduce active context:
"Let's focus narrowly on [specific task]. You don't need to reference 
[earlier topics] for this."

[INVOLVED] Emergency bootstrap:
Generate bootstrap immediately, save all files, continue in new chat.
See Section 14.4 (Token Collapse) for detailed protocol.

CROSS-REFERENCE: Section 6.4 (Token Management), Section 14.4 (Token Collapse)

--------------------------------------------------------------------------------
PROBLEM: Session ended unexpectedly without closeout
--------------------------------------------------------------------------------

SYMPTOMS:
- Chat closed without generating artifacts
- No bootstrap created
- No session log
- Work potentially lost

LIKELY CAUSES:
- Token exhaustion
- Platform timeout
- User error (closed tab)
- Browser/app crash

SOLUTIONS:

[MODERATE] Recover from chat history:
1. Access chat history/transcript if available
2. Export using Claude Exporter if chat is still accessible
3. Manually reconstruct key outputs from transcript

[INVOLVED] Reconstruct from memory:
1. Document what you remember immediately
2. List decisions made (mark as [RECONSTRUCTED])
3. Note uncertainties explicitly
4. Create reconstruction log for audit trail

[ESCALATE] If significant work lost:
Use TPL-009 (Failure Recovery Worksheet), Protocol D (Token Collapse)

CROSS-REFERENCE: Section 14.4 (Token Collapse), TPL-009

--------------------------------------------------------------------------------
PROBLEM: Bootstrap didn't transfer context effectively
--------------------------------------------------------------------------------

SYMPTOMS:
- New session AI seems confused
- AI asks questions answered in bootstrap
- Work direction doesn't match expectations
- AI makes different assumptions than previous session

LIKELY CAUSES:
- Bootstrap too compressed
- Key context omitted
- Ambiguous language in bootstrap
- AI interpretation differs

SOLUTIONS:

[QUICK FIX] Clarify specific points:
"The bootstrap mentioned [X]. To clarify: [explicit clarification]. 
Please confirm understanding."

[MODERATE] Supplement bootstrap:
Provide additional context files referenced in bootstrap.
Ask: "What questions do you have about the bootstrap before we proceed?"

[INVOLVED] Conduct alignment check:
"Based on the bootstrap, please summarize: (1) project objective, 
(2) current state, (3) immediate next actions, (4) key constraints."
Correct any misalignments before proceeding.

PREVENTION: Use "Questions to Address at Session Start" section in bootstrap.

CROSS-REFERENCE: Section 6.3 (Bootstrap Protocol), TPL-002


================================================================================
D.2 ALIGNMENT ISSUES
--------------------------------------------------------------------------------
PROBLEM: AI keeps going in the wrong direction
--------------------------------------------------------------------------------

SYMPTOMS:
- Outputs don't match expectations
- Repeated corrections don't stick
- AI seems to understand but produces wrong results
- Feeling of talking past each other

LIKELY CAUSES:
- Fundamental misalignment on objective
- Ambiguous instructions
- AI making unstated assumptions
- Scope confusion

SOLUTIONS:

[QUICK FIX] Reframe the request:
"Let me reframe this differently: [new framing]. Does this change your 
understanding of what I'm asking for?"

[MODERATE] Surface assumptions:
"Before your next attempt, list the assumptions you're making about: 
(1) what I want, (2) the format, (3) the constraints."

[INVOLVED] Use QA-001 (Cognitive Divergence Audit):
If Three-Strike threshold reached (3 failed corrections), deploy the 
formal divergence audit prompt from Appendix B.

[ESCALATE] If divergence persists:
Use TPL-009 (Failure Recovery Worksheet), Protocol G (Cognitive Divergence)

CROSS-REFERENCE: Section 14.7 (Cognitive Divergence), QA-001 prompt

--------------------------------------------------------------------------------
PROBLEM: AI is being too agreeable / not pushing back
--------------------------------------------------------------------------------

SYMPTOMS:
- AI agrees with everything without questioning
- No critical feedback on ideas
- Outputs accept flawed premises
- Feeling of "yes-man" responses

LIKELY CAUSES:
- Sycophantic training patterns
- User framing discourages pushback
- AI optimizing for approval

SOLUTIONS:

[QUICK FIX] Explicitly request criticism:
"I need you to be critical, not agreeable. What's wrong with this approach? 
What am I missing? Push back on weak points."

[MODERATE] Reframe role:
"Act as a skeptical reviewer. Your job is to find problems, not validate. 
What would a critic say about this?"

[MODERATE] Use adversarial prompting:
"Argue against this position. What's the strongest case that this is wrong?"

PREVENTION: Include "challenge assumptions" as standing instruction in 
Project Guide special instructions.

CROSS-REFERENCE: Section 13.3 (Over-Reliance Risks), Section 11.2 (QA Tiers)

--------------------------------------------------------------------------------
PROBLEM: AI outputs are inconsistent with prior work
--------------------------------------------------------------------------------

SYMPTOMS:
- New outputs contradict earlier outputs
- Terminology changes without reason
- Style/tone inconsistent
- Decisions seem to be re-made differently

LIKELY CAUSES:
- Context not carried forward
- Decision log not referenced
- Multiple sessions without continuity
- AI doesn't have access to prior work

SOLUTIONS:

[QUICK FIX] Provide prior work as reference:
"Here is the previous output: [paste or attach]. Ensure consistency with 
this in your new output."

[MODERATE] Reference decision log:
"Per Decision #[X], we established [decision]. Ensure your output aligns 
with this."

[INVOLVED] Conduct consistency audit:
Upload both outputs to a validation AI. Ask: "Compare these two outputs. 
Identify inconsistencies in terminology, approach, or conclusions."

PREVENTION: Maintain decision log, reference it in bootstraps, provide 
prior deliverables to new sessions.

CROSS-REFERENCE: Section 5.2 (Decision Logs), TPL-004


================================================================================
D.3 OUTPUT QUALITY ISSUES
--------------------------------------------------------------------------------
PROBLEM: AI output contains obvious errors or hallucinations
--------------------------------------------------------------------------------

SYMPTOMS:
- Factually incorrect statements
- Made-up references or citations
- Confident claims that feel wrong
- Internal contradictions in output

LIKELY CAUSES:
- AI hallucination (generating plausible but false content)
- Training data errors
- Extrapolation beyond knowledge
- No verification performed

SOLUTIONS:

[QUICK FIX] Direct correction:
"This statement is incorrect: [quote]. The correct information is [X]. 
Please revise."

[MODERATE] Request source verification:
"For each factual claim in this output, indicate your confidence level 
and whether I should verify independently."

[INVOLVED] Cross-AI validation:
Submit output to different AI platform using VERIF-001 prompt for 
independent review.

[ESCALATE] If pattern persists:
Use TPL-009, Protocol C (Unchecked AI Assumptions)

PREVENTION: Apply Tier 2 validation for important deliverables. 
Maintain skepticism on factual claims.

CROSS-REFERENCE: Section 13.1 (Hallucination Risks), Section 11.2 (QA Tiers)

--------------------------------------------------------------------------------
PROBLEM: Output is too generic / not specific enough
--------------------------------------------------------------------------------

SYMPTOMS:
- Boilerplate language
- Lacks project-specific detail
- Could apply to anything
- Missing concrete examples

LIKELY CAUSES:
- Insufficient context provided
- Instructions too vague
- AI defaulting to safe/general responses
- No examples to anchor on

SOLUTIONS:

[QUICK FIX] Request specificity:
"This is too generic. Revise to include: [specific elements]. 
Reference [specific context]."

[MODERATE] Provide examples:
"Here's an example of the specificity I need: [example]. 
Match this level of detail."

[MODERATE] Add constraints:
"Include at least [N] specific examples. Reference [specific documents]. 
Use [specific terminology]."

CROSS-REFERENCE: Section 5 (Alignment Principles)

--------------------------------------------------------------------------------
PROBLEM: Output is too long / verbose
--------------------------------------------------------------------------------

SYMPTOMS:
- Excessive length
- Repetitive content
- Padding with unnecessary detail
- Buries key points in verbiage

LIKELY CAUSES:
- No length constraint specified
- AI defaulting to comprehensive
- Mistaking thoroughness for quality

SOLUTIONS:

[QUICK FIX] Request compression:
"Reduce this by 50%. Keep only essential content. Remove redundancy."

[MODERATE] Specify constraints upfront:
"Maximum [N] words/paragraphs. Prioritize [key elements]. 
Omit [unnecessary elements]."

[MODERATE] Request executive summary:
"Provide a 3-sentence summary first, then the full content. 
I may only need the summary."

CROSS-REFERENCE: Section 5.3 (Clear Communication)

--------------------------------------------------------------------------------
PROBLEM: Output doesn't match requested format
--------------------------------------------------------------------------------

SYMPTOMS:
- Wrong structure
- Missing required sections
- Format inconsistent with examples
- Ignores template specifications

LIKELY CAUSES:
- Format instructions unclear
- Template not provided
- AI substituted own judgment
- Instructions buried in long prompt

SOLUTIONS:

[QUICK FIX] Restate format requirement:
"The format must be: [explicit structure]. Revise to match exactly."

[MODERATE] Provide template:
"Use this exact template: [template]. Fill in each section as specified."

[MODERATE] Break into steps:
"First, create the outline matching [structure]. Show me the outline. 
Then I'll confirm before you draft content."

CROSS-REFERENCE: Section 5.2 (Constraint Communication)


================================================================================
D.4 PROCESS ISSUES
--------------------------------------------------------------------------------
PROBLEM: Decisions keep getting re-debated
--------------------------------------------------------------------------------

SYMPTOMS:
- Same issues discussed repeatedly
- Contradictory decisions in different sessions
- Feeling of "didn't we already decide this?"
- Progress stalls on settled issues

LIKELY CAUSES:
- No decision log maintained
- Decision log not referenced
- Decisions not communicated to new sessions
- Rationale not documented

SOLUTIONS:

[MODERATE] Establish decision as settled:
"This was decided in Decision #[X]: [decision]. The rationale was [Y]. 
This is not open for re-debate. Proceed accordingly."

[INVOLVED] Reconstruct decision history:
Review chat transcripts to identify decisions made. Document with 
[RECONSTRUCTED] tag. Create decision log going forward.

[ESCALATE] If pattern systematic:
Use TPL-009, Protocol A (No Logs → Decision Churn)

PREVENTION: Maintain decision log from project start. Include key decisions 
in every bootstrap. Reference decision numbers when relevant.

CROSS-REFERENCE: Section 5.2 (Decision Logs), Section 14.1 (Decision Churn)

--------------------------------------------------------------------------------
PROBLEM: Actual practice has drifted from documented process
--------------------------------------------------------------------------------

SYMPTOMS:
- Doing things differently than SOP specifies
- Can't remember last time you followed the process
- Documentation feels outdated
- New habits developed without documentation

LIKELY CAUSES:
- Process evolved through practice
- Documentation not updated
- Workarounds became standard
- Process was impractical as designed

SOLUTIONS:

[MODERATE] Conduct SOP audit:
Compare documented procedure to actual practice. List specific divergences.

[INVOLVED] Determine resolution:
For each divergence, decide:
- Update SOP to match reality (practice is better)
- Retrain to match SOP (procedure is better)  
- Hybrid approach

[INVOLVED] Update documentation:
Document the resolution. Update SOP. Communicate changes.

[ESCALATE] If drift is significant:
Use TPL-009, Protocol B (SOP Drift)

PREVENTION: Include process check in weekly review (TPL-006). 
Update documentation when practices change.

CROSS-REFERENCE: Section 14.2 (SOP Drift), TPL-006 (Weekly Review)

--------------------------------------------------------------------------------
PROBLEM: Returning to project after absence - feeling lost
--------------------------------------------------------------------------------

SYMPTOMS:
- Can't remember where you left off
- Context feels stale
- Unsure what's been decided
- Risk of contradicting prior work

LIKELY CAUSES:
- Extended time away from project
- Context decay (normal human memory)
- No re-entry protocol followed

SOLUTIONS:

[MODERATE] Abbreviated re-entry (1-2 weeks away):
1. Review last session log
2. Review last bootstrap
3. Scan recent decisions
4. Quick reorientation prompt to AI
5. Resume

[INVOLVED] Full re-entry (2+ weeks away):
Use TPL-007 (Re-Entry Checklist) completely.
Conduct dedicated reorientation chat before resuming work.

[ESCALATE] If already produced contradictory work:
Use TPL-009, Protocol F (No Re-Entry → Regression)

PREVENTION: Always create bootstrap at session end. 
Follow re-entry protocol after any significant absence.

CROSS-REFERENCE: Section 9 (Re-Entry Protocol), TPL-007


================================================================================
D.5 TOOL ISSUES
--------------------------------------------------------------------------------
PROBLEM: Claude Exporter not capturing chat properly
--------------------------------------------------------------------------------

SYMPTOMS:
- Export is incomplete
- Formatting is broken
- Images/files not included
- Export fails entirely

SOLUTIONS:

[QUICK FIX] Try different format:
If MD fails, try TXT. If TXT fails, try PDF.

[MODERATE] Manual backup:
Select all chat content manually, paste into document.
Less elegant but preserves content.

[MODERATE] Check extension status:
Verify Claude Exporter is enabled and updated.
Try disabling/re-enabling extension.

[INVOLVED] Export in sections:
For very long chats, export may fail. Try scrolling to load all content 
first, or accept partial export and note gaps.

CROSS-REFERENCE: Appendix C.3 (Chat Archive Configuration)

--------------------------------------------------------------------------------
PROBLEM: NotebookLM not finding relevant content
--------------------------------------------------------------------------------

SYMPTOMS:
- Queries return irrelevant results
- Known content not surfaced
- "I couldn't find information about that" responses

SOLUTIONS:

[QUICK FIX] Rephrase query:
Try different terminology. Use exact phrases from documents.

[MODERATE] Check document upload:
Verify all relevant documents are uploaded to the notebook.
Check that documents processed successfully.

[MODERATE] Query more specifically:
Instead of broad queries, ask about specific sections or documents.
"What does the Project Guide say about [X]?"

[INVOLVED] Rebuild notebook:
If persistent issues, create fresh notebook and re-upload documents.

CROSS-REFERENCE: Appendix C.2 (NotebookLM Configuration)

--------------------------------------------------------------------------------
PROBLEM: Files not syncing or organizing properly
--------------------------------------------------------------------------------

SYMPTOMS:
- Files missing from expected locations
- Sync conflicts
- Duplicate files
- Folder structure inconsistent

SOLUTIONS:

[QUICK FIX] Manual verification:
Check cloud storage directly (web interface) to verify actual state.

[MODERATE] Force sync:
Trigger manual sync. Check sync client status.
Resolve any conflicts by choosing correct version.

[MODERATE] Clean up duplicates:
Identify authoritative version. Remove or archive duplicates.
Document resolution in session log.

[INVOLVED] Audit folder structure:
Compare actual structure to documented convention.
Reorganize if necessary. Update any broken references.

CROSS-REFERENCE: Appendix C.4 (File Organization)


================================================================================
D.6 ESCALATION GUIDE
When troubleshooting steps don't resolve the issue, escalate using the 
appropriate protocol.

--------------------------------------------------------------------------------
WHEN TO ESCALATE
--------------------------------------------------------------------------------

ESCALATE IMMEDIATELY if:
- Work quality is actively degrading
- Decisions are being made on faulty basis
- Significant rework may be required
- Pattern is systematic, not isolated

ESCALATE AFTER TROUBLESHOOTING if:
- Quick fixes and moderate solutions haven't worked
- Issue has recurred multiple times
- Root cause is unclear
- Process gap may exist

--------------------------------------------------------------------------------
ESCALATION PATHS
--------------------------------------------------------------------------------

| Issue Type | Escalation Protocol | Template |
|------------|---------------------|----------|
| Decision churn | Protocol A | TPL-009 |
| SOP drift | Protocol B | TPL-009 |
| AI assumptions | Protocol C | TPL-009 |
| Token collapse | Protocol D | TPL-009 |
| Cognitive fatigue | Protocol E | TPL-009 |
| Post-absence regression | Protocol F | TPL-009 |
| Cognitive divergence | Protocol G | TPL-009 |

--------------------------------------------------------------------------------
POST-ESCALATION
--------------------------------------------------------------------------------

After using TPL-009 (Failure Recovery Worksheet):

1. Complete the full worksheet including lessons learned
2. Identify prevention measures
3. Update documentation if process gaps identified
4. Note in weekly review (TPL-006) for pattern tracking
5. Consider whether manual updates needed

CROSS-REFERENCE: Section 14 (Failure Modes), TPL-009


================================================================================
D.7 QUICK REFERENCE: FIRST ACTIONS
| Problem Category | First Action |
|------------------|--------------|
| AI forgot context | Restate key parameters |
| Session degrading | Check token status |
| Wrong direction | Reframe the request |
| Too agreeable | Explicitly request criticism |
| Obvious errors | Direct correction + verification request |
| Too generic | Request specifics with examples |
| Decisions re-debated | Reference decision log |
| Lost after absence | Follow re-entry checklist |
| Tool not working | Try alternative approach |

UNIVERSAL PRINCIPLE:
PAUSE → DIAGNOSE → APPLY FIX → VERIFY → ESCALATE IF NEEDED

================================================================================
END OF APPENDIX D

================================================================================
APPENDIX E: VERSION HISTORY
This appendix tracks the revision history of the AI Operations Manual and 
establishes procedures for document versioning and amendments.

--------------------------------------------------------------------------------
E.1 VERSION NUMBERING CONVENTION
--------------------------------------------------------------------------------

VERSION FORMAT: [Major].[Minor].[Patch]

MAJOR VERSION (X.0.0):
- Significant structural changes
- New sections or parts added
- Fundamental approach changes
- Breaking changes to templates or processes

MINOR VERSION (0.X.0):
- New content within existing structure
- Template updates
- Process refinements
- Non-breaking additions

PATCH VERSION (0.0.X):
- Corrections and clarifications
- Typo fixes
- Minor wording improvements
- Cross-reference updates

EXAMPLES:
- 1.0.0 → Initial release
- 1.1.0 → Added new section on project tracking
- 1.1.1 → Fixed cross-reference errors
- 2.0.0 → Major restructure of Part III

--------------------------------------------------------------------------------
E.2 CHANGE LOG FORMAT
--------------------------------------------------------------------------------

Each version entry should include:

| Field | Description |
|-------|-------------|
| Version | Version number |
| Date | Release date |
| Author | Who made the changes |
| Type | Major / Minor / Patch |
| Summary | Brief description of changes |
| Sections Affected | List of modified sections |
| Migration Notes | Any actions needed for users of prior version |

--------------------------------------------------------------------------------
E.3 AMENDMENT PROCEDURES
--------------------------------------------------------------------------------

PROPOSING CHANGES:

1. Document proposed change with rationale
2. Identify sections affected
3. Assess version impact (major/minor/patch)
4. Draft revised content
5. Review against existing content for consistency

REVIEW PROCESS:

- Patch changes: Self-review, implement directly
- Minor changes: Review for unintended impacts, test templates
- Major changes: Full review of affected sections, consider user impact

IMPLEMENTATION:

1. Update affected sections
2. Update all cross-references
3. Update version number in document header
4. Add entry to Version History (this appendix)
5. Update any external references (Project Guides, etc.)

POST-IMPLEMENTATION:

- Notify stakeholders of changes (if applicable)
- Update NotebookLM/retrieval systems with new version
- Archive prior version (do not delete)

--------------------------------------------------------------------------------
E.4 DOCUMENT REVISION HISTORY
--------------------------------------------------------------------------------

================================================================================
VERSION: 1.0.0
DATE:               December 2025
AUTHOR:             [Organization]
TYPE:               Major (Initial Release)

SUMMARY:
Initial release of the AI Operations Manual documenting a comprehensive 
system for AI-assisted business operations, governance, and project management.

CONTENTS:

Part I: Foundation
- Section 1: Introduction
- Section 2: Core Concepts
- Section 3: Getting Started

Part II: Project & Session Management
- Section 4: Project Lifecycle
- Section 5: Alignment Principles
- Section 6: Session Lifecycle
- Section 7: Decision Logging

Part III: Verification & Quality
- Section 8: Verification Mindset
- Section 9: Re-Entry Protocol
- Section 10: Prompt Engineering
- Section 11: Quality Assurance

Part IV: Tools & Integration
- Section 12: Tool Ecosystem
- Section 13: AI-Specific Risks

Part V: Failure Modes & Recovery
- Section 14: Failure Mode Catalog
- Section 15: Recovery Protocols

Part VI: Mail & External Communications
- Section 16: AI-Assisted Email
- Section 17: External Document Review

Appendices:
- Appendix A: Template Reference (9 templates)
- Appendix B: Prompt Reference (Golden Prompt Library)
- Appendix C: Tool Configuration Guide
- Appendix D: Troubleshooting Guide
- Appendix E: Version History

TEMPLATES INCLUDED:
- TPL-001: Project Initiation Form
- TPL-002: Bootstrap Template
- TPL-003: Session Closeout Checklist
- TPL-004: Decision Log Entry
- TPL-005: Session Log
- TPL-006: Weekly Review Agenda
- TPL-007: Re-Entry Checklist
- TPL-008: Cross-AI Validation Protocol
- TPL-009: Failure Recovery Worksheet

MIGRATION NOTES:
Not applicable (initial release).

--------------------------------------------------------------------------------

================================================================================
VERSION: 1.0.3
DATE:               December 26, 2025
AUTHOR:             [Organization]
TYPE:               Minor (Content Addition)

SUMMARY:
Addition of Dialog Intake System content completing Issue #16 from Phase 6a.

CHANGES:

Section 3 Restructured:
- NEW Section 3.5: The Dialog Intake System (6 subsections)
  - 3.5.1 Purpose and Philosophy
  - 3.5.2 Entry Point Triage
  - 3.5.3 Confirmation Calibration Framework
  - 3.5.4 Context Freshness Protocol
  - 3.5.5 Touchpoint Reference
  - 3.5.6 Failure Mode Catalog Reference
- Section 3.5 (Cost-Benefit Reality) renumbered to 3.6
- Section 3.6 (Common Failure Modes) renumbered to 3.7

Appendix Additions:
- NEW Appendix F: Dialog Intake Failure Mode Catalog
  - 40+ failure modes organized by touchpoint category
  - Detection signals, severity, recovery paths for each

Template Additions:
- NEW TPL-011: Dialog Intake Quick Reference
  - Condensed reference for rapid AI lookup
  - Entry point triage tree, confirmation tiers, top failure modes
- Dialog Completion Option annotation added to TPL-001 through TPL-010

MIGRATION NOTES:
- Section 3.5/3.6 references in external documents should be updated
- Projects using old section numbers need cross-reference updates

--------------------------------------------------------------------------------

================================================================================
VERSION: 1.0.6
DATE:               December 28, 2025
AUTHOR:             Session 029
TYPE:               Patch (Cross-reference updates)

SUMMARY:
Phase 6b review identified and corrected missing cross-references to newly 
added content (Backlog Register, Appendix G).

CHANGES:

Section 1.4.4 (For Reference):
- Added reference to Appendix G: Artifact Taxonomy Reference

Section 2.7.2 (Scope Monitoring Duty):
- Added guidance to capture scope-expanding items in Backlog Register

Section 4.3.3 (Session Closeout):
- Added step 6: Review Backlog Register for items to carry forward
- Renumbered export step to 7

Section 4.3.5.3 (AI Responsibilities at Phase Boundaries):
- Added Backlog Register review at phase transitions

MIGRATION NOTES:
- No breaking changes
- Enhanced cross-referencing improves document navigability

--------------------------------------------------------------------------------

[FUTURE ENTRIES WILL BE APPENDED BELOW]

--------------------------------------------------------------------------------


================================================================================
E.5 TEMPLATE VERSION TRACKING
Templates are versioned independently of the manual. Track template versions 
here for reference.

| Template | Current Version | Last Updated | Changes |
|----------|-----------------|--------------|---------|
| TPL-001  | 1.0             | Dec 2025     | Initial release |
| TPL-002  | 1.0             | Dec 2025     | Initial release |
| TPL-003  | 1.0             | Dec 2025     | Initial release |
| TPL-004  | 1.0             | Dec 2025     | Initial release |
| TPL-005  | 1.0             | Dec 2025     | Initial release |
| TPL-006  | 1.0             | Dec 2025     | Initial release |
| TPL-007  | 1.0             | Dec 2025     | Initial release |
| TPL-008  | 1.0             | Dec 2025     | Initial release |
| TPL-009  | 1.0             | Dec 2025     | Initial release |
| TPL-010  | 1.0             | Dec 2025     | Initial release |
| TPL-011  | 1.0             | Dec 2025     | Initial release |

--------------------------------------------------------------------------------
TEMPLATE VERSIONING NOTES
--------------------------------------------------------------------------------

When updating templates:
1. Increment template version number in template header
2. Update this tracking table
3. Note changes in manual Version History if significant
4. Consider backward compatibility (can existing projects continue?)
5. Update Appendix A with revised template

--------------------------------------------------------------------------------
E.6 PROMPT LIBRARY VERSION TRACKING
--------------------------------------------------------------------------------

The Golden Prompt Library (Appendix B) is versioned separately.

| Library Version | Date | Prompts Added | Prompts Updated | Prompts Archived |
|-----------------|------|---------------|-----------------|------------------|
| 1.0 | Dec 2025 | ALIGN-001, RECOV-001, QA-001, VERIF-001 | — | — |

--------------------------------------------------------------------------------
PROMPT LIBRARY MAINTENANCE SCHEDULE
--------------------------------------------------------------------------------

- Quarterly: Review all prompts for continued relevance
- As needed: Add new high-performing prompts
- As needed: Archive deprecated prompts (do not delete)
- As needed: Version prompts that undergo significant refinement


================================================================================
E.7 RELATED DOCUMENT VERSIONS
The AI Operations Manual may reference or be referenced by other documents. 
Track key related documents here.

| Document | Relationship | Current Version | Compatibility |
|----------|--------------|-----------------|---------------|
| [Project-specific Project Guides] | References manual | Varies | Verify on use |
| [Organization SOP Manual] | Parent governance | Varies | Verify on use |

--------------------------------------------------------------------------------
COMPATIBILITY NOTES
--------------------------------------------------------------------------------

When the AI Operations Manual is updated:
1. Check Project Guides for outdated references
2. Verify SOP alignment if governance sections changed
3. Update NotebookLM sources
4. Communicate changes to active projects


================================================================================
END OF APPENDIX E

================================================================================
APPENDIX F: DIALOG INTAKE FAILURE MODE CATALOG
This appendix catalogs failure modes for the Dialog Intake System documented 
in Section 3.5. Each failure mode includes detection signals, severity 
classification, and recovery paths.

For orientation to this catalog, see Section 3.5.6.

================================================================================
F.1 ENTRY POINT FAILURES
Failures occurring when users initiate contact and the AI must determine 
their needs.

--------------------------------------------------------------------------------
F.1.1 TOUCHPOINT 1.1: First Contact / System Initiation
--------------------------------------------------------------------------------

FAILURE: VAGUE PROJECT OBJECTIVE
Severity: HIGH
Detection Signals:
- Circular answers to "what are you trying to accomplish?"
- "I'll know it when I see it" responses
- Inability to state a deliverable
- Objectives that change with each restatement
Recovery Path:
1. Route to Exploration Session (Section 1.4.5)
2. Use readiness assessment questions
3. Help user clarify thinking before formalizing as project
4. Do not create Project Guide until objective is articulable
Prevention: Never skip readiness assessment for new projects

FAILURE: SCOPE CONFUSION (ONE PROJECT IS ACTUALLY MULTIPLE)
Severity: HIGH
Detection Signals:
- Objectives that don't connect logically
- Sprawling scope that keeps expanding
- Multiple distinct deliverables for different purposes
Recovery Path:
1. Pause and acknowledge: "This sounds like it might be several things..."
2. Help user decompose into distinct projects
3. Establish relationships between projects if linked
4. Create separate Project Guides for each
Prevention: Watch for scope breadth early; question broad objectives

FAILURE: PREMATURE INITIATION
Severity: MEDIUM
Detection Signals:
- Frequent "I don't know" to readiness questions
- Can't answer what success looks like
- Dependencies on information user doesn't have
Recovery Path:
1. Redirect to Exploration Session
2. Identify what needs to happen before project can start
3. Offer to help gather prerequisites
4. Document as "pre-project" work
Prevention: Complete readiness assessment before creating Project Guide

FAILURE: MISIDENTIFIED NEED (SAYS NEW BUT ACTUALLY CONTINUING)
Severity: MEDIUM
Detection Signals:
- References to prior work in "new project" framing
- "We were working on..." language
- Asks for continuation of something AI doesn't recognize
Recovery Path:
1. Probe for prior context: "Have you worked on this before?"
2. Request any prior materials (bootstrap, notes, outputs)
3. Route to Re-Entry (1.3) if prior work exists
Prevention: Always check for continuation signals in initiation requests

FAILURE: FRAMEWORK REJECTION
Severity: MEDIUM
Detection Signals:
- Impatience with intake questions
- "Can't you just..." requests to skip process
- Resistance to documentation
Recovery Path:
1. Acknowledge preference for speed
2. Explain value briefly (not defensively)
3. Offer lightweight entry: "Three quick questions, then we start"
4. If resistance persists, note in session log and proceed minimally
Prevention: Frame intake as enabling speed, not creating overhead

--------------------------------------------------------------------------------
F.1.2 TOUCHPOINT 1.2: Session Continuation
--------------------------------------------------------------------------------

FAILURE: BOOTSTRAP NOT PROVIDED
Severity: HIGH
Detection Signals:
- User says "continue" but provides no context file
- Assumes AI remembers prior sessions
- References decisions/work AI has no record of
Recovery Path:
1. Request bootstrap explicitly
2. If unavailable, attempt reconstruction: "What do you remember?"
3. Route to abbreviated Re-Entry if gap is significant
4. Document bootstrap absence in session log
Prevention: Reinforce bootstrap requirement at every session closeout

FAILURE: BOOTSTRAP CONTEXT MISMATCH
Severity: MEDIUM
Detection Signals:
- Bootstrap state doesn't match user's stated current state
- User describes work not in bootstrap
- Confusion about "where we were"
Recovery Path:
1. Name the mismatch explicitly
2. Determine which is accurate (bootstrap or user statement)
3. Update context accordingly
4. Note discrepancy in session log
Prevention: Verify bootstrap at session start; don't assume accuracy

--------------------------------------------------------------------------------
F.1.3 TOUCHPOINT 1.3: Re-Entry After Extended Absence
--------------------------------------------------------------------------------

FAILURE: UNDERESTIMATED CONTEXT LOSS
Severity: HIGH
Detection Signals:
- User believes they remember more than they do
- Quick answers to re-entry questions that prove incomplete
- Confusion surfacing mid-session about prior decisions
Recovery Path:
1. When confusion surfaces, pause work
2. Conduct fuller re-entry assessment
3. Review prior artifacts together
4. Rebuild context before resuming
Prevention: Calibrate re-entry depth to absence duration; err on side of more

FAILURE: MISSING PRIOR ARTIFACTS
Severity: HIGH
Detection Signals:
- User can't locate bootstrap, Project Guide, or Decision Log
- References to files that aren't available
- Incomplete project record
Recovery Path:
1. Inventory what IS available
2. Reconstruct critical context from available sources
3. Note gaps explicitly
4. Consider reconstruction session before resuming work
Prevention: Reinforce external organization system; user actions checklist

--------------------------------------------------------------------------------
F.1.4 TOUCHPOINT 1.4: Phase Transition
--------------------------------------------------------------------------------

FAILURE: PREMATURE PHASE TRANSITION
Severity: HIGH
Detection Signals:
- Phase completion criteria not verified
- Open items from current phase unaddressed
- Rush to "move on" without closure
Recovery Path:
1. Stop transition
2. Review phase completion criteria
3. Identify what remains
4. Complete or explicitly defer remaining items
5. Then transition properly
Prevention: Always verify completion criteria before phase transition

FAILURE: TRANSITION WITHOUT REORIENTATION
Severity: MEDIUM
Detection Signals:
- Move to new phase without reviewing updated context
- New phase work begins with stale understanding
- Decisions appropriate for old phase applied to new
Recovery Path:
1. Pause new phase work
2. Conduct Phase Transition Session (Section 4.3.5)
3. Reorient to new phase requirements
4. Then proceed with appropriate context
Prevention: Mandatory reorientation at every phase boundary

================================================================================
F.2 ARTIFACT CREATION FAILURES
Failures occurring when AI gathers information to populate framework artifacts.

--------------------------------------------------------------------------------
F.2.1 TOUCHPOINT 2.1: Project Guide Creation/Major Update
--------------------------------------------------------------------------------

FAILURE: INCOMPLETE CONSTRAINTS CAPTURED
Severity: HIGH
Detection Signals:
- Timeline surprises ("Oh, I needed this by Friday")
- Format requirements surfacing late
- Dependencies discovered mid-project
Recovery Path:
1. Update Project Guide immediately when constraints surface
2. Assess impact on work already done
3. Adjust plan as needed
4. Add constraint discovery to lessons learned
Prevention: Comprehensive constraint elicitation at initiation; probing questions

FAILURE: SCOPE DOCUMENTED BUT NOT BOUNDED
Severity: MEDIUM
Detection Signals:
- Project Guide describes what's included but not what's excluded
- Scope creep occurs with "I thought that was included"
- Ambiguity about boundaries
Recovery Path:
1. Add explicit "Out of Scope" section to Project Guide
2. Review boundaries with user
3. Log as scope clarification decision
Prevention: Always document both in-scope AND out-of-scope

--------------------------------------------------------------------------------
F.2.2 TOUCHPOINT 2.2: Decision Capture
--------------------------------------------------------------------------------

FAILURE: DECISION MADE BUT NOT RECOGNIZED
Severity: HIGH
Detection Signals:
- Conversation moves past choice without capture
- User later references "the decision" AI didn't record
- Choices embedded in action without explicit acknowledgment
Recovery Path:
1. Retroactive capture: "I realize we decided [X] earlier—let me log that"
2. Reconstruct rationale from conversation context
3. Get user confirmation of reconstruction
Prevention: AI proactively flags apparent decisions; "Should I capture that?"

FAILURE: RATIONALE NOT DOCUMENTED
Severity: MEDIUM
Detection Signals:
- Decision recorded without "why"
- User says "it just felt right" without elaboration
- No alternatives documented
Recovery Path:
1. Ask explicitly: "What made you choose this over alternatives?"
2. AI can offer to articulate reasoning: "Let me try to put words to it..."
3. If no rationale available, note as "rationale not captured"
Prevention: Always ask "why" for significant decisions

--------------------------------------------------------------------------------
F.2.3 TOUCHPOINT 2.3: Bootstrap Generation
--------------------------------------------------------------------------------

FAILURE: NO BOOTSTRAP GENERATED
Severity: CRITICAL
Detection Signals:
- Session ends without bootstrap file
- User departs without closeout
- Next session has no continuity document
Recovery Path:
1. If discovered same session: Generate immediately, even if basic
2. If discovered next session: Reconstruct from transcript (if available)
3. If no transcript: Document gap; rebuild context manually
Prevention: Never end session without bootstrap; use emergency if rushed

FAILURE: INCOMPLETE BOOTSTRAP
Severity: HIGH
Detection Signals:
- Bootstrap missing key sections (decisions, next actions, concerns)
- Critical context not carried forward
- Next session starts with confusion
Recovery Path:
1. Supplement bootstrap from memory/transcript
2. Start next session with context verification
3. Note incomplete closeout in session log
Prevention: Use TPL-002 checklist completely; don't skip sections

--------------------------------------------------------------------------------
F.2.4 TOUCHPOINT 2.4: Session Log Entry
--------------------------------------------------------------------------------

FAILURE: SESSION LOG SKIPPED
Severity: LOW
Detection Signals:
- No session log produced at closeout
- Project history has gaps
Recovery Path:
1. Reconstruct from transcript if available
2. Create abbreviated log from memory
3. Note as incomplete record
Prevention: Session log is part of standard closeout; don't skip

--------------------------------------------------------------------------------
F.2.5 TOUCHPOINT 2.5: Re-Entry Checklist Completion
--------------------------------------------------------------------------------

FAILURE: ABBREVIATED RE-ENTRY WHEN FULL REQUIRED
Severity: MEDIUM
Detection Signals:
- Confusion surfaces mid-session after abbreviated re-entry
- User's actual context gap larger than assessed
Recovery Path:
1. Stop work when confusion surfaces
2. Conduct fuller re-entry
3. Rebuild context properly before continuing
Prevention: Err on side of more thorough re-entry; watch for uncertainty signals

--------------------------------------------------------------------------------
F.2.6 TOUCHPOINT 2.6: Failure Recovery Intake
--------------------------------------------------------------------------------

FAILURE: WRONG FAILURE TYPE IDENTIFIED
Severity: HIGH
Detection Signals:
- Recovery actions don't address actual problem
- Repeated recovery attempts fail
- Problem persists or worsens despite protocol
Recovery Path:
1. Acknowledge recovery isn't working
2. Re-diagnose from scratch
3. Consider alternative failure types
4. Try different recovery path
Prevention: Verify failure type before executing recovery; get user agreement

FAILURE: USER IN DENIAL ABOUT FAILURE
Severity: MEDIUM
Detection Signals:
- "It's fine, just continue" despite clear problems
- Minimization of issues
- Resistance to recovery process
Recovery Path:
1. Express concern respectfully but clearly
2. Offer brief assessment: "5 minutes to check could save hours later"
3. If user insists, note concern in session log
4. Monitor for escalation
Prevention: Frame recovery as protection, not criticism

================================================================================
F.3 PROCESS EXECUTION FAILURES
Failures occurring when ongoing processes require user input.

--------------------------------------------------------------------------------
F.3.1 TOUCHPOINT 3.1: Scope Change Detection and Resolution
--------------------------------------------------------------------------------

FAILURE: SCOPE CREEP UNDETECTED
Severity: CRITICAL
Detection Signals:
- Work requested outside defined scope not flagged
- Project expands without explicit acknowledgment
- "While we're at it..." additions accumulate
Recovery Path:
1. Stop and inventory what's been added
2. Formalize each addition as scope change
3. Update Project Guide
4. Assess impact on timeline/resources
Prevention: AI must flag ALL out-of-scope requests; no silent additions

FAILURE: SCOPE CHANGE FORCED WITHOUT RESOLUTION
Severity: HIGH
Detection Signals:
- User insists on addition without acknowledging it's scope change
- "Just add it" without discussion
- Change implemented without documentation
Recovery Path:
1. Implement if user insists
2. Document as unacknowledged scope change in session log
3. Flag in bootstrap for future attention
4. Note in Decision Log as "forced scope change"
Prevention: Gentle persistence on acknowledgment; clear explanation of why it matters

--------------------------------------------------------------------------------
F.3.2 TOUCHPOINT 3.2: Verification and Quality Assurance
--------------------------------------------------------------------------------

FAILURE: VERIFICATION FATIGUE
Severity: MEDIUM
Detection Signals:
- Declining engagement with verification findings
- "Looks fine" responses without review
- Minimal responses to detailed questions
Recovery Path:
1. Reduce verification depth for lower-stakes items
2. Batch verifications where possible
3. Offer to highlight only critical items
4. Note pattern in session log
Prevention: Calibrate verification to stakes; don't over-verify low-risk items

--------------------------------------------------------------------------------
F.3.3 TOUCHPOINT 3.3: Weekly Review Facilitation
--------------------------------------------------------------------------------

FAILURE: RUSHED WEEKLY REVIEW
Severity: MEDIUM
Detection Signals:
- Review compressed significantly from standard
- Key sections skipped
- Action items not captured
Recovery Path:
1. Note incomplete review in session log
2. Carry unaddressed items to next review
3. Consider scheduling catch-up session
Prevention: Protect review time; better to reschedule than rush

================================================================================
F.4 EDGE CASE FAILURES
Failures in non-standard situations requiring specialized handling.

--------------------------------------------------------------------------------
F.4.1 TOUCHPOINT 4.1: Ambiguous User Intent
--------------------------------------------------------------------------------

FAILURE: FALSE CLASSIFICATION
Severity: MEDIUM
Detection Signals:
- User corrects AI routing
- Work proceeds in wrong direction
- Confusion about what session was supposed to accomplish
Recovery Path:
1. Acknowledge misclassification
2. Redirect to correct touchpoint
3. Salvage any usable work
4. Proceed on correct path
Prevention: Ask clarifying questions when intent genuinely unclear

--------------------------------------------------------------------------------
F.4.2 TOUCHPOINT 4.2: Contradictory Information
--------------------------------------------------------------------------------

FAILURE: CONTRADICTION PROPAGATED
Severity: HIGH
Detection Signals:
- Artifacts contain conflicting information
- Downstream problems from unresolved inconsistency
- User confusion about "what we decided"
Recovery Path:
1. Surface the contradiction explicitly
2. Stop forward progress until resolved
3. Update all affected artifacts
4. Log resolution as decision
Prevention: Never proceed past unresolved contradictions; flag immediately

--------------------------------------------------------------------------------
F.4.3 TOUCHPOINT 4.3: User Resistance to Process
--------------------------------------------------------------------------------

FAILURE: LOST BENEFITS FROM ABANDONMENT
Severity: LOW-MEDIUM
Detection Signals:
- Process elements skipped at user insistence
- Gaps in documentation/governance
- Problems emerge that process would have prevented
Recovery Path:
1. When problems emerge, note connection to skipped process
2. Offer to retroactively apply (if salvageable)
3. Use as teaching moment (gently)
Prevention: Explain value clearly; offer lightweight alternatives rather than all-or-nothing

--------------------------------------------------------------------------------
F.4.4 TOUCHPOINT 4.4: Multi-Project Confusion
--------------------------------------------------------------------------------

FAILURE: CROSS-CONTAMINATION
Severity: MEDIUM
Detection Signals:
- Artifacts mixed between projects
- Decisions applied to wrong project
- Context from one project bleeds into another
Recovery Path:
1. Identify contamination points
2. Separate artifacts
3. Verify each project's integrity
4. Add explicit project identifiers
Prevention: Strict project naming; separate chats when possible; explicit project reference

--------------------------------------------------------------------------------
F.4.5 TOUCHPOINT 4.5: External Stakeholder Information
--------------------------------------------------------------------------------

FAILURE: UNVERIFIED DATA INCORPORATED
Severity: MEDIUM
Detection Signals:
- Third-party input treated as verified without checking
- Scope changes from external sources accepted uncritically
Recovery Path:
1. Flag unverified data in artifacts
2. Verify with user before incorporating
3. Note source limitations
Prevention: Always mark external input as "per [source]"; verify significant items

--------------------------------------------------------------------------------
F.4.6 TOUCHPOINT 4.6: User Inertia / Passive Acquiescence
--------------------------------------------------------------------------------

FAILURE: RUBBER-STAMPING (Error Propagation)
Severity: MEDIUM
Detection Signals:
- Repeated minimal acknowledgments: "ok", "k", "yes", "fine", "sure"
- Three or more such responses in sequence
- Decreasing response length over time
- Responses don't engage with substance presented
Recovery Path:
1. Pause forward progress
2. Name the pattern: "I notice we're moving quickly with brief confirmations"
3. Offer options:
   - "Would you like to take a brief break?"
   - "Should I walk through recent content in more detail?"
   - "Are you confident in what we've covered, or would a summary help?"
4. If user confirms engagement, proceed with heightened verification
5. If user is fatigued, suggest session break or abbreviated closeout
6. Note pattern in session log if significant
Prevention: Calibrate confirmation to stakes; batch lower-stakes confirmations

================================================================================
F.5 CROSS-CUTTING FAILURES
Failures that can occur across multiple touchpoints.

FAILURE: CONTEXT DEGRADATION UNDETECTED
Severity: HIGH
Applies to: All touchpoints
Detection Signals:
- AI loses track of earlier session content
- Repeated questions about already-discussed topics
- Inconsistencies between early and late session responses
Recovery Path:
1. Acknowledge degradation
2. Conduct context refresh
3. Consider session closeout if severe
4. Note in bootstrap
Prevention: Regular freshness checks; monitor for warning signs (Section 4.2.4)

FAILURE: CONFIRMATION TIER MISCALIBRATED
Severity: LOW-MEDIUM
Applies to: All touchpoints requiring confirmation
Detection Signals:
- Over-confirming low-stakes items (user frustration)
- Under-confirming high-stakes items (errors slip through)
Recovery Path:
1. Adjust tier based on feedback
2. Note preference in project context
Prevention: Use Confirmation Calibration Framework (Section 3.5.3)

================================================================================
END OF APPENDIX F

================================================================================
APPENDIX G: ARTIFACT TAXONOMY REFERENCE
================================================================================

G.1 INTRODUCTION
--------------------------------------------------------------------------------

This appendix provides a systematic classification framework for project 
artifacts, explaining why each artifact exists and how they relate to 
universal project functions. Users seeking practical guidance should 
reference Section 5 (Canonical Project Artifacts); this appendix provides 
deeper theoretical grounding for those who want to understand the system's 
design rationale.

The framework presented here emerged from taxonomy exploration work 
(GOV-AIPM-TAX branch, Sessions 025-026) analyzing universal project 
functions that any structured work requires. Understanding these functions 
illuminates why certain artifacts exist and prevents the creation of 
redundant documentation.


G.2 THE TWELVE UNIVERSAL PROJECT FUNCTIONS
--------------------------------------------------------------------------------

Every project, regardless of domain, must address twelve core functions. 
These functions exist whether explicitly managed or handled ad hoc. The 
GOV-AIPM system addresses each through specific artifacts and workflows.

FUNCTION 1: PROJECT DEFINITION
What: Establishing boundaries, objectives, and identity
Why: Without clear definition, work lacks direction and scope creep is 
     inevitable
GOV-AIPM Artifacts: Project Guide, TPL-001 (Project Initiation Form)

FUNCTION 2: CONTEXT MANAGEMENT
What: Maintaining and transferring project knowledge across sessions
Why: AI context windows are finite; external memory enables continuity
GOV-AIPM Artifacts: Bootstrap files (TPL-002), Project Guide, Session Logs

FUNCTION 3: SCOPE MONITORING
What: Tracking what's in-scope, out-of-scope, and deferred
Why: Prevents scope creep; ensures intentional decisions about exclusions
GOV-AIPM Artifacts: Project Guide, Backlog Register (TPL-023)

FUNCTION 4: DECISION CAPTURE
What: Recording decisions with rationale and alternatives considered
Why: Enables learning, prevents revisitation, supports auditability
GOV-AIPM Artifacts: Decision Log (TPL-004)

FUNCTION 5: PROGRESS TRACKING
What: Monitoring work completion against plan
Why: Provides visibility; enables adjustment; demonstrates advancement
GOV-AIPM Artifacts: Project Roadmap (TPL-010), Session Logs (TPL-005)

FUNCTION 6: SESSION ORCHESTRATION
What: Managing individual work sessions—initiation, execution, closeout
Why: Each session must accomplish meaningful work within constraints
GOV-AIPM Artifacts: Bootstrap (TPL-002), Session Closeout (TPL-003), 
                    Session Log (TPL-005)

FUNCTION 7: PHASE MANAGEMENT
What: Organizing work into coherent phases with transitions
Why: Complex projects require structured progression; phases enable planning
GOV-AIPM Artifacts: Project Roadmap (TPL-010), Phase Execution Briefs

FUNCTION 8: QUALITY ASSURANCE
What: Verifying work meets standards and requirements
Why: Prevents error accumulation; maintains deliverable quality
GOV-AIPM Artifacts: Cross-AI Validation Protocol (TPL-008), review workflows

FUNCTION 9: ACCUMULATION HANDLING
What: Managing deferred items, insights, and out-of-phase discoveries
Why: Not everything fits the current phase; systematic handling prevents loss
GOV-AIPM Artifacts: Backlog Register (TPL-023)

FUNCTION 10: RECOVERY SUPPORT
What: Handling failures, errors, and unexpected situations
Why: Problems occur; systematic recovery prevents cascading failures
GOV-AIPM Artifacts: Failure Recovery Worksheet (TPL-009), 
                    Token Exhaustion Recovery Protocol (Section 4.2.5)

FUNCTION 11: KNOWLEDGE TRANSFER
What: Moving information between contexts, sessions, and stakeholders
Why: AI instances don't share memory; humans need structured briefings
GOV-AIPM Artifacts: Bootstrap files, Closure Report Section 7, 
                    Re-Entry Checklist (TPL-007)

FUNCTION 12: PROJECT CLOSURE
What: Validating completion, documenting outcomes, preparing successors
Why: Projects must end cleanly; lessons must transfer forward
GOV-AIPM Artifacts: Project Closure Report (TPL-022)


G.3 ARTIFACT-TO-FUNCTION MAPPING
--------------------------------------------------------------------------------

The following table shows which artifacts serve which functions. Note that 
most artifacts serve multiple functions—this is by design. The system 
avoids creating single-purpose artifacts where existing ones can serve 
multiple needs.

                                    FUNCTIONS SERVED
ARTIFACT                    1  2  3  4  5  6  7  8  9  10 11 12
--------------------------------------------------------------------------------
Project Guide               ●  ●  ●                       ●
Bootstrap (TPL-002)            ●           ●              ●
Session Closeout (TPL-003)              ●  ●
Decision Log (TPL-004)            ●  ●
Session Log (TPL-005)          ●     ●  ●
Weekly Review (TPL-006)              ●  ●     ●
Re-Entry Checklist (TPL-007)   ●           ●              ●
Cross-AI Validation (TPL-008)                    ●
Failure Recovery (TPL-009)                          ●
Project Roadmap (TPL-010)         ●     ●     ●
Dialog Intake Ref (TPL-011)       ●        ●
Project Closure (TPL-022)         ●     ●        ●     ●  ●
Backlog Register (TPL-023)        ●  ●              ●     ●

LEGEND:
● = Primary function served by this artifact

KEY PATTERNS:

1. Multi-Purpose Artifacts: The Project Guide serves four functions 
   (Definition, Context, Scope, Transfer). Creating separate artifacts 
   for each would fragment related information.

2. The Reporting Triad: Functions 3, 5, 9, and 12 are interconnected:
   - Backlog Register accumulates (Function 9)
   - Roadmap tracks progress (Function 5) and monitors scope (Function 3)
   - Closure Report synthesizes all (Function 12)
   
   This triad replaces what might otherwise require separate phase-end 
   reports, scope change logs, and deferred item trackers.

3. Context as Foundation: Function 2 (Context Management) underlies 
   almost everything. The Bootstrap, Project Guide, Session Logs, and 
   Dialog Intake system all contribute to maintaining context across 
   sessions. This reflects the core principle: "External Memory Always 
   Wins" (Section 2.1.2).


G.4 LIFECYCLE MAPPING
--------------------------------------------------------------------------------

Different artifacts become relevant at different project phases. This 
mapping shows when each artifact is typically created, actively used, 
or primarily referenced.

PHASE               ARTIFACTS ACTIVE                 ARTIFACTS REFERENCED
--------------------------------------------------------------------------------
INITIATION          TPL-001 → Project Guide         (none yet)
                    Initial Bootstrap
                    Initial Roadmap

ACTIVE EXECUTION    Session Logs                    Project Guide
                    Decision Log                    Roadmap
                    Bootstraps (per session)        Prior Session Logs
                    Backlog Register (ongoing)

PHASE TRANSITIONS   Updated Roadmap                 Prior Phase work
                    New Phase Bootstrap             Backlog Register
                    Phase Execution Brief

RE-ENTRY            Re-Entry Checklist              Project Guide
                    Fresh Bootstrap                 Recent Session Logs
                                                   Decision Log summary

RECOVERY            Failure Recovery Worksheet      All relevant artifacts
                    Branch Session Bootstrap        (triage determines which)

CLOSURE             Project Closure Report          ALL project artifacts
                    Final Backlog Register          (comprehensive review)
                    Archive preparation


G.5 ARTIFACT ECOSYSTEM PRINCIPLES
--------------------------------------------------------------------------------

Several principles govern the artifact ecosystem:

PRINCIPLE 1: MINIMUM SUFFICIENT DOCUMENTATION
Create only artifacts that serve identified functions. Resist the urge 
to document for documentation's sake. Every artifact should answer: 
"What function does this serve that isn't already served?"

PRINCIPLE 2: INFORMATION LIVES IN ONE PLACE
Avoid duplicating information across artifacts. Cross-reference instead. 
When information must appear in multiple places, designate one as 
authoritative and others as references.

PRINCIPLE 3: ARTIFACTS CONNECT
Artifacts reference each other. The Bootstrap points to the Project Guide; 
the Closure Report draws from the Backlog Register; Session Logs reference 
Decision Logs. These connections form a coherent system.

PRINCIPLE 4: ARTIFACTS EVOLVE
Some artifacts (Project Guide, Backlog Register) are living documents 
updated throughout the project. Others (Decision Log entries, Session 
Logs) are append-only records. Know which type each artifact is.

PRINCIPLE 5: ARTIFACTS SERVE TRANSFER
Every artifact should support handoff—to a new AI session, to a future 
human reader, or to a follow-on project. Write for someone who has no 
context beyond what the artifact provides.


G.6 WHEN TO CREATE NEW ARTIFACTS
--------------------------------------------------------------------------------

Before creating a new artifact type, verify:

1. FUNCTION GAP: Does an unserved function exist? If all twelve functions 
   are adequately addressed, new artifacts risk redundancy.

2. EXISTING COVERAGE: Can an existing artifact be extended? Often a new 
   section or field addition is preferable to a new artifact.

3. INTEGRATION PATH: How will the new artifact connect to existing ones? 
   Orphaned artifacts create maintenance burden.

4. LIFECYCLE FIT: When is this artifact created, updated, and referenced? 
   If the lifecycle is unclear, the artifact may not be needed.

5. TOKEN ECONOMICS: What's the context cost? Heavy artifacts that must 
   be loaded frequently may not justify their value.

The taxonomy exploration that produced this appendix evaluated eleven 
proposed templates and validated only two (TPL-022 and TPL-023). Most 
proposals failed because existing artifacts already served the identified 
functions, or the function itself was out of scope for the target use 
case.


G.7 SCOPE BOUNDARIES
--------------------------------------------------------------------------------

This framework deliberately excludes certain domains:

OUT OF SCOPE:

Multi-Team Coordination: RACI matrices, team communication protocols, 
and coordination frameworks assume multiple humans. The GOV-AIPM target 
use case is a single user working with AI assistance.

Formal Project Management: Work breakdown structures, Gantt charts, 
critical path analysis. These formal methods add overhead inappropriate 
for the target use case. The system handles decomposition implicitly 
through phases and sessions.

Pre-Project Exploration: Client assessment, project crystallization, 
and consultative discovery. These activities precede what GOV-AIPM 
manages. They might use GOV-AIPM artifacts but operate in a different 
mode.

Multi-Agent Systems: Coordination between multiple AI agents or 
autonomous agent workflows. The framework assumes human-in-the-loop 
with one primary AI assistant.

These exclusions are intentional scope decisions, not gaps. Future 
systems might address these domains but would interface with GOV-AIPM 
rather than expand it.


G.8 GLOSSARY INTEGRATION
--------------------------------------------------------------------------------

The following terms from this appendix appear in Section 17.2 (Glossary):

Artifact - A project document with defined purpose and structure
Artifact Ecosystem - The interconnected set of project artifacts
Function (Project) - A universal capability that projects require
Reporting Triad - The integrated Backlog/Roadmap/Closure system
Lifecycle Mapping - Assignment of artifacts to project phases

For additional terminology, see Section 17.2.


G.9 FURTHER DEVELOPMENT
--------------------------------------------------------------------------------

This appendix represents v1.0 of the taxonomy reference. Future versions 
may include:

- Detailed artifact templates annotated with function served
- Visual artifact relationship diagrams
- Extended lifecycle guidance for complex projects
- Integration patterns for governance documentation

Full taxonomy exploration continues in the GOV-AIPM-TAX branch. Findings 
from that work will inform future appendix revisions.


================================================================================
END OF APPENDIX G

================================================================================
APPENDIX H: QUICK START GUIDE
================================================================================

Version: 1.0
Date: December 28, 2025

This appendix contains the complete Quick Start Guide, also available as a
standalone document for distribution. New users should begin here before
engaging the full manual.

--------------------------------------------------------------------------------

================================================================================
GOV-AIPM QUICK START GUIDE
================================================================================

================================================================================
WHAT IS THIS?
================================================================================

The AI Operations Manual is a comprehensive framework for AI-assisted project
management. It enables structured collaboration between you and AI across
multi-session projects while maintaining quality, continuity, and governance.

This Quick Start Guide gets you from "I just downloaded this" to "I'm working
on my first project" in about 30 minutes.

================================================================================
BEFORE YOU BEGIN: ARE YOU READY?
================================================================================

Not every task needs this framework. Quick questions, single-session tasks,
and casual exploration work fine without it. Use GOV-AIPM when:

- Your work will span multiple AI sessions
- You need to maintain continuity across days or weeks
- Quality and accuracy matter (business, legal, technical work)
- You want structured decision tracking
- Multiple deliverables or phases are involved

If you're unsure, that's fine—start with an Exploration Session (see below).

--------------------------------------------------------------------------------
READINESS CHECK
--------------------------------------------------------------------------------

Before initiating a formal project, you should be able to answer:

1. What am I trying to produce? (State in one sentence)
2. Who will use or receive the output?
3. How will I know when it's done?
4. Do I have the inputs I need, or know how to get them?
5. Is this one project or several?

If these answers are clear → Proceed to "Starting Your First Project"
If these answers are unclear → Start with an Exploration Session

--------------------------------------------------------------------------------
EXPLORATION SESSIONS (When You're Not Ready Yet)
--------------------------------------------------------------------------------

Some ideas need development before structured execution. An Exploration
Session uses AI to clarify your thinking.

To run an Exploration Session:
1. Open a new AI chat
2. Upload the AI Operations Manual
3. Say: "I have an idea I'd like to explore before committing to a formal
   project. Help me think through [describe your idea]."

Useful exploration prompts:
- "What questions should I be able to answer before starting this?"
- "What are the components of this? Which should I tackle first?"
- "What would success look like?"
- "What am I assuming that might not be true?"

Exploration sessions are low-stakes. Their output helps you complete the
Readiness Check when you're ready.

================================================================================
STARTING YOUR FIRST PROJECT
================================================================================

This is the core workflow. Follow these steps exactly.

--------------------------------------------------------------------------------
STEP 1: GATHER YOUR FILES
--------------------------------------------------------------------------------

You need two files:
- GOV-AIPM_Complete_Manual_v1.0.7.txt (the framework)
- TPL-001 Project Initiation Form (the intake template)

TPL-001 is included in Appendix A of the manual. Extract it to a separate
file, or use it directly from the manual.

IMPORTANT: You do NOT need to complete TPL-001 before uploading. In fact,
partial or blank forms are the norm, not the exception. The form is a
conversation starter, not a prerequisite. You'll develop it through dialog
with the AI—that's the point.

--------------------------------------------------------------------------------
STEP 2: OPEN A NEW AI CHAT AND UPLOAD
--------------------------------------------------------------------------------

Start a fresh AI chat session. Upload both files.

--------------------------------------------------------------------------------
STEP 3: INITIATE THE INTAKE DIALOG
--------------------------------------------------------------------------------

Send a message like:

  "I'm starting a new project using the GOV-AIPM framework. I've uploaded the
  manual and my TPL-001 [partial/blank]. Per Section 1.4.6.1, let's engage in
  collaborative project design—I expect you to probe my assumptions, challenge
  vague criteria, and help me think through scope before we finalize anything."

Then STOP. Wait for the AI to respond.

--------------------------------------------------------------------------------
STEP 4: ENGAGE IN COLLABORATIVE PROJECT DESIGN
--------------------------------------------------------------------------------

THIS IS THE CRITICAL STEP. Read carefully.

Project initiation is NOT:
- Fill out form → Submit → AI tells you what to do
- One message in, one Project Guide out
- A passive, transactional process

Project initiation IS:
- A collaborative design session
- 10-20 exchanges of substantive back-and-forth (typically 15-30 minutes—far
  less than you'll lose to rework if you start with a vague definition)
- An investment that prevents costly rework later

WHAT THE AI WILL DO:

The AI will actively probe your project definition:

- Ask clarifying questions about your objective
  ("When you say 'improve,' what specific outcome are you targeting?")

- Surface assumptions you haven't stated
  ("You mentioned the client. Are there other stakeholders whose needs
  should shape this?")

- Probe scope boundaries
  ("Would revising the existing policy be in scope, or only documenting
  the current one?")

- Propose phasing options and trade-offs
  ("We could tackle this in two phases: research then drafting. Or combine
  them. What's your timeline pressure?")

- Challenge vague success criteria
  ("'Make it better' is hard to verify. What would you point to and say
  'yes, this is what I wanted'?")

WHAT YOU SHOULD DO:

- Answer thoughtfully, not quickly
  Take time to consider each question. The goal is clarity, not speed.

- Push back if the AI misunderstands
  "No, that's not what I meant. Let me clarify..."

- Raise concerns or constraints as they occur to you
  "Actually, I should mention that we can't use external data sources."

- Request alternatives
  "What if we approached it differently? What are our options?"

- Ask your own questions
  "What am I not thinking about that I should be?"

- Be honest about uncertainty
  "I don't know yet. Can we leave that open and decide later?"

This dialog IS the work. It's not overhead before the work begins. A
well-defined project executes smoothly. A poorly-defined project generates
rework, misalignment, and frustration.

--------------------------------------------------------------------------------
WHAT THIS LOOKS LIKE IN PRACTICE
--------------------------------------------------------------------------------

Here's a typical intake exchange:

  USER: "I want to create a policy document for our company."

  AI: "What kind of policy—operational, compliance, HR? And who's the
  primary audience: employees, regulators, or both?"

  USER: "HR policy. Employees are the audience."

  AI: "Got it. Is this a new policy or revising an existing one? And what's
  driving this—a compliance gap, employee feedback, or something else?"

  USER: "New policy. We've had some issues with remote work expectations."

  AI: "So the trigger is ambiguity around remote work. Before we scope this:
  should the policy cover eligibility (who can work remote), expectations
  (availability, responsiveness), or both? And are there existing documents
  —employee handbook sections, informal guidelines—I should know about?"

  USER: "Both eligibility and expectations. We have an old handbook section
  but it's outdated."

  AI: "Understood. One more question: what's your authority here? Are you
  drafting for your own approval, or does this need sign-off from leadership
  or legal?"

Notice what's happening: the AI isn't waiting for a complete brief. It's
actively narrowing scope, surfacing constraints, and ensuring the user has
thought through dimensions they might have skipped. This is collaborative
design in action.

--------------------------------------------------------------------------------
STEP 5: REVIEW THE PROJECT GUIDE
--------------------------------------------------------------------------------

After sufficient dialog, the AI will synthesize your conversation into a
Project Guide—a structured document that defines:

- Project objective and scope
- What's in-scope and out-of-scope
- Constraints and dependencies
- Deliverables and success criteria
- Phase structure and timeline
- Naming conventions and file expectations

Review this carefully. The Project Guide becomes your project's constitution.
Everything flows from it.

If something is wrong or missing, say so:
  "The scope section doesn't capture [X]. Let's revise that."
  "I'm not sure about the phasing. Can we discuss alternatives?"

Iterate until the Project Guide accurately reflects your intent.

--------------------------------------------------------------------------------
STEP 6: CONFIRM ALIGNMENT AND BEGIN
--------------------------------------------------------------------------------

When the Project Guide is right, confirm:

  "This Project Guide captures my intent. I'm ready to proceed."

The AI will then:
- Confirm the first phase objectives
- Identify what's needed to begin
- Start substantive work

You are now operating within the GOV-AIPM framework.

================================================================================
WHAT HAPPENS NEXT
================================================================================

After your first session, you'll use different workflows:

CONTINUING A PROJECT (Session to Session):
- Upload the Bootstrap from your prior session
- The AI reorients and continues where you left off
- See Manual Section 1.4.6.2

RESUMING AFTER A BREAK (Days or Weeks Away):
- Use the Re-Entry Protocol
- See Manual Section 1.4.6.3

TRANSITIONING BETWEEN PHASES:
- Use the Phase Transition Protocol
- See Manual Section 1.4.6.4

Each of these is documented in detail in the manual.

================================================================================
KEY CONCEPTS TO UNDERSTAND
================================================================================

BOOTSTRAP: A handoff document generated at the end of each session. Contains
everything needed to resume. Always save it. Always upload it next session.

PROJECT GUIDE: The authoritative definition of your project. Created during
intake, updated as the project evolves.

SESSION CLOSEOUT: The discipline of ending sessions cleanly—saving files,
generating bootstrap, logging decisions. Non-negotiable.

DECISION LOG: Record of significant choices made during the project. Captures
what was decided, why, and what alternatives were rejected.

For complete definitions, see Manual Section 17.2 (Glossary).

================================================================================
COMMON MISTAKES TO AVOID
================================================================================

1. SKIPPING THE INTAKE DIALOG
   Uploading a form and expecting instant output. The dialog is where
   alignment happens.

2. RUSHING THROUGH QUESTIONS
   Answering "yes" or "fine" without thinking. Vague inputs produce vague
   outputs.

3. NOT SAVING THE BOOTSTRAP
   Ending a session without saving the bootstrap. You will lose context
   and waste time reconstructing.

4. CHERRY-PICKING THE FRAMEWORK
   Using some templates but not others. The components interlock. Skipping
   pieces creates gaps that surface later.

5. TREATING AI OUTPUT AS FINAL
   Accepting what the AI produces without review. AI is a capable assistant,
   not an oracle. You remain accountable.

================================================================================
GETTING HELP
================================================================================

If you get stuck:
- Upload the manual and ask: "I'm having trouble with [X]. What does the
  framework say about this?"
- The AI can reference specific sections and guide you.

If something isn't working:
- Check the Troubleshooting Guide (Manual Appendix D)
- Review the Failure Mode Catalog (Manual Appendix F)

================================================================================
SUMMARY: YOUR FIRST PROJECT IN 6 STEPS
================================================================================

1. GATHER FILES
   Manual + TPL-001 (partial or blank is normal)

2. UPLOAD
   New AI chat, attach both files

3. INITIATE
   "I'm starting a new project using GOV-AIPM. Per Section 1.4.6.1, let's
   engage in collaborative project design—probe my assumptions and help me
   think through scope before we finalize anything."

4. ENGAGE
   10-20 exchanges of back-and-forth (15-30 minutes). Answer thoughtfully.
   Push back. Raise concerns. Request alternatives.

5. REVIEW
   Check the Project Guide draft. Iterate until it's right.

6. CONFIRM
   "This captures my intent. I'm ready to proceed."

================================================================================
END OF APPENDIX H (QUICK START GUIDE)
================================================================================

================================================================================
END OF APPENDICES (A-H COMPLETE)
Document:        AI Operations Manual — Appendices A-H
Version:         1.0.7-DRAFT
Date:            December 28, 2025
Status:          Phase 6b Review and Validation

Contents:
- Appendix A: Template Reference (13 templates, full text)
- Appendix B: Prompt Reference (Golden Prompt Library)
- Appendix C: Tool Configuration Guide
- Appendix D: Troubleshooting Guide
- Appendix E: Version History
- Appendix F: Dialog Intake Failure Mode Catalog
- Appendix G: Artifact Taxonomy Reference
- Appendix H: Quick Start Guide

Total Lines: ~5,500

================================================================================
================================================================================
                          END OF AI OPERATIONS MANUAL
================================================================================
================================================================================

Document:        AI Operations Manual: A Practical System for 
                 AI-Assisted Business Operations
Code:            GOV-AIPM
Version:         1.0.7-DRAFT
Assembly Date:   December 28, 2025
Phase:           6b - Review and Validation

Contents:
- Part I: Foundations (Sections 1-3)
- Part II: Project Management (Sections 4-6)
- Part III: Governance Integration (Sections 7-8)
- Part IV: Tools & Formats (Sections 9-11)
- Part V: Advanced Patterns (Sections 12-14)
- Part VI: Mail & External Communications (Sections 15-17)
- Appendix A: Template Reference (13 templates)
- Appendix B: Prompt Reference (Golden Prompt Library)
- Appendix C: Tool Configuration Guide
- Appendix D: Troubleshooting Guide
- Appendix E: Version History
- Appendix F: Dialog Intake Failure Mode Catalog
- Appendix G: Artifact Taxonomy Reference
- Appendix H: Quick Start Guide

Revisions Integrated During Assembly:
- Section 2.1.4 (Use the Full System) - Added
- Section 4.2.4 (Warning Signs of Context Exhaustion) - Added
- Section 4.2.5 (Token Exhaustion Recovery Protocol) - Added

v1.0.1 Corrections (December 23, 2025):
- Section 9 (AI Role Separation) restored - was missing from v1.0.0 assembly
- Section 10.1-10.4 restored - were missing from v1.0.0 assembly
- Section 10.5 opening restored (Cloud Storage, Version-Aware Storage paragraphs)
- Part IV header and introduction added
- Orphaned fragment removed from Section 8/10 boundary

Phase 6a Additions (December 23, 2025, Session 012):
- Section 2.5 (The Three Cycles of AI-Assisted Work) - NEW foundational content
- Section 2.6 (The Orientation Principle) - NEW foundational content
- Section 1.4.5 (Project Readiness Assessment) - NEW guidance content
- TOC updated to reflect new Section 2 subsections

v1.0.3 Additions (December 26, 2025, Sessions 015-021):
- Section 3.5 (The Dialog Intake System) - NEW comprehensive system
- Section 3.5/3.6 renumbered to 3.6/3.7
- Appendix F (Dialog Intake Failure Mode Catalog) - NEW
- TPL-011 (Dialog Intake Quick Reference) - NEW
- Template annotations added to TPL-001 through TPL-010

v1.0.4 Additions (December 27, 2025, Sessions 027-028A):
- Section 5.7 (Backlog Register) - NEW section
- Section 5.8 (Project Closure Report) - NEW section
- Cross-references updated throughout Section 5

v1.0.5 Additions (December 28, 2025, Sessions 028B-028C):
- TPL-022 (Project Closure Report) - NEW template
- TPL-023 (Backlog Register) - NEW template
- Template Index updated (11 → 13 templates)
- Template Usage Guidance updated
- Appendix G (Artifact Taxonomy Reference) - NEW appendix
- Section 5 introduction references Appendix G (already present)

v1.0.6 Corrections (December 28, 2025, Session 029):
- Cross-reference audit completed
- Four missing references identified and fixed
- Phase 6b external validation initiated

v1.0.7 Additions (December 28, 2025, Session 030):
- Section 4.3.5.2.1 (Token Considerations for Phase Transitions) - NEW subsection
  - Headroom assessment guidance for phase transitions
  - Segmentation strategy for substantial deliverables
- Appendix H (Quick Start Guide) - NEW appendix
  - Standalone onboarding guide for new users
  - Emphasizes collaborative dialog intake process
  - Includes example exchange demonstrating intake workflow

Phase 6b Review and Validation: IN PROGRESS
Next Phase: 7 - Final Polish and Release

================================================================================
                              END OF DOCUMENT
================================================================================
================================================================================
