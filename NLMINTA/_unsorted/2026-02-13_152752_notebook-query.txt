================================================================================
EXTRACTION: 2026-02-13_152752_notebook-query
================================================================================
Branch:          NLMINTA
Source Package:  N/A
Pass Type:       N/A (N/A)
Pass Number:     N/A
Theme/Focus:     N/A
Project:         N/A
Session:         020
Routing:         Tier 1b (NLMINTA/_unsorted)
Query:           What were the key findings from J6 (explicit exclusion testing) and J7 (report tool testing) in Session 019? Specifically: the validated D1 extraction pipeline stages, Pathway C methodology details, the anti-consolidation prompting technique, and any caveats or limitations discovered during testing that should inform the J8 full-scale capstone test design.
Timestamp:       2026-02-13T15:27:52.000229+00:00
Notebook ID:     b8fd3d1c-db62-40e6-bddf-6d8474a3733d
Source IDs:      all
Conversation ID: fe876645-0bee-4f7e-bfbc-160ffb8d303c
Chat Configure:  default
Thread:          N/A
Notes:           N/A
Response Chars:  11739
Duration:        331000ms
================================================================================

Here is the complete full text of **Appendix B: Prompt Reference (Golden Prompt Library)** from the GOV-AIPM manual.\
\
***\
\
**APPENDIX B: PROMPT REFERENCE**\
\
This appendix contains the Golden Prompt Library...a curated collection of refined, high-performing prompts that have proven effective across AI-assisted projects. Rather than reinventing prompts each session or losing successful formulations in chat transcripts, this library provides a reusable reference. [1]\
\
### ABOUT THE PROMPT LIBRARY\
\
**PURPOSE:**\
The prompt library captures prompts that have been tested and refined through actual use. These are not theoretical constructs but working tools that have demonstrated effectiveness in real project contexts. [2]\
\
**USAGE:**\
*   Before starting common tasks, check the library for existing prompts\
*   Copy and adapt as needed (replace `{{VARIABLES}}` with specifics)\
*   Note adaptations that improve performance...these become version updates\
*   Add new prompts when you develop ones that work exceptionally well [2]\
\
**MAINTENANCE:**\
*   Review quarterly for stale or underperforming entries\
*   Archive deprecated prompts rather than deleting\
*   Version prompts that undergo significant refinement [3]\
\
**INTEGRATION:**\
*   Upload to NotebookLM for queryable access\
*   Reference during project rooting to select appropriate prompts\
*   See Section 10.7 of the AI Operations Manual for full guidance [3]\
\
---\
\
### PROMPT CATEGORIES\
\
| Category | Code | Description |\
| :--- | :--- | :--- |\
| **Alignment** | ALIGN | Establishing shared understanding, confirming params |\
| **Drafting** | DRAFT | Content generation, document creation |\
| **Verification** | VERIF | Cross-checking, validation, quality assurance |\
| **Recovery** | RECOV | Context reconstruction, failure recovery, resumption |\
| **Pivot** | PIVOT | Redirecting AI when off-track or stuck |\
| **Closeout** | CLOSE | Session ending, bootstrap generation, handoff prep | [4]\
\
### PROMPT INDEX\
\
| ID | Name | Category | Use Case |\
| :--- | :--- | :--- | :--- |\
| **ALIGN-001** | Explicit Alignment Grounding | Alignment | Project initiation |\
| **RECOV-001** | Bootstrap Generator | Recovery | Session handoff |\
| **QA-001** | Cognitive Divergence Audit | Verification | Drift detection |\
| **VERIF-001** | Adversarial Cross-AI Validator | Verification | Tier 2 validation | [5, 6]\
\
---\
\
### PROMPT ENTRIES\
\
#### PROMPT ID: ALIGN-001\
\
**NAME:** Explicit Alignment Grounding\
**CATEGORY:** Alignment\
**CONTEXT:** Start of Root Chat or after major pivot. Use when establishing shared understanding before substantive work begins.\
**MODEL AFFINITY:** Model-Agnostic (tested: Claude, GPT-4) [6]\
\
**THE PROMPT:**\
> I am initiating a project under the GOV-AIPM framework. I will provide {{PROJECT_INITIATION_FORM or PROJECT_GUIDE}}. Your task is to:\
>\
> 1. Internalize the scope and constraints\
> 2. Identify any assumptions you are making about this request\
> 3. Ask 3-5 clarifying questions that, once answered, will ensure alignment\
>\
> Do not begin drafting until I confirm we are aligned. [7]\
\
**WHAT TO AVOID:**\
*   Don't skip the clarifying questions step even if the brief seems clear\
*   Don't provide assumptions as questions...state them as assumptions for user confirmation [8]\
\
**PERFORMANCE NOTES:**\
*   High success rate in preventing mid-project pivots\
*   The \"identify assumptions\" step catches approximately 80% of alignment gaps early in the process [8]\
\
**RELATED PROMPTS:** RECOV-001 (for session handoff alignment)\
**SOURCE:** GOV-AIPM development\
**VERSION:** 1.0 **LAST UPDATED:** December 21, 2025 [8]\
\
---\
\
#### PROMPT ID: RECOV-001\
\
**NAME:** Bootstrap Generator\
**CATEGORY:** Recovery / Closeout\
**CONTEXT:** End of session when preparing handoff to next chat. Use when approaching token limits or completing a session phase.\
**MODEL AFFINITY:** Claude-optimized (structure aligns with TPL-002) [9]\
\

**THE PROMPT:**\
> We are closing this session. Review our conversation and generate a bootstrap for the next AI instance. Include:\
>\
> 1. Current project state and logic position\
> 2. All decisions made (reference Decision Log entries by number)\
> 3. Open items and immediate next actions\
> 4. Warning signs the next instance should monitor for context drift\
>\
> Structure per TPL-002. Prioritize fidelity over brevity, but compress where possible without losing critical context. [9, 10]\
\
**WHAT TO AVOID:**\
*   Don't sacrifice decision rationale for compression\
*   Don't omit warning signs even if session went smoothly [10]\
\
**PERFORMANCE NOTES:**\
*   Works best when Decision Log has been maintained during session\
*   \"Warning signs\" section catches issues that manifest in subsequent sessions [11]\
\
**RELATED PROMPTS:** ALIGN-001 (for receiving chat alignment)\
**SOURCE:** GOV-AIPM development\
**VERSION:** 1.0 **LAST UPDATED:** December 21, 2025 [11]\
\
---\
\
#### PROMPT ID: QA-001\
\
**NAME:** Cognitive Divergence Audit\
**CATEGORY:** Verification\
**CONTEXT:** Use when AI outputs consistently miss the mark despite clear instructions. Diagnoses whether the AI is operating from a flawed mental model. [11]\
\
**THE PROMPT:**\
> I suspect we have diverged. Pause generation.\
>\
> Review our interaction history and identify the specific point where your understanding of the objective may have separated from my instructions.\
>\
> 1. State the core objective as you currently understand it.\
> 2. List the constraints you believe are active.\
> 3. Identify the assumption you made that led to the current output path.\
>\
> Do not generate new content. Only diagnose the divergence. [11]\
\
**WHAT TO AVOID:**\
*   Don't use for simple errors (typos, formatting); use for structural misunderstanding\
*   Don't allow the AI to \"apologize and retry\" without the diagnosis step [11]\
\
**PERFORMANCE NOTES:**\
*   Forces the AI to step out of \"generation mode\" and into \"analysis mode\"\
*   Often reveals that a constraint from 20 turns ago was dropped due to context compaction [11]\
\
**RELATED PROMPTS:** See TPL-009 Protocol G if divergence persists after correction\
**CROSS-REFERENCE:** Section 14.3 (Cognitive Divergence Detection)\
**SOURCE:** GOV-AIPM development\
**VERSION:** 1.0 **LAST UPDATED:** December 21, 2025 [12]\
\
---\
\
#### PROMPT ID: VERIF-001\
\
**NAME:** Adversarial Cross-AI Validator\
**CATEGORY:** Verification\
**CONTEXT:** Tier 2 validation using secondary AI. Use when primary AI output requires independent verification per TPL-008.\
**MODEL AFFINITY:** Best used in AI different from the one that produced the work (e.g., GPT-4 validating Claude output, or vice versa) [13]\
\
**THE PROMPT:**\
> You are an adversarial auditor. Review the attached {{DOCUMENT_TYPE}} generated by another AI.\
>\
> Identify:\
> 1. Logic gaps or unsupported claims\
> 2. Instances of \"hallucinated consistency\" (conclusions that sound coherent but lack foundation)\
> 3. Sections where the AI was \"too agreeable\" (accepted flawed premises)\
> 4. Tone shifts from technical/instructional to conversational filler\
>\
> Provide a Reliability Score (1-10) with specific findings supporting your rating. Do not soften criticism. [13, 14]\
\
**WHAT TO AVOID:**\
*   Don't use the same AI that produced the work\
*   Don't accept high scores without specific supporting evidence [14]\
\
**PERFORMANCE NOTES:**\
*   \"Too agreeable\" check catches sycophantic patterns effectively\
*   Works best when validator has no context from original session (fresh eyes)\
*   Reliability Score provides quick triage for further review needs [14, 15]\
\
**RELATED PROMPTS:** See TPL-008 for full validation protocol\
**SOURCE:** GOV-AIPM development\
**VERSION:** 1.0 **LAST UPDATED:** December 21, 2025 [15]\
\
---\
\
### ADDITIONAL PROMPT PATTERNS\
\
The following prompt patterns appear throughout the manual and are useful for specific situations. They are presented in condensed form for quick reference. [15]\
\
#### ALIGNMENT PROMPTS (Pre-Work)\
\
**RESTATE UNDERSTANDING:**\
\"Before proceeding, restate what you understand the task to be.\"\
\
**SURFACE ASSUMPTIONS:**\
\"What assumptions are you making about this request?\"\
\
**FAILURE ANTICIPATION:**\
\"What would cause this approach to fail?\"\
\
**SCOPE CONFIRMATION:**\
\"Confirm: What is IN scope and what is OUT of scope for this task?\" [16]\
\
#### DRAFTING PROMPTS\
\
**STRUCTURED OUTPUT REQUEST:**\
\"Generate [output type] with the following structure: [structure]. Include [required elements]. Omit [excluded elements].\"\
\
**ITERATIVE REFINEMENT:**\
\"Here is your previous output. Revise to address: [specific issues]. Maintain [elements to preserve]. Do not change [protected elements].\"\
\
**TONE CALIBRATION:**\
\"Adjust the tone of this draft to be more [formal/casual/technical/accessible]. The audience is [audience description].\" [17, 18]\
\
#### VERIFICATION PROMPTS\
\
**SELF-CHECK:**\
\"Review your output for: alignment with objective, gaps in coverage, unstated assumptions. Report findings before I proceed.\"\
\
**SOURCE CHECK:**\
\"For each factual claim in your response, indicate your confidence level and whether verification is recommended.\"\
\
**CONSISTENCY CHECK:**\
\"Compare this output to [prior output/established parameter]. Identify any inconsistencies or contradictions.\" [18, 19]\
\
#### RECOVERY PROMPTS\
\
**CONTEXT RECONSTRUCTION:**\
\"I need to reconstruct context. Based on [available materials], summarize: current state, recent decisions, and immediate next actions.\"\
\
**FRESH START:**\
\"Disregard our previous exchange on this topic. Let me reframe the request from the beginning: [fresh framing].\"\
\
**CLARIFICATION REQUEST:**\
\"I'm not confident we're aligned. Please tell me: What do you understand the objective to be? What are you trying to accomplish with your response?\" [19, 20]\
\
#### CLOSEOUT PROMPTS\
\
**SESSION SUMMARY:**\
\"Summarize this session: objectives, accomplishments, decisions made, open items, and recommended next actions.\"\
\
**DECISION EXTRACTION:**\
\"List all decisions made in this session in the format:\
Decision: [statement] | Rationale: [brief rationale]\"\
\
**FILE INVENTORY:**\
\"List all files we've discussed creating or modifying this session, with their current status (created/modified/pending).\" [21]\
\
---\
\
### ADDING NEW PROMPTS TO THE LIBRARY\
\
When a prompt works exceptionally well:\
\
1.  NOTE IT during session (flag in Session Log under \"Process Observations\")\
2.  DRAFT ENTRY at session closeout or weekly review\
3.  GENERALIZE the prompt (remove project-specific references, add variables)\
4.  ADD METADATA (context, model affinity, what to avoid, performance notes)\
5.  ASSIGN ID per category convention: [CATEGORY]-[###]\
6.  APPEND to this library [22]\
\
#### NEW PROMPT ENTRY TEMPLATE\
\
**PROMPT ID:** [CATEGORY]-[###]\
**NAME:** [Descriptive name]\
**CATEGORY:** [Alignment / Drafting / Verification / Recovery / Pivot / Closeout]\
**CONTEXT:** [When to use...situation or trigger]\
**MODEL AFFINITY:** [Claude / GPT / Gemini / Model-Agnostic]\
**THE PROMPT:** [Exact prompt text with {{VARIABLES}} for user substitution]\
**WHAT TO AVOID:** [Known failure modes or anti-patterns when using this prompt]\
**PERFORMANCE NOTES:** [Success rate or qualitative assessment]\
**RELATED PROMPTS:** [Cross-references to related entries]\
**SOURCE:** [Project/session origin]\
**VERSION:** [#] **LAST UPDATED:** [Date] [23, 24]\
\
---\
\
### VERSION HISTORY\
\
| Version | Date | Changes |\
| :--- | :--- | :--- |\
| **1.0** | 2025-12-21 | Initial library with 4 core prompts | [25]\
\
**END OF APPENDIX B**

================================================================================
END OF EXTRACTION
================================================================================
