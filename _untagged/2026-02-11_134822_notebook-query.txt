================================================================================
EXTRACTION: 2026-02-11_134822_notebook-query
================================================================================
Source Package:  N/A
Pass Type:       N/A (N/A)
Pass Number:     N/A
Theme/Focus:     Untagged
Routing:         Tier 3 (Untagged)
Query:           Section 4.2 Token Management full text including headroom discipline, segmentation, token exhaustion recovery
Timestamp:       2026-02-11T13:48:22.000672+00:00
Notebook ID:     a0a96d92-1e44-4907-85a7-09e286058fdf
Source IDs:      all
Conversation ID: 7e029a99-affb-43f5-9d80-69dc7bfd857c
Chat Configure:  default
Response Chars:  7073
Duration:        48999ms
================================================================================

Phase Protocol confirmed. I have reviewed the AIPM Phase Protocol from Project Knowledge and confirm adherence to all behavioral requirements.\
\
Here is the full text of **Section 4.2 Token Management** from the AI Operations Manual (v1.0.7).\
\
***\
\
### 4.2 TOKEN MANAGEMENT\
\
Tokens are the fundamental constraint of AI-assisted work. Every input and output consumes tokens from a finite context window. Effective token management extends productive session life and prevents catastrophic context loss.\
\
#### 4.2.1 Token Economics\
\
Understanding token consumption:\
\
**Inputs Consume Tokens:**\
- User messages\
- Uploaded files\
- System context (invisible but real)\
- Conversation history (cumulative)\
\
**Outputs Consu< 2026/02/11 13:49:11.000399815  length=6381 from=8693 to=15073
me Tokens:**\
- AI responses\
- Generated content\
- Reasoning traces\
\
**Cumulative Effect:**\
Each exchange adds to the running total. Long conversations consume the context window progressively. There is no way to selectively forget...the entire conversation occupies context until the session ends.\
\
**Practical Implication:**\
Long, meandering conversations are token-expensive. Focused, scoped sessions preserve headroom for substantive work.\
\
#### 4.2.2 Headroom Discipline\
\
Token headroom is the remaining context capacity. Managing headroom is essential for controlled session completion.\
\
**THE HEADROOM RULE:**\
Before initiating substantial work (drafting, analysis, complex reasoning), assess remaining headroom. If headroom is marginal, STOP. Do not draft into a token cliff.\
\
**Marginal Headroom Indicators:**\
- Session has been running for extended period\
- Multiple large files have been uploaded\
- Conversation includes many long exchanges\
- AI begins truncating responses or losing thread\
\
**When Headroom is Marginal:**\
1. Pause substantive work\
2. Generate a bootstrap file (see Section 5.4)\
3. Save all in-progress work\
4. Close the session cleanly\
5. Resume in a fresh chat with the bootstrap\
\
**The Alternative (What Happens Without This Discipline):**\
- Token limit hit mid-draft\
- Work is lost or truncated\
- No clean transition point\
- Context degradation in final outputs\
- Recovery requires reconstruction rather than resumption\
\
#### 4.2.3 Token-Efficient Practices\
\
Reduce unnecessary token consumption:\
\
**File Format:**\
- Use TXT over formatted documents (fewer tokens for same content)\
- Strip unnecessary formatting from inputs\
- Provide relevant excerpts rather than full documents when possible\
\
**Conversation Hygiene:**\
- Keep exchanges focused\
- Avoid extended tangents\
- Use separate chats for separate topics\
- Don't load a chat with content \"just in case\"\
\
**Output Requests:**\
- Specify length expectations\
- Request sections rather than complete documents when drafting\
- Build incrementally rather than requesting massive single outputs\
\
**Strategic Loading:**\
- Front-load essential context\
- Defer nice-to-have context until needed\
- Use bootstraps to carry only necessary information forward\
\
#### 4.2.4 Warning Signs of Context Exhaustion\
\
Token limits don't announce themselves with error messages. Context exhaustion manifests as degraded performance before the session terminates. Recognizing warning signs allows proactive bootstrap before quality collapses.\
\
Watch for these indicators:\
\
**Instruction Drift**\
The AI begins ignoring constraints that were clearly established earlier in the session. Negative instructions (\"don't use bullet points,\" \"avoid technical jargon\") are particularly vulnerable. If the AI starts violating explicit constraints without acknowledgment, context is compressing.\
\
**Repetitive Loops**\
The AI produces responses that echo previous outputs without meaningful progression. It may rephrase the same point multiple ways, circle back to topics already resolved, or generate text that feels like filler rather than advancement. This indicates the working context is saturated.\
\
**Hallucinated References**\
The AI refers to decisions, documents, or prior exchanges that don't exist in the current session. It may cite \"as we discussed\" for discussions that never occurred, or reference \"Decision #X\" for decisions not in the active log. This is confabulation under context pressure.\
\
**Shallow Responses**\
Complex questions that earlier in the session received thorough treatment now receive superficial answers. The AI may acknowledge complexity but fail to engage it, or provide generic responses where specific ones were previously standard.\
\
**Response to Warning Signs:**\
When any warning sign appears:\
1. Note the approximate point in conversation where degradation began\
2. Explicitly ask the AI to assess its current context state\
3. If degradation is confirmed, initiate closeout immediately\
4. Generate bootstrap before further substantive work\
5. In the next session, reference where degradation occurred to calibrate future token monitoring\
\
These warning signs connect to the Token Exhaustion Recovery Protocol (see Section 4.2.5 and TPL-009).\
\
#### 4.2.5 Token Exhaustion Recovery Protocol\
\
When a session reaches token limits unexpectedly ... or warning signs weren't caught in time ... use this recovery protocol:\
\
**Step 1: Mark the Exhausted Session**\
Add \"EXH\" (Exhausted) tag to the chat name and folder:\
Original: `003 GOV-AIPM-EX (parts III-IV draft)`\
Tagged: `003 GOV-AIPM-EX (parts III-IV draft) EXH`\
\
This prevents confusion about session status and signals that recovery materials exist elsewhere.\
\
**Step 2: Export the Chat**\
Use Claude Exporter or equivalent to capture the full transcript before any context is lost. Save to the chat folder immediately.\
\
**Step 3: Initiate Recovery Branch**\
Start a new chat specifically for context recovery:\
Name: `003a GOV-AIPM-BR (context recovery)`\
\
Provide the exported transcript and request:\
- Reconstruction of key decisions and their rationale\
- Identification of work completed vs. work in progress\
- Assessment of where token usage became problematic\
- Generation of a Recovery Bootstrap or Supplemental Bootstrap\
\
**Step 4: Create Replacement Session**\
Initiate the replacement chat with proper naming (as if the exhausted session hadn't occurred):\
Name: `004 GOV-AIPM-EX (parts III-IV draft)`\
\
Provide:\
- Original bootstrap from the exhausted session\
- Recovery Bootstrap from the recovery branch\
- Relevant project files\
\
**Step 5: Intensify Monitoring**\
In the replacement session, increase token monitoring frequency. Request headroom assessments before major work blocks, not just at session transitions. Note in the Session Log that this session follows a recovery.\
\
**Step 6: Document the Incident**\
Log the exhaustion event in the Session Log with:\
- Point where exhaustion occurred\
- Cause assessment (too much content loaded, scope creep, etc.)\
- Recovery steps taken\
- Lessons for future sessions\
\
[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13]

================================================================================
END OF EXTRACTION
================================================================================
