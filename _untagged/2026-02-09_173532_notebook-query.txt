================================================================================
EXTRACTION: 2026-02-09_173532_notebook-query
================================================================================
Branch:          N/A
Source Package:  N/A
Pass Type:       N/A (N/A)
Pass Number:     N/A
Theme/Focus:     Untagged
Routing:         Tier 3 (Untagged)
Query:           [NLMINTA|014|PE] What is the full validated extraction pipeline methodology developed across NLMINTA sessions? List all pass types (PE, TM, CE, TX, VX, TS, EV, VT), their purposes, recommended sequencing, configuration guidance (chat_configure settings, response_length), query formulation best practices, and any production workflow recommendations. Include findings numbers.
Timestamp:       2026-02-09T17:35:32.000922+00:00
Notebook ID:     b8fd3d1c-db62-40e6-bddf-6d8474a3733d
Source IDs:      all
Conversation ID: d60668a4-0bc3-491d-a861-da8257871b6d
Chat Configure:  default
Response Chars:  6824
Duration:        45999ms
================================================================================

The NLMINTA sessions established a validated **Three-Stage Over-Capture Pipeline** for extracting research material using NotebookLM. The methodology prioritizes **pass separation** (distinct queries for different data types) and **thematic segmentation** (extracting by topic rather than file position) to maximize yield and fidelity.\
\
### **1. Validated Pass Types (The Query Taxonomy)**\
\
The extraction protocol uses specific pass codes defined in the SeraphRecorder specification and validated through NLMINTA testing. **Note:** The legacy code **CE** (Concept Extraction) has been deprecated in favor of **TX** (Thematic Extraction) to align with the thematic segmentation discovery [1, 2].\
\
| Code | Pass Name | Purpose & Function | Finding Ref |\
| :--- | :--- | :--- | :--- |\
| **PE** | **Pre-Extraction Evaluation** | Scans source for coherence, factual accuracy, structure quality, and estimated density before extraction begins. Serves as corpus reconnaissance [2, 3]. | **F73** |\
| **TM** | **Theme Mapping** | Identifies the major themes, con< 2026/02/09 17:36:18.000057825  length=5802 from=10159 to=15960
ceptual clusters, and taxonomic structure of the source material. Serves as the \"table of contents\" for subsequent extraction passes [2, 4]. | **F22, F61, F66** |\
| **TX** | **Thematic Extraction** | Extracts discrete concept packets focused on a specific theme identified in the TM pass. Replaces sequential/chunk-based extraction [2, 5]. | **F20, F63, F67** |\
| **VX** | **Vocabulary Extraction** | Extracts specialized terms, jargon, and definitions into structured glossary entries. Produces qualitatively distinct output from concept extraction [2, 6]. | **F41** |\
| **TS** | **Thesis String Extraction** | Extracts argumentative claims, thesis statements, and logical dependencies. Distinguishes arguments from descriptive concepts [2, 7]. | **F42** |\
| **EV** | **Evaluation Pass** | Performs critical analysis of the source: identifies internal contradictions, factual gaps, logical inconsistencies, and terminology drift [2, 8]. | **F43** |\
| **GD** | **Gap Detection** | Apophatic search asking NLM to identify concepts *not* covered by previous themes. Essential for catching meta-content and operational details [2, 9]. | **F58** |\
| **GX** | **Gap Extraction** | Targeted extraction pass to capture the specific orphan concepts identified during Gap Detection [2, 10]. | **F58** |\
| **CL** | **Classification** | Applies the project's domain taxonomy (DDS) to extracted concepts. Must be a query-directed pass, not passive influence [2, 11]. | **F39** |\
| **VT** | **Variance Trace** | *New/Proposed.* Isolates a single concept and traces its evolutionary arc across the corpus to identify developmental phases [12]. | **F64, F69** |\
| **TL** | **Thesis-Lens Extraction** | Uses a formalized thesis tree as a filter to extract and organize concepts. Primarily a Phase 4 structuring tool [2, 13]. | **F50, F51** |\
\
### **2. Recommended Sequencing (The Standard Protocol)**\
\
Per Finding **F45** (Pass Separation Principle) and **D-029**, extraction must be executed in a specific order to build context and ensure coverage [14, 15].\
\
1.  **PE (Pre-Extraction):** Assess the source to determine quality and structure.\
2.  **TM (Theme Mapping):** Generate the theme list.\
3.  **TX (Thematic Extraction):** Run **one separate query for each theme** identified in TM.\
4.  **VX (Vocabulary):** Extract the glossary.\
5.  **TS (Thesis Strings):** Extract the arguments.\
6.  **EV (Evaluation):** Analyze for contradictions/issues (identifies harmonization work).\
7.  **GD (Gap Detection):** Check for missing content.\
8.  **GX (Gap Extraction):** Extract anything found in GD.\
9.  **CL (Classification):** Apply domain tags to the accumulated dataset.\
\
### **3. Configuration Guidance**\
\
Findings from Session 009 established the optimal configuration for the `chat_configure` tool [16, 17].\
\
*   **`chat_configure` Custom Prompt:**\
    *   **Function:** Provides \"soft guidance\" or nudging (e.g., getting consistent ID formatting) rather than hard constraints [18].\
    *   **Recommendation (D-027):** Use a **Dual-Config Strategy**. Run critical TX passes twice...once with default settings and once with a custom \"persona\" prompt (e.g., \"You are a concept extraction engine...\"). Each configuration surfaces unique concepts the other misses, maximizing yield [19, 20].\
*   **`response_length` Parameter:**\
    *   **Setting:** Set to `longer` for extraction passes.\
    *   **Effect:** Controls **depth**, not breadth. It produces richer individual concept packets (more details, longer quotes) but *does not* increase the number of concepts returned [21].\
\
### **4. Query Formulation Best Practices**\
\
*   **Pass Separation (D-029):** **Never combine** pass types (e.g., \"Extract concepts, vocabulary, and theses\"). Combined queries reduce yield by **29...56%** and degrade depth because NLM splits its output budget. Always use dedicated queries [22, 23].\
*   **Signal String Tagging (D-030):** Append the **[SERAPH]** metadata tag at the **beginning** of every query to enable automated routing and file naming by the SeraphRecorder pipeline [24, 25].\
    *   Format: `[SERAPH: PROJECT=clgysing, SP=03, PASS=TX, NUM=01...]`\
*   **Source Filtering:** Always use the `source_ids` parameter to constrain queries to specific packages. Natural language instructions (\"look in source 3\") are unreliable [26, 27].\
*   **Thematic over Sequential:** Do not ask for \"the first 3,000 words.\" Ask for \"all concepts related to [Theme X].\" NLM operates semantically, not positionally [5, 28].\
*   **Query-Directed Classification:** Do not expect NLM to passively use an uploaded taxonomy. You must explicitly ask it to \"Classify these concepts using the Domain Taxonomy source\" [29].\
\
### **5. Production Workflow Recommendations**\
\
*   **Scale & Segmentation:** NLM handles massive corpora (up to ~362K words or ~6,000 pages) without degradation (**F61**, **F66**). It often produces *better* quality on larger corpora because it can synthesize evolutionary variants (**F63**) [30, 31].\
*   **Save Protocol (D-026):** Use **SeraphRecorder** (server-side logging) to capture outputs. Do not rely on chat history or manual copy-paste [32].\
*   **Iterative Convergence Model (F60):** For refining complex concepts, use the corpus scale to your advantage. Extract the \"state of the art\" concept (Degree 1), augment it with human clarification, and re-extract (Degree 2) to stabilize terminology [33].\
*   **Raw vs. Curated:** Raw conversational logs (e.g., ChatGPT exports) are valuable for **Variance Tracing (VT)** because they preserve the evolutionary history of a concept that curated articles often compress away (**F70**) [34].

================================================================================
END OF EXTRACTION
================================================================================
