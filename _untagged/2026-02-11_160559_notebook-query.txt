================================================================================
EXTRACTION: 2026-02-11_160559_notebook-query
================================================================================
Source Package:  N/A
Pass Type:       N/A (N/A)
Pass Number:     N/A
Theme/Focus:     Untagged
Routing:         Tier 3 (Untagged)
Query:           What is the exact code or implementation of flush_pipeline in the Chronicler server? What does push-to-github.sh do exactly? What is the file path of the Chronicler server code on EC2?
Timestamp:       2026-02-11T16:05:59.000787+00:00
Notebook ID:     78ccac4a-4c28-4902-b721-0f77e25e1d59
Source IDs:      all
Conversation ID: 785d094c-4e4d-4a9b-9a6f-94d07ab66483
Chat Configure:  default
Response Chars:  2882
Duration:        40000ms
================================================================================

Based on the provided sources, here are the details regarding the Chronicler server's `flush_pipeline` tool, the `push-to-github.sh` script, and the server code location on the EC2 instance.

### 1. `flush_pipeline` Implementation
The `flush_pipeline` tool was added to the Chronicler server in **Session 018** to trigger on-demand data extraction and synchronization without waiting for the daily cron job.

*   **Logic/Implementation**: The tool executes a specific sequence of shell commands on the EC2 instance to synchronize the repository and process logs.
*   **Command Sequence**:
    1.  **Git Sync**: Performs `git pull --rebase` in the repository directory to ensure the local repo is up-to-date and avoid conflicts with Chronicler's own commits.
    2.  **Extraction**: Runs `python3 extract.py --all` in the logger directory to process all traffic logs.
    3.  **Push**: Runs `bash push-to-github.sh` to sync the new extractions to the remote GitHub repository.
*   **Manual Equivalent**: The functionality matches running this command chain on the server:
    ```bash
    cd /home/ubuntu/clgysing-extractions && git pull --rebase && 
    cd /home/ubuntu/mcp-logger && python3 extract.py --all && 
    bash push-to-github.sh
    ```

### 2. `push-to-github.sh` Functionality
This script, located at `/home/ubuntu/mcp-logger/push-to-github.sh`, is responsible for syncing extracted text files from the EC2 staging area to the private GitHub repository.

**Exact Steps Performed:**
1.  **Repo Check**: Verifies the local git repository exists at `/home/ubuntu/clgysing-extractions`.
2.  **File Check**: Checks if there are any extracted files in `/home/ubuntu/mcp-logs/extracted`.
3.  **Sync**: Uses `rsync` to copy files from the extracted directory to the local repository folder (preserving directory structure).
4.  **Git Update**: Runs `git pull --rebase` to integrate any remote changes (preventing conflicts with Chronicler operations).
5.  **Commit & Push**: Stages all changes (`git add -A`), creates a commit with the current date (e.g., "Extract: YYYY-MM-DD..."), and pushes to `origin main`.

### 3. Chronicler Server File Path
The Chronicler server code is located on the EC2 instance at:
` /home/ubuntu/chronicler/chronicler.py`

*   **Configuration**: The environment variables (including the GitHub Personal Access Token) are stored in `/home/ubuntu/chronicler/.env`.
*   **Service**: It runs as a systemd service named `chronicler.service`.

**Sources:**
[1] `v1.1_REF_Chronicler-Quick-Reference.txt` (flush_pipeline description)
[2] `Claude-018 AWSNBLM (Layer 4 Automation & Testing).md` (flush_pipeline logic verification)
[3] `v1.0_REF_SeraphRecorder-Tool-Reference.txt` (push-to-github.sh behavior)
[4] `Claude-013 AWSNBLM (Pipeline Build).md` (push-to-github.sh script content)
[5] `v1.0_SPEC_Chronicler-GitHub-Repo-Tool.txt` (Chronicler file path)

================================================================================
END OF EXTRACTION
================================================================================
